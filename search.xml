<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Homework 2 - Income Prediction]]></title>
    <url>%2F2019%2F03%2F29%2FHomework-2-Income-Prediction%2F</url>
    <content type="text"><![CDATA[收入预测123456import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom random import shufflefrom math import floor, logfrom numpy.linalg import inv数据处理12train_data = pd.read_csv("train.csv")train_data.info()1train_data.head()对数据进行可视化观察12345678910111213141516171819202122232425262728293031323334353637383940plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号fig = plt.subplots(figsize=(16,8),dpi=80)plt.figure(1)ax1 = plt.subplot(231)train_data.income.value_counts().plot(kind='bar')plt.xlabel("收入情况")plt.ylabel("人数") ax2 = plt.subplot(232)train_data.education.value_counts().plot(kind='bar')plt.ylabel("人数")# #plt.xticks([1, 2, 3])plt.xlabel("教育情况")ax3 = plt.subplot(233)plt.hist(train_data.age,20)# #plt.yticks([0, 1])plt.ylabel('人数')plt.xlabel('年龄')ax4 = plt.subplot(234)train_data.sex.value_counts().plot(kind='bar')plt.ylabel("人数")plt.xlabel('性别')ax5 = plt.subplot(235)train_data.workclass.value_counts().plot(kind='bar')plt.ylabel("人数") plt.xlabel("所属企业类型")ax6 = plt.subplot(236)train_data.race.value_counts().plot(kind='bar')plt.ylabel("人数")plt.xlabel("种族")# 调整每隔子图之间的距离 plt.tight_layout()plt.show()1234567# 将&gt;50K转为1，&lt;=50K转为0， 方便数据可视化操作# 使用train_data.income[train_data.income == " &gt;50K"] = 1# 会弹出A value is trying to be set on a copy of a slice from a DataFrame.# 修改数据最好不要使用链式操作train_data.loc[train_data.income == " &gt;50K", 'income'] = 1train_data.loc[train_data.income ==" &lt;=50K", 'income'] = 0train_data.head()12345678910fig = plt.figure() fig.set(alpha=0.2) male = train_data.income[train_data.sex == ' Male'].value_counts() female = train_data.income[train_data.sex == ' Female'].value_counts() df=pd.DataFrame(&#123;'male':male, 'female':female&#125;) df.plot(kind='bar', stacked=True) plt.xlabel("收入情况") plt.ylabel("人数") plt.show()train_data[["sex", "income"]].groupby(['sex'], as_index=False).mean().sort_values(by='income', ascending=False)查看学历对应的收入情况12345678910111213141516171819202122232425262728293031323334fig = plt.subplots(figsize=(16,8),dpi=80)plt.figure(1)ax1 = plt.subplot(231) train_data.income[train_data.education == ' HS-grad'].value_counts().plot(kind='bar', label=" HS-grad", color='red') ax1.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax1.legend(["HS研究生学位"], loc='best') ax2 = plt.subplot(232) train_data.income[train_data.education == ' Doctorate'].value_counts().plot(kind='bar', label=" Doctorate", color='lightblue') ax2.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax2.legend(["博士学位"], loc='best') ax3 = plt.subplot(233) train_data.income[train_data.education == ' Masters'].value_counts().plot(kind='bar', label=" Masters", color='blue') ax3.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax3.legend(["硕士学位"], loc='best') ax4 = plt.subplot(234) train_data.income[train_data.education == ' Bachelors'].value_counts().plot(kind='bar', label=" Bachelors", color='pink') ax4.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax4.legend(["学士学位"], loc='best') ax5 = plt.subplot(235) train_data.income[train_data.education == ' Assoc-voc'].value_counts().plot(kind='bar', label=" Assoc-voc", color='steelblue') ax5.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax5.legend(["副学士学位"], loc='best') ax6 = plt.subplot(236) train_data.income[train_data.education == ' Some-college'].value_counts().plot(kind='bar', label=" Some-college", color='#FA2479') ax6.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax6.legend(["本科生学位"], loc='best') plt.show()12# 各国家人数train_data.native_country.value_counts()将所有含有缺失值的行都去掉，可以使用RandomForestRegressor填补缺失12345678910111213141516171819202122232425262728# from sklearn.ensemble import RandomForestRegressor# 使用RandomForestRegressor填补缺失的年龄属性# def set_missing_workclass(df):# # 把已有的数值型特征取出来丢进RandomForestRegressor中# workclass_df = df[['workclass','age', 'education', 'race', 'income']]## # 提取未知值和已知值# known_workclass = workclass_df[workclass_df.workclass.notnull()].values# unknown_workclass = workclass_df[workclass_df.workclass.isnull()].values # # y即目标workclass# y = known_workclass[:, 0] # # X即特征属性值# X = known_workclass[:, 1:] # # fit到RandomForestRegressor之中# rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)# rfr.fit(X, y) # # 用得到的模型进行未知年龄结果预测# predictedAges = rfr.predict(unknown_workclass[:, 1::]) # # 用得到的预测结果填补原缺失数据# df.loc[ (df.workclass.isnull()), 'workclass' ] = predictedAges # return df, rfr# train_data, rfr = set_missing_ages(train_data)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def dataProcess_X(raw_data): if "income" in raw_data.columns: # data = raw_data.drop(["sex", 'income'], axis=1) data = raw_data.drop(["income"], axis=1) else: # data = raw_data.drop(["sex"], axis=1) pass # 处理数据无效字符 data_clean = data.replace(regex=[r'\?|\.|\$'], value=np.nan) data = data_clean.dropna(how='any') # data_clean.isnull().any() # 剔除没有用的数据特征fnlwgt # fnlwgt: 连续性数值变量；人口普查员认为观察值的人数 data.drop(['fnlwgt'], axis=1) # 处理sex data.loc[data.sex == " Male", 'sex'] = 1 data.loc[data.sex ==" Female", 'sex'] = 0 listObjectColumn = [col for col in data.columns if data[col].dtypes == "object"] #读取非数字的column listNonObjedtColumn = [x for x in list(data) if x not in listObjectColumn] ObjectData = data[listObjectColumn] NonObjectData = data[listNonObjedtColumn] # NonObjectData.insert(0 ,"sex", (raw_data["sex"] == " Female").astype(np.int)) # 使用pd.get_dummies()特征因子化 # 也可以使用one-hot # from sklearn.feature_extraction import DictVectorizer # dict_vect=DictVectorizer(sparse=False) # X_train=dict_vect.fit_transform(X_train.to_dict(orient='record')) # X_test=dict_vect.transform(X_test.to_dict(orient='record')) # dict_vect.feature_names_ ObjectData = pd.get_dummies(ObjectData) data = pd.concat([NonObjectData, ObjectData], axis=1) X = data.astype("int64") # 标准化 X = (X - X.mean()) / X.std() return np.array(X)def dataProcess_y(raw_data): data = raw_data.copy() # 处理数据无效字符 data_clean = data.replace(regex=[r'\?|\.|\$'],value=np.nan) data = data_clean.dropna(how='any') # data_clean.isnull().any() try: # y = data['income'] # y = pd.DataFrame((y ==' &gt;50K').astype("int64"), columns=["income"]) data.loc[data.income == " &gt;50K", 'income'] = 1 data.loc[data.income == " &lt;=50K", 'income'] = 0 except: pass y = np.array(data['income']) y = y.reshape(y.shape[0], 1) return yX_train = dataProcess_X(train_data)y_train = dataProcess_y(train_data)X_train.shape, y_train.shape# ((30162, 103), (30162, 1))导入测试集12345test_data = pd.read_csv("test.csv")X_test = dataProcess_X(train_data)y_test = dataProcess_y(train_data)X_test.shape, y_test.shape# ((30162, 103), (30162, 1))12345678910111213141516171819202122def _shuffle(X, y): #X and Y are np.array randomize = np.arange(X.shape[0]) np.random.shuffle(randomize) return (X[randomize], y[randomize])def split_valid_set(X, y, percentage): all_size = X.shape[0] valid_size = int(floor(all_size * percentage)) X, y = _shuffle(X, y) X_valid, y_valid = X[ : valid_size], y[ : valid_size] X_train, y_train = X[valid_size:], y[valid_size:] return X_train, y_train, X_valid, y_valid# 也可用sklearn函数打散数据# from sklearn.model_selection import train_test_split# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=1)X_train, y_train, X_valid, y_valid = split_valid_set(X_train, y_train, 0.1)X_train.shape, y_train.shape, X_valid.shape, y_valid.shape# ((27146, 103), (27146, 1), (3016, 103), (3016, 1))Generative方法123456789101112131415161718192021222324252627282930313233343536373839404142def train(X_train, y_train): # vaild_set_percetange = 0.1 # X_train, Y_train, X_valid, Y_valid = split_valid_set(X, Y, vaild_set_percetange) #Gussian distribution parameters train_data_size = X_train.shape[0] cnt1 = 0 cnt2 = 0 mu1 = np.zeros((1, X_train.shape[1])) mu2 = np.zeros((1, X_train.shape[1])) for i in range(train_data_size): if y_train[i] == 1: # &gt;50k mu1 += X_train[i] cnt1 += 1 else: mu2 += X_train[i] cnt2 += 1 mu1 /= cnt1 mu2 /= cnt2 sigma1 = np.zeros((X_train.shape[1], X_train.shape[1])) sigma2 = np.zeros((X_train.shape[1], X_train.shape[1])) for i in range(train_data_size): if y_train[i] == 1: sigma1 += np.dot(np.transpose(X_train[i].reshape(1,103) - mu1), X_train[i] - mu1) else: sigma2 += np.dot(np.transpose(X_train[i].reshape(1,103) - mu2), X_train[i] - mu2) sigma1 /= cnt1 sigma2 /= cnt2 shared_sigma = (float(cnt1) / train_data_size) * sigma1 + (float(cnt2) / train_data_size) * sigma2 N1 = cnt1 N2 = cnt2 return mu1, mu2, shared_sigma, N1, N2mu1, mu2, shared_sigma, N1, N2 = train(X_train, y_train)mu1.shape, mu2.shape, shared_sigma.shape, N1, N212345def sigmoid(z): res = 1 / (1.0 + np.exp(-z)) return np.clip(res, 1e-8, (1-(1e-8)))# from scipy.special import expit1234567891011121314151617def valid(funname, X, Y, mu1, mu2, shared_sigma, N1, N2): sigma_inv = inv(shared_sigma) w = np.dot((mu1-mu2), sigma_inv) X_t = X.T b = (-0.5) * np.dot(np.dot(mu1, sigma_inv), mu1.T) + (0.5) * np.dot(np.dot(mu2, sigma_inv), mu2.T) + np.log(float(N1)/N2) a = np.dot(w,X_t) + b y = sigmoid(a) y_ = np.around(y) result = (np.squeeze(Y) == y_) print(f'&#123;funname&#125; acc = %f' % (float(result.sum()) / X.shape[0]))valid("train", X_train, y_train, mu1, mu2, shared_sigma, N1, N2)valid("valid", X_valid, y_valid, mu1, mu2, shared_sigma, N1, N2)valid("test", X_test, y_test, mu1, mu2, shared_sigma, N1, N2)# train acc = 0.837140# valid acc = 0.851459# test acc = 0.838572使用Discriminative方法mini_batch1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# mini_batch def train(X_train, y_train): w = np.zeros((1, len(X_train[0]))) l_rate = 0.0001 batch_size = 32 train_dataz_size = len(X_train) step_num = int(floor(train_dataz_size / batch_size)) epoch_num = 300 list_cost = [] total_loss = 0.0 for epoch in range(1, epoch_num): total_loss = 0.0 X_train, y_train = _shuffle(X_train, y_train) for idx in range(1, step_num): X = X_train[idx*batch_size:(idx+1)*batch_size] #32*104 Y = y_train[idx*batch_size:(idx+1)*batch_size] #32*1 s_grad = np.zeros((1,len(X[0]))) z = np.dot(X, w.T) # 32*104*104*1 y = sigmoid(z) # squeeze 函数：从数组的形状中删除单维度条目，即把shape中为1的维度去掉 # loss = y - np.squeeze(Y) loss = y - Y cross_entropy = -1 * (np.dot(Y.T, np.log(y)) + np.dot((1 - Y.T),\ np.log(1 - y)))/ len(Y) total_loss += cross_entropy[0][0] #grad = np.sum(-1 * X * (np.squeeze(Y) - y).reshape((batch_size, 1)), axis=0) grad = np.sum(np.dot((y - Y).T, X), axis=0) #1*32*32*104 # grad = np.dot(X.T, loss) w = w - l_rate * grad # s_grad += grad ** 2 # ada = np.sqrt(s_grad) # w = w - l_rate * grad / ada list_cost.append(total_loss) # valid(X_valid, Y_valid, w) plt.plot(np.arange(len(list_cost)), list_cost) plt.title("Train Process") plt.xlabel("epoch_num") plt.ylabel("Cost Function (Cross Entropy)") plt.show() return wdef valid(funname, X, Y, w): a = np.dot(w, X.T) y = sigmoid(a) y_ = np.around(y) result = (np.squeeze(Y) == y_) print(f'&#123;funname&#125; acc = %f' % (float(result.sum()) / X.shape[0]))X_train_logi = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)X_valid_logi = np.concatenate((np.ones((X_valid.shape[0], 1)), X_valid), axis=1)X_test_logi = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)w_train = train(X_train_logi, y_train)valid("train", X_train_logi, y_train, w_train)valid("valid", X_valid_logi, y_valid, w_train)valid("test", X_test_logi, y_test, w_train)学习率使用Ada12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 学习率使用Adadef train(X_train, y_train): w = np.zeros((1, len(X_train[0]))) s_grad = np.zeros((1, len(X_train[0]))) l_rate = 0.1 epoch_num = 10000 list_cost = [] total_loss = 0.0 X = X_train y = y_train for epoch in range(epoch_num): z = np.dot(X, w.T) # n*104*104*1 Y = sigmoid(z) loss = Y - y cross_entropy = -1 * (np.dot(y.T, np.log(Y)) + np.dot((1 - y.T),\ np.log(1 - Y)))/ len(y) if abs(total_loss - cross_entropy[0][0]) &lt; 10**-9: break else: total_loss = cross_entropy[0][0] list_cost.append(total_loss) grad = np.sum(np.dot((Y - y).T, X), axis=0) #1*104 s_grad += grad**2 ada = np.sqrt(s_grad) w = w - l_rate * grad / ada print("times:", len(list_cost)) plt.plot(np.arange(len(list_cost)), list_cost) plt.title("Train Process") plt.xlabel("epoch_num") plt.ylabel("Cost Function (Cross Entropy)") plt.show() return wX_train_logi = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)X_valid_logi = np.concatenate((np.ones((X_valid.shape[0], 1)), X_valid), axis=1)X_test_logi = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)w_train = train(X_train_logi, y_train)valid("train", X_train_logi, y_train, w_train)valid("valid", X_valid_logi, y_valid, w_train)valid("test", X_test_logi, y_test, w_train)使用Kreas12from keras.models import Sequentialfrom keras.layers import Dense, Activation1234567891011121314151617model = Sequential()model.add(Dense(units=600, activation='sigmoid', input_dim=103))model.add(Dense(units=600, activation='sigmoid'))model.add(Dense(units=1, activation='sigmoid'))model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])model.fit(X_train, y_train, batch_size=32, epochs=50)score = model.evaluate(X_test, y_test)result = np.squeeze(model.predict(X_test))# print('Total loss on Testing set: ', score[0])# print('Accuracy of Testing set: ', score[1])# Total loss on Testing set: 0.2154487861520091# Accuracy of Testing set: 0.90295736357400741234y_ = np.around(result).astype(np.int)result = (np.squeeze(y_test) == y_)print('Test acc = %f' % (float(result.sum()) / X_test.shape[0]))# Test acc = 0.902957使用Tensorflow搭建3层神经网络1import tensorflow as tf12345678910111213141516171819xs = tf.placeholder(tf.float32, [None, 103])ys = tf.placeholder(tf.float32, [None, 1])w1 = tf.Variable(tf.random_normal([103, 600], stddev=1, seed=1))w2 = tf.Variable(tf.random_normal([600, 600], stddev=1, seed=1))w3 = tf.Variable(tf.random_normal([600, 1], stddev=1, seed=1))a = tf.nn.relu(tf.matmul(xs, w1))b = tf.sigmoid(tf.matmul(a, w2))y = tf.sigmoid(tf.matmul(b, w3))# a = tf.matmul(xs, w1)# y = tf.matmul(a, w2)y_ = tf.round(y)cross_entropy = -tf.reduce_mean(ys*tf.log(tf.clip_by_value(y,1e-10,1.0)))train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy)correct_prediction = tf.equal(y_, ys)accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))1234567891011result = Nonewith tf.Session() as sess: init = tf.global_variables_initializer() sess.run(init) for step in range(101): # training sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train&#125;) result = sess.run(y, feed_dict=&#123;xs: X_train, ys: y_train&#125;) if step % 100 == 0: print("accuracy:",sess.run(accuracy, feed_dict=&#123;xs: X_train, ys: y_train&#125;))# accuracy: 0.7285788随机森林和XGBoost1234# 随机森林from sklearn import metricsfrom sklearn.ensemble import RandomForestClassifierrfc=RandomForestClassifier()123# XGBoostfrom xgboost import XGBClassifierxgbc=XGBClassifier()123456# 选取k-1折的数据进行模型训练import warningswarnings.filterwarnings("ignore")from sklearn.model_selection import cross_val_score cross_val_score(rfc,X_train, y_train.ravel(),cv=5).mean(), cross_val_score(xgbc,X_train, y_train.ravel(),cv=5).mean()# (0.8434758405223647, 0.8613788898712886)12345#默认随机森林预测rfc.fit(X_train, y_train)rfc_y_predict = rfc.predict(X_valid)rfc.score(X_valid, y_valid)# 0.839854111405835512345# XGBoost预测xgbc.fit(X_train, y_train)xgbc_y_predict = xgbc.predict(X_valid)xgbc.score(X_valid, y_valid)# 0.862068965517241312345from sklearn.metrics import classification_reportprint('随机森林的预测准确率:')print(classification_report(y_valid, rfc_y_predict, target_names=['result'])) print('XGBoost的预测准确率:') print(classification_report(y_valid, xgbc_y_predict, target_names=['result']))保存数据1234# df = pd.DataFrame(&#123;"id": np.arange(1, 16282), "label": y_&#125;)# if not os.path.exists(output_dir):# os.mkdir(output_dir)# df.to_csv(os.path.join(output_dir + 'nn_output.csv'), sep='\t', index=False)]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homework 1 - PM2.5 Prediction]]></title>
    <url>%2F2019%2F03%2F25%2FHomework-1-PM2-5-Prediction%2F</url>
    <content type="text"><![CDATA[PM2.5预测123import numpy as npimport pandas as pdimport matplotlib.pyplot as plt数据处理1234train_data = pd.read_csv("train.csv")# pm2_5 = train_data[train_data['Obvservations']=='PM2.5'].iloc[:,3:]# pm2_5.info()train_data.info() # 240条数据，每条数据18个feature12345# 将RAINFALL值为NR的数据置0train_data[train_data[train_data['Obvservations']=='RAINFALL'].iloc[:,3:] == 'NR'] = 0# pm2_5 = train_data[train_data['Obvservations']=='PM2.5'].iloc[:,3:]# pm2_5.info()train_data[train_data['Obvservations']=='RAINFALL'].head()123456789101112tempxlist = [] tempylist = [] # 一天内总共有24-10+1 =15条记录for j in range(0, 240): for i in range(15): tempx = np.array(train_data.iloc[j*18:(j+1)*18,3:].iloc[:, i:i+9], float).reshape(1, 18*9) tempy = np.array(train_data.iloc[j*18+9:j*18+10,3:].iloc[:, i+9], float) # tempx = pm2_5.iloc[:,i:i+9] #使用前9小时数据作为feature # tempy = pm2_5.iloc[:,i+9] #使用第10个小数数据作为lable tempxlist.append(tempx) tempylist.append(tempy)12345678# X = np.array(pd.concat(tempxlist), float)X = np.concatenate(tempxlist, axis=0)# 插入列向量[1;1;...;1;]X = np.insert(X, 0, values=np.ones((1, X.shape[0])), axis=1)# y = np.array(pd.concat(tempylist), float)y = np.concatenate(tempylist, axis=0)y = y.reshape(y.shape[0], 1)X, y12X.shape, y.shape# ((3600, 163), (3600, 1))12# 特征归一化(Feature Scaling)X = (X - X.mean()) / X.std()12345# 代价函数def cost(y, w): temp = np.dot(X,w) loss = np.square(y - temp) return np.sum(loss)/len(y)开始训练adagrad算法123456789101112131415161718192021# adagrad算法def ada(X, y, w, lr, iteration, lambdaL2): list_cost = [] s_grad = np.zeros([len(X[0]), 1]) for i in range(iteration): hypo = np.dot(X,w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) grad = np.dot(X.T, loss)/len(X) + lambdaL2*w s_grad += grad**2 ada = np.sqrt(s_grad) w = w - lr*grad/ada return w, list_costlr_ada = 10w_ada = np.zeros([X.shape[1], 1])w_ada, list_cost_ada = ada(X, y, w_ada, lr_ada, 10000, 0.)cost(y, w_ada)# 38.190334888655144SGD算法1234567891011121314151617181920# SGD算法def SGD(X, y, w, lr, iteration, lambdaL2): list_cost = [] for i in range(iteration): hypo = np.dot(X,w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) rand = np.random.randint(0, len(X)) grad = X[rand].reshape(X.shape[1], 1)*loss[rand].reshape(loss.shape[1], 1)/len(X) + lambdaL2*w w = w - lr*grad return w, list_costw_sgd = np.zeros([X.shape[1], 1])lr_sgd = 0.1w_sgd, list_cost_sgd = SGD(X, y, w_sgd, lr_sgd, 10000, 0.)cost(y, w_sgd)# 209.00938349734912GD算法1234567891011121314151617def GD(X, y, w, lr, iteration, lambdaL2): list_cost = [] for i in range(iteration): hypo = np.dot(X, w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) grad = np.dot(X.T, loss)/len(X) + lambdaL2 * w w = w - lr*grad return w, list_costw_gd = np.zeros([X.shape[1], 1])lr_gd = 0.01w_gd, list_cost_gd = GD(X, y, w_gd, lr_gd, 10000, 0.)cost(y, w_gd)# 44.80863160107732正规方程1234#close formw_cf = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)cost_wcf = np.sum((X.dot(w_cf)-y)**2) / len(X)hori = [cost_wcf for i in range(10000-3)]12345678910fig = plt.figure(figsize=(12,8))plt.plot(np.arange(len(list_cost_ada[3:])), list_cost_ada[3:], 'b', label="ada")plt.plot(np.arange(len(list_cost_sgd[3:])), list_cost_sgd[3:], 'g', label='sgd')plt.plot(np.arange(len(list_cost_gd[3:])), list_cost_gd[3:], 'r', label='gd')plt.plot(np.arange(len(list_cost_ada[3:])), hori, 'y--', label='close-form')plt.title('Train Process')plt.xlabel('Iteration')plt.ylabel('Loss Function(Quadratic)')plt.legend()plt.show()使用Sklearn1234from sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import cross_val_predictfrom sklearn import metrics1234567X_train, X_test, y_train, y_test = train_test_split(X[:, 1:], y, random_state=1)linreg = LinearRegression()linreg.fit(X_train, y_train)predicted = cross_val_predict(linreg, X, y, cv=10)# linreg.intercept_, linreg.coef_metrics.mean_squared_error(y, predicted)# 42.57659928722291使用Tensorflow1import tensorflow as tf1234567891011121314151617181920212223242526272829303132333435363738def linear_regression(X_data, y_data, alpha, epoch, optimizer=tf.train.GradientDescentOptimizer): tf.reset_default_graph() xs = tf.placeholder(tf.float32, [None, X_data.shape[1]]) ys = tf.placeholder(tf.float32, [None, 1]) W = tf.Variable(tf.random_uniform([X_data.shape[1], 1], -10.0, 10.0)) y_pred = tf.matmul(xs, W) loss = tf.reduce_mean(tf.square(ys - y_pred)) if optimizer == tf.train.GradientDescentOptimizer: alpha = 0.01 elif optimizer == tf.train.AdagradOptimizer: alpha = 10 elif optimizer == tf.train.AdamOptimizer: alpha = 0.1 elif optimizer == tf.train.FtrlOptimizer: alpha = 10 elif optimizer == tf.train.RMSPropOptimizer: alpha = 10 opt = optimizer(learning_rate=alpha) opt_operation = opt.minimize(loss) # run the session with tf.Session() as sess: sess.run(tf.global_variables_initializer()) loss_data = [] for i in range(epoch): _, loss_val, W_val = sess.run([opt_operation, loss, W], feed_dict=&#123;xs: X_data, ys: y_data&#125;) loss_data.append(loss_val) if len(loss_data) &gt; 1 and np.abs(loss_data[-1] - loss_data[-2]) &lt; 10 ** -9: break tf.reset_default_graph() return &#123;'loss': loss_data, 'parameters': W_val&#125; # just want to return in row vector format123456789101112131415161718epoch = 10000alpha = 0.0001# 各种优化函数optimizer_dict=&#123;'GD': tf.train.GradientDescentOptimizer, 'Adagrad': tf.train.AdagradOptimizer, #'Adam': tf.train.AdamOptimizer, #'Ftrl': tf.train.FtrlOptimizer, #'RMS': tf.train.RMSPropOptimizer #'Momentum': tf.train.MomentumOptimizer两个参数 &#125;results = []t_loss = dict()for name in optimizer_dict: # 这里X应该是X[:, 1:] res = linear_regression(X, y, alpha, epoch, optimizer=optimizer_dict[name]) res['name'] = name t_loss[name] = res results.append(res)1234567891011121314fig, ax = plt.subplots(figsize=(16, 9))for res in results: loss_data = res['loss'] ax.plot(np.arange(len(loss_data[10:])), loss_data[10:], label=res['name'])ax.plot(np.arange(len(list_cost_ada[10:])), hori[0: 9990], label='close-form') ax.set_xlabel('epoch', fontsize=18)ax.set_ylabel('cost', fontsize=18)ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)ax.set_title('different optimizer', fontsize=18)plt.show()12t_loss["Adagrad"]["loss"][-1], t_loss["GD"]["loss"][-1]# (39.07505, 168.04092)保存数据123test_data = pd.read_csv("test.csv")# pm2_5_test = test_data[test_data['AMB_TEMP'] == 'PM2.5'].iloc[:,2:]# pm2_5_test.info()1234# 对X_test进行一系列数据处理即可，不赘述# X_test = np.array(pm2_5_test, float)# X_test = np.insert(X_test, 0, values=np.ones((1, X_test.shape[0])), axis=1)X_test12345#预测y_star = np.dot(X_test, w)y_pre = pd.read_csv("sampleSubmission.csv")y_pre.value = y_stary_pre.to_csv('predict.csv', index=False)]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好！]]></title>
    <url>%2F2019%2F02%2F28%2F%E8%BF%99%E6%98%AF%E4%B8%80%E6%AC%A1%E5%B0%8F%E5%B0%8F%E7%9A%84%E5%B0%9D%E8%AF%95%EF%BC%81%2F</url>
    <content type="text"><![CDATA[那些都是很好很好的，可我偏偏不喜欢。]]></content>
  </entry>
</search>
