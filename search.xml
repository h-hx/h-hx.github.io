<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Homework 1 - PM2.5 Prediction]]></title>
    <url>%2F2019%2F03%2F25%2FHomework-1-PM2-5-Prediction%2F</url>
    <content type="text"><![CDATA[PM2.5预测123import numpy as npimport pandas as pdimport matplotlib.pyplot as plt数据处理1234train_data = pd.read_csv("train.csv")# pm2_5 = train_data[train_data['Obvservations']=='PM2.5'].iloc[:,3:]# pm2_5.info()train_data.info() # 240条数据，每条数据18个feature12345# 将RAINFALL值为NR的数据置0train_data[train_data[train_data['Obvservations']=='RAINFALL'].iloc[:,3:] == 'NR'] = 0# pm2_5 = train_data[train_data['Obvservations']=='PM2.5'].iloc[:,3:]# pm2_5.info()train_data[train_data['Obvservations']=='RAINFALL'].head()123456789101112tempxlist = [] tempylist = [] # 一天内总共有24-10+1 =15条记录for j in range(0, 240): for i in range(15): tempx = np.array(train_data.iloc[j*18:(j+1)*18,3:].iloc[:, i:i+9], float).reshape(1, 18*9) tempy = np.array(train_data.iloc[j*18+9:j*18+10,3:].iloc[:, i+9], float) # tempx = pm2_5.iloc[:,i:i+9] #使用前9小时数据作为feature # tempy = pm2_5.iloc[:,i+9] #使用第10个小数数据作为lable tempxlist.append(tempx) tempylist.append(tempy)12345678# X = np.array(pd.concat(tempxlist), float)X = np.concatenate(tempxlist, axis=0)# 插入列向量[1;1;...;1;]X = np.insert(X, 0, values=np.ones((1, X.shape[0])), axis=1)# y = np.array(pd.concat(tempylist), float)y = np.concatenate(tempylist, axis=0)y = y.reshape(y.shape[0], 1)X, y12X.shape, y.shape# ((3600, 163), (3600, 1))12# 特征归一化(Feature Scaling)X = (X - X.mean()) / X.std()12345# 代价函数def cost(y, w): temp = np.dot(X,w) loss = np.square(y - temp) return np.sum(loss)/len(y)123456789101112131415161718192021# adagrad算法def ada(X, y, w, lr, iteration, lambdaL2): list_cost = [] s_grad = np.zeros([len(X[0]), 1]) for i in range(iteration): hypo = np.dot(X,w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) grad = np.dot(X.T, loss)/len(X) + lambdaL2*w s_grad += grad**2 ada = np.sqrt(s_grad) w = w - lr*grad/ada return w, list_costlr_ada = 10w_ada = np.zeros([X.shape[1], 1])w_ada, list_cost_ada = ada(X, y, w_ada, lr_ada, 10000, 0.)cost(y, w_ada)# 38.1903348886551441234567891011121314151617181920# SGD算法def SGD(X, y, w, lr, iteration, lambdaL2): list_cost = [] for i in range(iteration): hypo = np.dot(X,w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) rand = np.random.randint(0, len(X)) grad = X[rand].reshape(X.shape[1], 1)*loss[rand].reshape(loss.shape[1], 1)/len(X) + lambdaL2*w w = w - lr*grad return w, list_costw_sgd = np.zeros([X.shape[1], 1])lr_sgd = 0.1w_sgd, list_cost_sgd = SGD(X, y, w_sgd, lr_sgd, 10000, 0.)cost(y, w_sgd)# 209.009383497349121234567891011121314151617def GD(X, y, w, lr, iteration, lambdaL2): list_cost = [] for i in range(iteration): hypo = np.dot(X, w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) grad = np.dot(X.T, loss)/len(X) + lambdaL2 * w w = w - lr*grad return w, list_costw_gd = np.zeros([X.shape[1], 1])lr_gd = 0.01w_gd, list_cost_gd = GD(X, y, w_gd, lr_gd, 10000, 0.)cost(y, w_gd)# 44.808631601077321234#close formw_cf = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)cost_wcf = np.sum((X.dot(w_cf)-y)**2) / len(X)hori = [cost_wcf for i in range(10000-3)]12345678910fig = plt.figure(figsize=(12,8))plt.plot(np.arange(len(list_cost_ada[3:])), list_cost_ada[3:], 'b', label="ada")plt.plot(np.arange(len(list_cost_sgd[3:])), list_cost_sgd[3:], 'g', label='sgd')plt.plot(np.arange(len(list_cost_gd[3:])), list_cost_gd[3:], 'r', label='gd')plt.plot(np.arange(len(list_cost_ada[3:])), hori, 'y--', label='close-form')plt.title('Train Process')plt.xlabel('Iteration')plt.ylabel('Loss Function(Quadratic)')plt.legend()plt.show()使用Sklearn1234from sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import cross_val_predictfrom sklearn import metrics1234567X_train, X_test, y_train, y_test = train_test_split(X[:, 1:], y, random_state=1)linreg = LinearRegression()linreg.fit(X_train, y_train)predicted = cross_val_predict(linreg, X, y, cv=10)# linreg.intercept_, linreg.coef_metrics.mean_squared_error(y, predicted)# 42.57659928722291使用Tensorflow1import tensorflow as tf1234567891011121314151617181920212223242526272829303132333435363738def linear_regression(X_data, y_data, alpha, epoch, optimizer=tf.train.GradientDescentOptimizer): tf.reset_default_graph() xs = tf.placeholder(tf.float32, [None, X_data.shape[1]]) ys = tf.placeholder(tf.float32, [None, 1]) W = tf.Variable(tf.random_uniform([X_data.shape[1], 1], -10.0, 10.0)) y_pred = tf.matmul(xs, W) loss = tf.reduce_mean(tf.square(ys - y_pred)) if optimizer == tf.train.GradientDescentOptimizer: alpha = 0.01 elif optimizer == tf.train.AdagradOptimizer: alpha = 10 elif optimizer == tf.train.AdamOptimizer: alpha = 0.1 elif optimizer == tf.train.FtrlOptimizer: alpha = 10 elif optimizer == tf.train.RMSPropOptimizer: alpha = 10 opt = optimizer(learning_rate=alpha) opt_operation = opt.minimize(loss) # run the session with tf.Session() as sess: sess.run(tf.global_variables_initializer()) loss_data = [] for i in range(epoch): _, loss_val, W_val = sess.run([opt_operation, loss, W], feed_dict=&#123;xs: X_data, ys: y_data&#125;) loss_data.append(loss_val) if len(loss_data) &gt; 1 and np.abs(loss_data[-1] - loss_data[-2]) &lt; 10 ** -9: break tf.reset_default_graph() return &#123;'loss': loss_data, 'parameters': W_val&#125; # just want to return in row vector format123456789101112131415161718epoch = 10000alpha = 0.0001# 各种优化函数optimizer_dict=&#123;'GD': tf.train.GradientDescentOptimizer, 'Adagrad': tf.train.AdagradOptimizer, #'Adam': tf.train.AdamOptimizer, #'Ftrl': tf.train.FtrlOptimizer, #'RMS': tf.train.RMSPropOptimizer #'Momentum': tf.train.MomentumOptimizer两个参数 &#125;results = []t_loss = dict()for name in optimizer_dict: # 这里X应该是X[:, 1:] res = linear_regression(X, y, alpha, epoch, optimizer=optimizer_dict[name]) res['name'] = name t_loss[name] = res results.append(res)1234567891011121314fig, ax = plt.subplots(figsize=(16, 9))for res in results: loss_data = res['loss'] ax.plot(np.arange(len(loss_data[10:])), loss_data[10:], label=res['name'])ax.plot(np.arange(len(list_cost_ada[10:])), hori[0: 9990], label='close-form') ax.set_xlabel('epoch', fontsize=18)ax.set_ylabel('cost', fontsize=18)ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)ax.set_title('different optimizer', fontsize=18)plt.show()12t_loss["Adagrad"]["loss"][-1], t_loss["GD"]["loss"][-1]# (39.07505, 168.04092)保存数据123test_data = pd.read_csv("test.csv")# pm2_5_test = test_data[test_data['AMB_TEMP'] == 'PM2.5'].iloc[:,2:]# pm2_5_test.info()1234# 对X_test进行一系列数据处理即可，不赘述# X_test = np.array(pm2_5_test, float)# X_test = np.insert(X_test, 0, values=np.ones((1, X_test.shape[0])), axis=1)X_test12345#预测y_star = np.dot(X_test, w)y_pre = pd.read_csv("sampleSubmission.csv")y_pre.value = y_stary_pre.to_csv('predict.csv', index=False)]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好！]]></title>
    <url>%2F2019%2F02%2F28%2F%E8%BF%99%E6%98%AF%E4%B8%80%E6%AC%A1%E5%B0%8F%E5%B0%8F%E7%9A%84%E5%B0%9D%E8%AF%95%EF%BC%81%2F</url>
    <content type="text"><![CDATA[那些都是很好很好的，可我偏偏不喜欢。]]></content>
  </entry>
</search>
