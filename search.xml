<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[排序算法小结]]></title>
    <url>%2F2019%2F08%2F02%2F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>算法学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homework 3 - Image Sentiment Classification]]></title>
    <url>%2F2019%2F04%2F11%2FHomework-3-Image-Sentiment-Classification%2F</url>
    <content type="text"><![CDATA[摘要李宏毅机器学习作业二，情感图片分类情感图片分类导入相关库123import pandas as pdimport numpy as npfrom matplotlib import pyplot as plt数据处理123df_train = pd.read_csv("train.csv")df_test = pd.read_csv("test.csv")df_train.info()1sentiment = ['angry','disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']123456789plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号fig = plt.figure(figsize=(6,4))ax = fig.add_subplot(111)# plt.hist(train_df['label'], 7)df_train['label'].value_counts().plot(kind='bar')plt.ylabel("人数")plt.show()df_train['label'].value_counts()12123456# 传入来的每张图片feature都是字符串X = df_train.feature.apply(lambda x : np.array(x.split()).astype(np.float32))X = np.array(X.map(lambda x: x.reshape(48,48,1)).values.tolist()) X = (X/255.0*0.99) + 0.001X.shape# (28709, 48, 48, 1)12345X_test = df_test.feature.apply(lambda x : np.array(x.split()).astype(np.float32))X_test = np.array(X_test.map(lambda x: x.reshape(48,48,1)).values.tolist()) X_test = (X_test/255.0*0.99) + 0.001X_test.shape# (7178, 48, 48, 1)123y = df_train.label.values.reshape(-1, 1)y.shape# (28709, 1)123456# 查看一张图片fig = plt.figure(figsize=(3, 3))ax = fig.subplots(1)ax.imshow(X[0].reshape(48, 48), cmap = 'gray')plt.xlabel(sentiment[int(y[0])])plt.show()开始训练123456789import tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, \ Flatten, BatchNormalization, InputLayer, Input, Activationfrom tensorflow.keras.optimizers import RMSpropfrom tensorflow.keras.models import Modelfrom tensorflow.keras.preprocessing.image import ImageDataGeneratorfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard1234# one-hoty = keras.utils.to_categorical(y, 7)y.shape# (28709, 7)1234from sklearn.model_selection import train_test_splitX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3)X_train.shape, X_valid.shape, y_train.shape, y_valid.shape# ((20096, 48, 48, 1), (8613, 48, 48, 1), (20096, 7), (8613, 7))12CNN1234567891011121314151617181920212223242526272829303132cnn_model = Sequential()# 48*48*1 -&gt; 48*48*64cnn_model.add(Conv2D(filters= 64, kernel_size=(5, 5), strides=1, padding='Same', activation='relu',input_shape=(48,48,1)))# -&gt;24*24*64 cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=2))cnn_model.add(BatchNormalization())cnn_model.add(Dropout(0.25)) # -&gt; 24*24*128cnn_model.add(Conv2D(filters= 128, kernel_size=(5,5), strides=1, padding='Same', activation='relu'))# -&gt;12*12*128cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=2))cnn_model.add(BatchNormalization())cnn_model.add(Dropout(0.25))# -&gt; 12*12*256cnn_model.add(Conv2D(filters= 256, kernel_size=(5,5), strides=1, padding='Same', activation='relu'))# -&gt;6*6*256cnn_model.add(MaxPooling2D(pool_size=(2,2), strides=2))cnn_model.add(BatchNormalization())cnn_model.add(Dropout(0.5))# -&gt;9216cnn_model.add(Flatten())cnn_model.add(BatchNormalization())# -&gt;128cnn_model.add(Dense(128, activation='relu')) # -&gt;7cnn_model.add(BatchNormalization())cnn_model.add(Dense(7, activation='softmax'))12batch_size = 64epochs = 100 # 1012# optimizer = RMSprop(lr = 0.001, decay=0.0)# optimizer = keras.optimizers.Adam()1234cnn_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])123456789101112131415161718# class_weight = 'auto', 用于处理skewed classes# 也可以model.compile(.... metrics=[Precision, Recall])# 或者# from sklearn.utils.class_weight import compute_class_weight# class_weight = compute_class_weight(class_weight='balanced',# classes=np.unique(train_data.label),# y=train_data.label)# model.fit(... class_weight=class_weight)# import os# os.environ["CUDA_VISIBLE_DEVICES"] = "0"cnn_result = cnn_model.fit(x = X_train, y = y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), verbose =2, class_weight = 'auto') # callbacks=[tensorboard]12345678910# 查看accfig,ax = plt.subplots(2,1,figsize=(10,10))ax[0].plot(cnn_result.history['loss'], label='Train Loss')ax[0].plot(cnn_result.history['val_loss'], label='Validation Loss')ax[1].plot(cnn_result.history['acc'], label='Train acc')ax[1].plot(cnn_result.history['val_acc'], label='Validation Acc')plt.legend()plt.show()DNN123456789101112131415161718192021222324#DNN modelinputs = Input(shape=(48,48,1))dnn = Flatten()(inputs)dnn = Dense(512)(dnn)dnn = BatchNormalization(axis = -1)(dnn)dnn = Activation('relu')(dnn)dnn = Dropout(0.25)(dnn)dnn = Dense(1024)(dnn)dnn = BatchNormalization(axis = -1)(dnn)dnn = Activation('relu')(dnn)dnn = Dropout(0.5)(dnn)dnn = Dense(512)(dnn)dnn = BatchNormalization(axis = -1)(dnn)dnn = Activation('relu')(dnn)dnn = Dropout(0.5)(dnn)dnn = Dense(7)(dnn)dnn = BatchNormalization(axis = -1)(dnn)dnn = Activation('softmax')(dnn)1234567outputs = dnndnn_model = Model(inputs=inputs, outputs=outputs)# tensorboard = TensorBoard(log_dir="logs/&#123;&#125;".format(time()))dnn_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])123456dnn_result = model.fit(x = X_train, y = y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_valid, y_valid), verbose =2, class_weight = 'auto')123456789fig,ax = plt.subplots(2,1,figsize=(10,10))ax[0].plot(dnn_result.history['loss'], label='Train Loss')ax[0].plot(dnn_result.history['val_loss'], label='Validation Loss')ax[1].plot(dnn_result.history['acc'], label='Train acc')ax[1].plot(dnn_result.history['val_acc'], label='Validation Acc')plt.legend()plt.show()保存h5文件12cnn_model.save('cnn.h5')dnn_model.save('dnn.h5')模型分析混淆矩阵绘制1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from sklearn.metrics import confusion_matrixdef plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues): """ This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. """ if not title: if normalize: title = 'Normalized confusion matrix' else: title = 'Confusion matrix, without normalization' # Compute confusion matrix cm = confusion_matrix(y_true, y_pred) # Only use the labels that appear in the data #classes = classes[unique_labels(y_true, y_pred)] if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print("Normalized confusion matrix") else: print('Confusion matrix, without normalization') print(cm) fig, ax = plt.subplots() im = ax.imshow(cm, interpolation='nearest', cmap=cmap) ax.figure.colorbar(im, ax=ax) # We want to show all ticks... ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), # ... and label them with the respective list entries xticklabels=classes, yticklabels=classes, title=title, ylabel='True label', xlabel='Predicted label') # Rotate the tick labels and set their alignment. plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor") # Loop over data dimensions and create text annotations. fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i in range(cm.shape[0]): for j in range(cm.shape[1]): ax.text(j, i, format(cm[i, j], fmt), ha="center", va="center", color="white" if cm[i, j] &gt; thresh else "black") fig.tight_layout() return ax1234567cnn_predict = cnn_model.predict(X_valid)cnn_cls = np.argmax(cnn_predict, axis=1)dnn_predict = dnn_model.predict(X_valid)dnn_cls = np.argmax(dnn_predict, axis=1)y_label = data = [np.argmax(one_hot)for one_hot in y_valid]1plot_confusion_matrix(y_label, cnn_cls, sentiment)1plot_confusion_matrix(y_label, dnn_cls, sentiment)错误图片查看1234true_cls = pd.Series(y_label, name='true_cls')[y_label!=cnn_cls]wrong_cls = pd.Series(cnn_cls, name='wrong_cls')[y_label!=cnn_cls]wrong = pd.concat([true_cls, wrong_cls], axis = 1)特征图查看卷积核的可视化参考文献http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.htmlhttps://github.com/maplezzz/NTU_ML2017_Hung-yi-Lee_HWhttps://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/5.4-visualizing-what-convnets-learn.ipynb]]></content>
      <categories>
        <category>李宏毅机器学习作业</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic: Machine Learning from Disaster]]></title>
    <url>%2F2019%2F04%2F06%2FTitanic-Machine-Learning-from-Disaster%2F</url>
    <content type="text"><![CDATA[Kaggle入坑题目数据处理导入基础库1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns读取并处理数据1234train_data = pd.read_csv('train.csv')test_data = pd.read_csv('test.csv')# train_data.columns.valuestrain_data.head()#查看前五行1train_data.info()#查看数据信息123456# 查看空值数目print('train_data:')print(train_data.isnull().sum())print("-"*20)print('test_data:')print(test_data.isnull().sum())train_data中891位乘客信息，其中属性Age，Cabin和Embarked有数据丢失。test_data中，属性Age，Fare和Cabin有数据丢失。对于缺少数据，使用年龄的中位数填补年龄空值，去除丢失Embarked，Fare属性的数据。对于Cabin属性，种类很多，观察其是否丢失与是否存活之间的关系，将Cabin属性分为是否丢失两类。1234567891011121314plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号fig = plt.figure() fig.set(alpha=0.2) # 设定图表颜色alpha参数 not_null_cabin = train_data.Survived[train_data.Cabin.isnull()].value_counts() null_cabin = train_data.Survived[train_data.Cabin.notnull()].value_counts() df = pd.DataFrame(&#123;'Cabin丢失':not_null_cabin , 'Cabin未丢失':null_cabin&#125;) df.plot(kind='bar', stacked=True) plt.xlabel("是否存活") plt.ylabel("人数") plt.show()# train_data.Cabin.value_counts()12345678910111213141516171819202122 # 使用年龄的中位数填补年龄空值train_data['Age'].fillna(train_data['Age'].median(), inplace = True)test_data['Age'].fillna(test_data['Age'].median(), inplace = True)# Cabin根据是否缺失分为两类train_data.loc[ (train_data.Cabin.notnull()), 'Cabin' ] = "Yes"train_data.loc[ (train_data.Cabin.isnull()), 'Cabin' ] = "No"test_data.loc[ (test_data.Cabin.notnull()), 'Cabin' ] = "Yes"test_data.loc[ (test_data.Cabin.isnull()), 'Cabin' ] = "No" # 去除丢失Embarked，Fare的数据train_data = train_data.dropna()# test_data补充test_data.loc[ (test_data.Fare.isnull()), 'Fare' ] = test_data['Fare'].median()# 去除无关数据PassengerId和Tickettrain_data.drop(['PassengerId', 'Ticket'], axis=1, inplace = True)test_id = test_data.PassengerIdtest_data.drop(['PassengerId', 'Ticket'], axis=1, inplace = True)train_data.info()# test_data.info()进一步处理数据12345678910111213141516171819202122232425262728293031323334def clean_data(dataset): # 新建家庭大小属性：堂兄弟/妹个数 + 父母与小孩个数 dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1 # 是否仅有自身一人，1：是，0：否 dataset['IsAlone'] = 1 dataset['IsAlone'].loc[dataset['FamilySize'] &gt; 1] = 0 # 分离出称谓 Mr/Mrs/Miss/Master... dataset['Title'] = dataset['Name'].str.split(", ", expand=True)[1].str.split(".", expand=True)[0] # 比起把Fare和Age当作特征列，将这些列的值进行二进制转换更为合理。 # Scikit-Learn 开发了新的估计器 KBinsDiscretizer 来执行这一操作。 # 它不仅将这些值转换为二进制码，还会对其进行编码。 # 也可以通过 Pandas 的 cut 和 qcut 函数手动完成这个过程。 # qcut据这些值的频率来选择箱子的均匀间隔，即每个箱子中含有的数的数量是相同的 dataset['FareBin'] = pd.qcut(dataset['Fare'], 4) # cut将根据值本身来选择箱子均匀间隔，即每个箱子的间距都是相同的 dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)clean_data(train_data)clean_data(test_data) # print(train_data['Title'].value_counts())stat_min = 10 # 判断某一称谓人数是否大于10title_names = (train_data['Title'].value_counts() &lt; stat_min) # 判称谓人数小于10的转换为Misctrain_data['Title'] = train_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)print(train_data['Title'].value_counts())12345678# print(train_data['Title'].value_counts())stat_min = 10 # 判断某一称谓人数是否大于10title_names = (test_data['Title'].value_counts() &lt; stat_min) # 判称谓人数小于10的转换为Misctest_data['Title'] = test_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)print(test_data['Title'].value_counts())123456789101112# from sklearn.preprocessing import OneHotEncoder, LabelEncoder# LabelEncoder()对不连续的数字或文本编号# 这里使用one-hotdummy = ['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Title', 'Embarked', 'FamilySize', 'IsAlone','FareBin', 'AgeBin'] Target = ['Survived']train_dummy = pd.get_dummies(train_data[dummy])test_dummy = pd.get_dummies(test_data[dummy])train_dummy.columns.values1train_dummy = train_dummy.drop(['Age', 'Fare'], axis=1)123456from sklearn import model_selectiontrain_x_dummy, valid_x_dummy, train_y, valid_y = model_selection.train_test_split \ (train_dummy, train_data[Target], random_state = 0)train_x_dummy.shape, valid_x_dummy.shape, train_y.shape, valid_y.shape# ((666, 26), (223, 26), (666, 1), (223, 1))进行数据分析12345678for x in ['Pclass', 'Sex','SibSp', 'Parch', 'Cabin', 'Embarked', \ 'FamilySize', 'IsAlone', 'Title']: # 打印和Survived相关的属性，'Age'和'Fare' 除外 print('Survival Correlation by:', x) print(train_data[[x, "Survived"]].groupby(x, as_index=False).mean()) print('-'*10, '\n')# print(pd.crosstab(train_x_dummy.Pclass, train_y.Survived))12345678910111213141516171819202122232425262728293031323334# plt.figure(figsize=[18,10])# plt.subplot(231)# plt.boxplot(x=train_data['Fare'], showmeans = True, meanline = True)# plt.subplot(232)# plt.boxplot(train_data['Age'], showmeans = True, meanline = True)# plt.subplot(233)# plt.boxplot(train_data['FamilySize'], showmeans = True, meanline = True)# plt.subplot(234)# plt.hist(x = [train_data[train_data['Survived']==1]['Fare'], \# train_data[train_data['Survived']==0]['Fare']], # stacked=True, label = ['Survived','Dead'])# plt.subplot(235)# plt.hist(x = [train_data[train_data['Survived']==1]['Age'], \# train_data[train_data['Survived']==0]['Age']], # stacked=True, label = ['Survived','Dead'])# plt.subplot(236)# plt.hist(x = [train_data[train_data['Survived']==1]['FamilySize'], \# train_data[train_data['Survived']==0]['FamilySize']], # stacked=True, label = ['Survived','Dead'])fig, saxis = plt.subplots(2,3,figsize=(18,10))sns.boxplot(y = 'Fare', hue = 'Survived', data = train_data, ax = saxis[0,0])sns.boxplot(y = 'Age', hue = 'Survived', data = train_data, ax = saxis[0,1])sns.boxplot(y = 'FamilySize', hue = 'Survived', data = train_data, ax = saxis[0,2])sns.barplot(x = 'FareBin', y = 'Survived', data = train_data, ax = saxis[1,0])sns.barplot(x = 'AgeBin', y = 'Survived', data = train_data, ax = saxis[1,1])sns.barplot(x = 'FamilySize', y = 'Survived', data = train_data, ax = saxis[1,2])123456789fig, saxis = plt.subplots(2, 3,figsize=(18,10))sns.barplot(x = 'Embarked', y = 'Survived', data=train_data, ax = saxis[0,0])sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=train_data, ax = saxis[0,1])sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=train_data, ax = saxis[0,2])sns.pointplot(x = 'FareBin', y = 'Survived', data=train_data, ax = saxis[1,0])sns.pointplot(x = 'AgeBin', y = 'Survived', data=train_data, ax = saxis[1,1])sns.pointplot(x = 'FamilySize', y = 'Survived', data=train_data, ax = saxis[1,2])12345fig, saxis = plt.subplots(1,3,figsize=(18,5))sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=train_data, ax = saxis[0])sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=train_data, ax = saxis[1])sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=train_data, ax = saxis[2])12345678910fig, saxis = plt.subplots(1, 2,figsize=(12,5))sns.pointplot(x="FamilySize", y="Survived", hue="Sex", data=train_data, palette=&#123;"male": "blue", "female": "pink"&#125;, markers=["*", "o"], linestyles=["-", "--"], ax = saxis[0])sns.pointplot(x="Pclass", y="Survived", hue="Sex", data=train_data, palette=&#123;"male": "blue", "female": "pink"&#125;, markers=["*", "o"], linestyles=["-", "--"], ax = saxis[1])123e = sns.FacetGrid(train_data, col = 'Embarked')e.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')e.add_legend()1234e = sns.FacetGrid(train_data, hue = 'Survived', aspect=4 )e.map(sns.kdeplot, 'Age', shade= True )e.set(xlim=(0 , train_data['Age'].max()))e.add_legend()123e = sns.FacetGrid(train_data, row = 'Sex', col = 'Pclass', hue = 'Survived')e.map(plt.hist, 'Age', alpha = .75)e.add_legend()12e = sns.pairplot(train_data, hue = 'Survived', palette = 'deep', height=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )e.set(xticklabels=[])12345678910111213141516171819# 热力图def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) # 创建分散颜色 _ = sns.heatmap( df.corr(), cmap = colormap, square=True, # 设置热力图矩阵小块形状，默认值是False cbar_kws=&#123;'shrink':.9 &#125;, # 热力图侧边绘制颜色刻度条时，相关字体设置，默认值是None ax=ax, annot=True, # 在热力图每个方格写入数据；如果是矩阵，在热力图每个方格写入该矩阵对应位置数据，默认值是False linewidths=0.1,vmax=1.0, linecolor='white', # vmax热力图的颜色取值最大范围 annot_kws=&#123;'fontsize':12 &#125; ) plt.title('Pearson Correlation of Features', y=1.05, size=15)correlation_heatmap(train_data)选择MLA进行训练1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sklearn import ensemble, gaussian_process, linear_model, naive_bayes, \ neighbors, svm, tree, discriminant_analysisfrom xgboost import XGBClassifierfrom sklearn.linear_model import stochastic_gradientMLA = [ # 集成方法 ensemble.AdaBoostClassifier(), ensemble.BaggingClassifier(), ensemble.ExtraTreesClassifier(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(), # 高斯过程 gaussian_process.GaussianProcessClassifier(), # 广义线性模型 linear_model.LogisticRegressionCV(), linear_model.PassiveAggressiveClassifier(max_iter=5), linear_model.RidgeClassifierCV(), stochastic_gradient.SGDClassifier(max_iter=5), linear_model.Perceptron(max_iter=5), # 朴素贝叶斯 naive_bayes.BernoulliNB(), naive_bayes.GaussianNB(), #邻近算法 neighbors.KNeighborsClassifier(), # 支持向量机 svm.SVC(probability=True), svm.NuSVC(probability=True), svm.LinearSVC(), # 树 tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier(), # 判别分析 discriminant_analysis.LinearDiscriminantAnalysis(), discriminant_analysis.QuadraticDiscriminantAnalysis(), # xgboost XGBClassifier() ]123456789101112131415161718192021222324252627282930313233343536from sklearn.preprocessing import StandardScaler# X = pd.concat([train_x_dummy,valid_x_dummy])# y = pd.concat([train_y,valid_y])# 防止线性相关X = train_dummy.drop(['Sex_male', 'Cabin_No'], axis=1)y = train_data[Target]# 标准化scaler = StandardScaler()scaler.fit(X) X = scaler.fit_transform(X)MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', \ 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']MLA_compare = pd.DataFrame(columns = MLA_columns)cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )row_index = 0for alg in MLA: MLA_name = alg.__class__.__name__ MLA_compare.loc[row_index, 'MLA Name'] = MLA_name MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params()) cv_results = model_selection.cross_validate(alg, X, y.values.ravel(), cv = cv_split, return_train_score=True) MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean() MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3 row_index+=1MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)MLA_compare123456sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare)plt.title('Machine Learning Algorithm Accuracy Score \n')plt.xlabel('Accuracy Score (%)')plt.ylabel('Algorithm')plt.show()12345678910111213141516171819202122232425262728vote_est = [ # Ensemble Methods ('ada', ensemble.AdaBoostClassifier()), ('bc', ensemble.BaggingClassifier()), ('etc',ensemble.ExtraTreesClassifier()), ('gbc', ensemble.GradientBoostingClassifier()), ('rfc', ensemble.RandomForestClassifier()), # Gaussian Processes ('gpc', gaussian_process.GaussianProcessClassifier()), # GLM ('lr', linear_model.LogisticRegressionCV()), # Navies Bayes ('bnb', naive_bayes.BernoulliNB()), ('gnb', naive_bayes.GaussianNB()), # Nearest Neighbor ('knn', neighbors.KNeighborsClassifier()), # SVM ('svc', svm.SVC(probability=True)), # xgboost ('xgb', XGBClassifier())]1234567891011121314151617181920212223import warningswarnings.filterwarnings("ignore")# Hard Votevote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')vote_hard_cv = model_selection.cross_validate(vote_hard, X, y.values.ravel(), cv = cv_split)vote_hard.fit(X, y.values.ravel())print("Hard Voting Training w/bin score mean: &#123;:.2f&#125;". format(vote_hard_cv['train_score'].mean()*100)) print("Hard Voting Test w/bin score mean: &#123;:.2f&#125;". format(vote_hard_cv['test_score'].mean()*100))print("Hard Voting Test w/bin score 3*std: +/- &#123;:.2f&#125;". format(vote_hard_cv['test_score'].std()*100*3))print('-'*10)#Soft Vote or weighted probabilitiesvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')vote_soft_cv = model_selection.cross_validate(vote_soft, X, y.values.ravel(), cv = cv_split)vote_soft.fit(X, y.values.ravel())print("Soft Voting Training w/bin score mean: &#123;:.2f&#125;". format(vote_soft_cv['train_score'].mean()*100)) print("Soft Voting Test w/bin score mean: &#123;:.2f&#125;". format(vote_soft_cv['test_score'].mean()*100))print("Soft Voting Test w/bin score 3*std: +/- &#123;:.2f&#125;". format(vote_soft_cv['test_score'].std()*100*3))print('-'*10)进行网格调参123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136import timegrid_n_estimator = [10, 50, 100, 300]grid_ratio = [.1, .25, .5, .75, 1.0]grid_learn = [.01, .03, .05, .1, .25]grid_max_depth = [2, 4, 6, 8, 10, None]grid_min_samples = [5, 10, .03, .05, .10]grid_criterion = ['gini', 'entropy']grid_bool = [True, False]grid_seed = [0]grid_param = [ [&#123; #AdaBoostClassifier 'n_estimators': grid_n_estimator, #default=50 'learning_rate': grid_learn, #default=1 #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R 'random_state': grid_seed &#125;], [&#123; #BaggingClassifier 'n_estimators': grid_n_estimator, #default=10 'max_samples': grid_ratio, #default=1.0 'random_state': grid_seed &#125;], [&#123; #ExtraTreesClassifier 'n_estimators': grid_n_estimator, #default=10 'criterion': grid_criterion, #default=”gini” 'max_depth': grid_max_depth, #default=None 'random_state': grid_seed &#125;], [&#123; #GradientBoostingClassifier #'loss': ['deviance', 'exponential'], #default=’deviance’ 'learning_rate': [.05], 'n_estimators': [300], #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse” 'max_depth': grid_max_depth, #default=3 'random_state': grid_seed &#125;], [&#123; #RandomForestClassifier 'n_estimators': grid_n_estimator, #default=10 'criterion': grid_criterion, #default=”gini” 'max_depth': grid_max_depth, #default=None 'oob_score': [True], #default=False 'random_state': grid_seed &#125;], [&#123; #GaussianProcessClassifier 'max_iter_predict': grid_n_estimator, #default: 100 'random_state': grid_seed &#125;], [&#123; #LogisticRegressionCV 'fit_intercept': grid_bool, #default: True #'penalty': ['l1','l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs 'random_state': grid_seed &#125;], [&#123; #BernoulliNB 'alpha': grid_ratio, #default: 1.0 &#125;], #GaussianNB [&#123;&#125;], [&#123; #KNeighborsClassifier 'n_neighbors': [1,2,3,4,5,6,7], #default: 5 'weights': ['uniform', 'distance'], #default = ‘uniform’ 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'] &#125;], [&#123; #SVC #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'C': [1,2,3,4,5], #default=1.0 'gamma': grid_ratio, #edfault: auto 'decision_function_shape': ['ovo', 'ovr'], #default:ovr 'probability': [True], 'random_state': grid_seed &#125;], [&#123; #XGBClassifier 'learning_rate': grid_learn, #default: .3 'max_depth': [1,2,4,6,8,10], #default 2 'n_estimators': grid_n_estimator, 'seed': grid_seed &#125;] ]start_total = time.perf_counter() for clf, param in zip (vote_est, grid_param): #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm #print(param) start = time.perf_counter() best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc') best_search.fit(X, y.values.ravel()) run = time.perf_counter() - start best_param = best_search.best_params_ print('The best parameter for &#123;&#125; is &#123;&#125; with a runtime of &#123;:.2f&#125; seconds.'\ .format(clf[1].__class__.__name__, best_param, run)) clf[1].set_params(**best_param) run_total = time.perf_counter() - start_totalprint('Total optimization time was &#123;:.2f&#125; minutes.'.format(run_total/60))print('-'*10)1234567891011121314151617181920212223242526# Hard Vote or majority rules w/Tuned Hyperparametersgrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')grid_hard_cv = model_selection.cross_validate(grid_hard, X, y.values.ravel(), cv = cv_split)grid_hard.fit(X, y.values.ravel())print("Hard Voting w/Tuned Hyperparameters Training w/bin score mean: &#123;:.2f&#125;"\ . format(grid_hard_cv['train_score'].mean()*100)) print("Hard Voting w/Tuned Hyperparameters Test w/bin score mean: &#123;:.2f&#125;"\ . format(grid_hard_cv['test_score'].mean()*100))print("Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- &#123;:.2f&#125;"\ . format(grid_hard_cv['test_score'].std()*100*3))print('-'*10)#Soft Vote or weighted probabilities w/Tuned Hyperparametersgrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')grid_soft_cv = model_selection.cross_validate(grid_soft, X, y.values.ravel(), cv = cv_split)grid_soft.fit(X, y.values.ravel())print("Soft Voting w/Tuned Hyperparameters Training w/bin score mean: &#123;:.2f&#125;"\ .format(grid_soft_cv['train_score'].mean()*100)) print("Soft Voting w/Tuned Hyperparameters Test w/bin score mean: &#123;:.2f&#125;"\ . format(grid_soft_cv['test_score'].mean()*100))print("Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- &#123;:.2f&#125;"\ . format(grid_soft_cv['test_score'].std()*100*3))print('-'*10)保存预测结果1234X_test = test_dummy.drop(['Sex_male', 'Cabin_No'], axis=1).valuesy_test = grid_hard.predict(X_test)f = pd.DataFrame(&#123;'PassengerId':test_id.values, 'Survived':y_test&#125;)f.to_csv("C:/Users/DHX17/Jupyter/Kaggle/titanic/ans.csv", index=False)参考资料https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MNIST 手写数字识别]]></title>
    <url>%2F2019%2F03%2F31%2FMNIST-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[摘要使用神经网络进行MNIST手写数字识别《Python神经网络编程》代码导入相关库123import numpy as npfrom scipy.special import expitimport matplotlib.pyplot as plt搭建神经网络12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class neuralNetwork: def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate): self.inodes = inputnodes self.hnodes = hiddennodes self.onodes = outputnodes self.lr = learningrate #随机初始化权重，当然权重在0到1之间（不含）。这里通过高斯函数产生权重 #其中均值是零，方差是1/sqrt(连接数)，当然连接数等于节点数。后一个是矩阵hnodes行， # numpy.random.normal(loc=0.0, scale=1.0, size=None) self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes)) self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes)) self.activation_function = lambda x: expit(x) pass def train(self,inputs_list, targets_list): inputs = np.array(inputs_list, ndmin=2).T targets = np.array(targets_list, ndmin=2).T hidden_inputs = np.dot(self.wih, inputs) hidden_outputs = self.activation_function(hidden_inputs) final_inputs = np.dot(self.who,hidden_outputs) final_outputs = self.activation_function(final_inputs) #误差 output_errors = targets - final_outputs hidden_errors = np.dot(self.who.T, output_errors) #权重更新 self.who += self.lr*np.dot((output_errors*final_outputs*(1.0 - final_outputs)), np.transpose(hidden_outputs)) self.wih += self.lr*np.dot((hidden_errors*hidden_outputs*(1.0-hidden_outputs)), np.transpose(inputs)) pass def query(self, inputs_list): inputs = np.array(inputs_list, ndmin=2).T hidden_inputs = np.dot(self.wih, inputs) hidden_outputs = self.activation_function(hidden_inputs) final_inputs = np.dot(self.who, hidden_outputs) final_outputs = self.activation_function(final_inputs) return final_outputs数据处理1234567input_nodes = 784 hidden_nodes = 100 output_nodes = 10 learning_rate = 0.3n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)1234#导入训练数据 training_data_file = open('./mnist_train_100.csv','r') training_data_list = training_data_file.readlines() training_data_file.close()1training_data_list[0]123all_values = training_data_list[0].split(',')image_array = np.asfarray(all_values[1:]).reshape((28, 28))plt.imshow(image_array, cmap='Greys', interpolation='None')进行数据训练123456789for record in training_data_list: all_values = record.split(',') #输入数据数学处理，使其在0.01到1之间.颜色的范围是[0,255] inputs = (np.asfarray(all_values[1:])/255.0*0.99)+0.01 #初始化目标值，使其在0.01到0.99之间 targets = np.zeros(output_nodes) + 0.01 targets[int(record[0])] = 0.99 n.train(inputs,targets) pass123test_data_file = open('./mnist_test_10.csv','r') test_data_list = test_data_file.readlines() test_data_file.close()12345678all_values = test_data_list[0].split(',')correct_label = int(all_values[0])image_array = np.asfarray(all_values[1:]).reshape((28,28))plt.imshow(image_array,cmap='Greys')inputs = (np.asfarray(all_values[1:])/255.0*0.99) + 0.01outputs = n.query(inputs)label = np.argmax(outputs) #argmax返回最大值的索引值print("correct:&#123;0&#125;, predict:&#123;1&#125;".format(correct_label, label))1234567891011121314151617181920scorecard = []#多个数据检测for record in test_data_list: all_values = record.split(',') correct_label = int(all_values[0]) print(correct_label, "correct label") inputs = (np.asfarray(all_values[1:])/255.0*0.99) + 0.01 outputs = n.query(inputs) label = np.argmax(outputs) print(label, "network's answer") if(label == correct_label): scorecard.append(1) else: scorecard.append(0) print(scorecard) # [1, 0, 1, 1, 1, 1, 1, 0, 0, 0]123scorecard_array = np.asarray(scorecard)print("performance = ", scorecard_array.sum() / scorecard_array.size)# performance = 0.6一些改进调整学习率多次运行改变网络形状基于Keras导入相关库12345678910111213141516import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as sn# sklearn的mnist数据集是8x8# from sklearn.datasets import load_digits# from sklearn.preprocessing import LabelBinarizer# from sklearn.model_selection import train_test_split# from sklearn.metrics import confusion_matriximport kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flattenfrom keras.optimizers import RMSpropfrom keras.preprocessing.image import ImageDataGeneratorfrom keras.callbacks import ModelCheckpoint处理数据1234567891011121314151617181920212223242526# 导入数据# digits = load_digits()# X = digits.data# y = digits.target# X /= 8# y = LabelBinarizer().fit_transform(y)# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)# X_train.shape, X_test.shape, y_train.shape, y_test.shape# (X_train, y_train), (X_test, y_test) = mnist.load_data()path = 'datasets/mnist.npz' f = np.load(path)X_train, y_train = f['x_train'], f['y_train']X_test, y_test = f['x_test'], f['y_test']f.close()X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)X_test = X_test.reshape(X_test.shape[0], 28, 28, 1) X_train = X_train.astype('float32')X_test = X_test.astype('float32') X_train /= 255 X_test /= 255X_train.shape, X_test.shape# ((60000, 28, 28, 1), (10000, 28, 28, 1))1234y_train = keras.utils.to_categorical(y_train, 10)y_test = keras.utils.to_categorical(y_test, 10)y_train.shape, y_test.shape# ((60000, 10), (10000, 10))12plt.imshow(X_train[0].reshape(28, 28), cmap='Greys', interpolation='None')y_train[0]搭建神经网络12345678910111213141516171819model = Sequential() # 使用3x3的卷积核，激活函数为ReLU# 池化核大小2x2model.add(Conv2D(filters=28, kernel_size=(3, 3), padding='Same', activation='relu',input_shape=(28,28,1)))model.add(MaxPooling2D(pool_size=(2,2), strides=1))model.add(Dropout(0.25)) model.add(Conv2D(filters=64, kernel_size=(3,3), padding='Same', activation='relu'))model.add(MaxPooling2D(pool_size=(2,2), strides=1))model.add(Dropout(0.25))# Fully connected layer.model.add(Flatten())model.add(Dense(256, activation='relu')) model.add(Dropout(0.25)) #10 outputsmodel.add(Dense(10, activation='softmax'))12batch_size = 250epochs = 1012optimizer = RMSprop(lr = 0.001, decay=0.0)# optimizer = keras.optimizers.Adam()1234model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])12345678910111213141516171819# reduce_lr = LearningRateScheduler(function)reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, #每次减少学习率的因子，学习率将以lr = lr*factor的形式被减少 patience=10, # 当patience个epoch过去而模型性能不提升时，学习率减少的动作会被触发 verbose=0, mode='auto', # 在min模式下，如果检测值触发学习率减少。在max模式下，当检测值不再上升则触发学习率减少。 min_delta=0.0001, # 用来确定是否进入检测值的“平原区” cooldown=0, # 学习率减少后，会经过cooldown个epoch才重新进行正常操作 min_lr=0.00001)gen = ImageDataGenerator(# featurewise_center=True,对输入的图片每个通道减去每个通道对应均值 # featurewise_std_normalization=True,每张图片减去样本均值, 使得每个样本均值为0 rotation_range=20, # 旋转范围 zoom_range= 0.2, # 缩放范围 width_shift_range=0.2, # 水平平移范围 height_shift_range=0.2, # 垂直平移范围 horizontal_flip=True) #水平反转train_generator = gen.flow(X_train, y_train, batch_size=batch_size)123456result = model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs= epochs, validation_data=(X_test, y_test), verbose =2, callbacks=[reduce_lr])12345678910fig,ax = plt.subplots(2,1,figsize=(10,10))ax[0].plot(result.history['loss'], label='Train Loss')ax[0].plot(result.history['val_loss'], label='Validation Loss')ax[1].plot(result.history['acc'], label='Train acc')ax[1].plot(result.history['val_acc'], label='Validation Acc')plt.legend()plt.show()参考文献《Python神经网络编程》代码https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homework 2 - Income Prediction]]></title>
    <url>%2F2019%2F03%2F29%2FHomework-2-Income-Prediction%2F</url>
    <content type="text"><![CDATA[摘要李宏毅机器学习作业二，收入预测收入预测导入相关库123456import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom random import shufflefrom math import floor, logfrom numpy.linalg import inv数据处理12train_data = pd.read_csv("train.csv")train_data.info()1train_data.head()对数据进行可视化观察12345678910111213141516171819202122232425262728293031323334353637383940plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签plt.rcParams['axes.unicode_minus']=False #用来正常显示负号fig = plt.subplots(figsize=(16,8),dpi=80)plt.figure(1)ax1 = plt.subplot(231)train_data.income.value_counts().plot(kind='bar')plt.xlabel("收入情况")plt.ylabel("人数") ax2 = plt.subplot(232)train_data.education.value_counts().plot(kind='bar')plt.ylabel("人数")# #plt.xticks([1, 2, 3])plt.xlabel("教育情况")ax3 = plt.subplot(233)plt.hist(train_data.age,20)# #plt.yticks([0, 1])plt.ylabel('人数')plt.xlabel('年龄')ax4 = plt.subplot(234)train_data.sex.value_counts().plot(kind='bar')plt.ylabel("人数")plt.xlabel('性别')ax5 = plt.subplot(235)train_data.workclass.value_counts().plot(kind='bar')plt.ylabel("人数") plt.xlabel("所属企业类型")ax6 = plt.subplot(236)train_data.race.value_counts().plot(kind='bar')plt.ylabel("人数")plt.xlabel("种族")# 调整每隔子图之间的距离 plt.tight_layout()plt.show()1234567# 将&gt;50K转为1，&lt;=50K转为0， 方便数据可视化操作# 使用train_data.income[train_data.income == " &gt;50K"] = 1# 会弹出A value is trying to be set on a copy of a slice from a DataFrame.# 修改数据最好不要使用链式操作train_data.loc[train_data.income == " &gt;50K", 'income'] = 1train_data.loc[train_data.income ==" &lt;=50K", 'income'] = 0train_data.head()12345678910fig = plt.figure() fig.set(alpha=0.2) male = train_data.income[train_data.sex == ' Male'].value_counts() female = train_data.income[train_data.sex == ' Female'].value_counts() df=pd.DataFrame(&#123;'male':male, 'female':female&#125;) df.plot(kind='bar', stacked=True) plt.xlabel("收入情况") plt.ylabel("人数") plt.show()train_data[["sex", "income"]].groupby(['sex'], as_index=False).mean().sort_values(by='income', ascending=False)查看学历对应的收入情况12345678910111213141516171819202122232425262728293031323334fig = plt.subplots(figsize=(16,8),dpi=80)plt.figure(1)ax1 = plt.subplot(231) train_data.income[train_data.education == ' HS-grad'].value_counts().plot(kind='bar', label=" HS-grad", color='red') ax1.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax1.legend(["HS研究生学位"], loc='best') ax2 = plt.subplot(232) train_data.income[train_data.education == ' Doctorate'].value_counts().plot(kind='bar', label=" Doctorate", color='lightblue') ax2.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax2.legend(["博士学位"], loc='best') ax3 = plt.subplot(233) train_data.income[train_data.education == ' Masters'].value_counts().plot(kind='bar', label=" Masters", color='blue') ax3.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax3.legend(["硕士学位"], loc='best') ax4 = plt.subplot(234) train_data.income[train_data.education == ' Bachelors'].value_counts().plot(kind='bar', label=" Bachelors", color='pink') ax4.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax4.legend(["学士学位"], loc='best') ax5 = plt.subplot(235) train_data.income[train_data.education == ' Assoc-voc'].value_counts().plot(kind='bar', label=" Assoc-voc", color='steelblue') ax5.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax5.legend(["副学士学位"], loc='best') ax6 = plt.subplot(236) train_data.income[train_data.education == ' Some-college'].value_counts().plot(kind='bar', label=" Some-college", color='#FA2479') ax6.set_xticklabels(["&gt;50K", "&lt;=50K"], rotation=0) ax6.legend(["本科生学位"], loc='best') plt.show()12# 各国家人数train_data.native_country.value_counts()将所有含有缺失值的行都去掉，可以使用RandomForestRegressor填补缺失12345678910111213141516171819202122232425262728# from sklearn.ensemble import RandomForestRegressor# 使用RandomForestRegressor填补缺失的年龄属性# def set_missing_workclass(df):# # 把已有的数值型特征取出来丢进RandomForestRegressor中# workclass_df = df[['workclass','age', 'education', 'race', 'income']]## # 提取未知值和已知值# known_workclass = workclass_df[workclass_df.workclass.notnull()].values# unknown_workclass = workclass_df[workclass_df.workclass.isnull()].values # # y即目标workclass# y = known_workclass[:, 0] # # X即特征属性值# X = known_workclass[:, 1:] # # fit到RandomForestRegressor之中# rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)# rfr.fit(X, y) # # 用得到的模型进行未知年龄结果预测# predictedAges = rfr.predict(unknown_workclass[:, 1::]) # # 用得到的预测结果填补原缺失数据# df.loc[ (df.workclass.isnull()), 'workclass' ] = predictedAges # return df, rfr# train_data, rfr = set_missing_ages(train_data)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def dataProcess_X(raw_data): if "income" in raw_data.columns: # data = raw_data.drop(["sex", 'income'], axis=1) data = raw_data.drop(["income"], axis=1) else: # data = raw_data.drop(["sex"], axis=1) pass # 处理数据无效字符 data_clean = data.replace(regex=[r'\?|\.|\$'], value=np.nan) data = data_clean.dropna(how='any') # data_clean.isnull().any() # 剔除没有用的数据特征fnlwgt # fnlwgt: 连续性数值变量；人口普查员认为观察值的人数 data.drop(['fnlwgt'], axis=1) # 处理sex data.loc[data.sex == " Male", 'sex'] = 1 data.loc[data.sex ==" Female", 'sex'] = 0 listObjectColumn = [col for col in data.columns if data[col].dtypes == "object"] #读取非数字的column listNonObjedtColumn = [x for x in list(data) if x not in listObjectColumn] ObjectData = data[listObjectColumn] NonObjectData = data[listNonObjedtColumn] # NonObjectData.insert(0 ,"sex", (raw_data["sex"] == " Female").astype(np.int)) # 使用pd.get_dummies()特征因子化 # 也可以使用one-hot # from sklearn.feature_extraction import DictVectorizer # dict_vect=DictVectorizer(sparse=False) # X_train=dict_vect.fit_transform(X_train.to_dict(orient='record')) # X_test=dict_vect.transform(X_test.to_dict(orient='record')) # dict_vect.feature_names_ ObjectData = pd.get_dummies(ObjectData) data = pd.concat([NonObjectData, ObjectData], axis=1) X = data.astype("int64") # 标准化 X = (X - X.mean()) / X.std() return np.array(X)def dataProcess_y(raw_data): data = raw_data.copy() # 处理数据无效字符 data_clean = data.replace(regex=[r'\?|\.|\$'],value=np.nan) data = data_clean.dropna(how='any') # data_clean.isnull().any() try: # y = data['income'] # y = pd.DataFrame((y ==' &gt;50K').astype("int64"), columns=["income"]) data.loc[data.income == " &gt;50K", 'income'] = 1 data.loc[data.income == " &lt;=50K", 'income'] = 0 except: pass y = np.array(data['income']) y = y.reshape(y.shape[0], 1) return yX_train = dataProcess_X(train_data)y_train = dataProcess_y(train_data)X_train.shape, y_train.shape# ((30162, 103), (30162, 1))导入测试集12345test_data = pd.read_csv("test.csv")X_test = dataProcess_X(train_data)y_test = dataProcess_y(train_data)X_test.shape, y_test.shape# ((30162, 103), (30162, 1))12345678910111213141516171819202122def _shuffle(X, y): #X and Y are np.array randomize = np.arange(X.shape[0]) np.random.shuffle(randomize) return (X[randomize], y[randomize])def split_valid_set(X, y, percentage): all_size = X.shape[0] valid_size = int(floor(all_size * percentage)) X, y = _shuffle(X, y) X_valid, y_valid = X[ : valid_size], y[ : valid_size] X_train, y_train = X[valid_size:], y[valid_size:] return X_train, y_train, X_valid, y_valid# 也可用sklearn函数打散数据# from sklearn.model_selection import train_test_split# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, random_state=1)X_train, y_train, X_valid, y_valid = split_valid_set(X_train, y_train, 0.1)X_train.shape, y_train.shape, X_valid.shape, y_valid.shape# ((27146, 103), (27146, 1), (3016, 103), (3016, 1))Generative方法123456789101112131415161718192021222324252627282930313233343536373839404142def train(X_train, y_train): # vaild_set_percetange = 0.1 # X_train, Y_train, X_valid, Y_valid = split_valid_set(X, Y, vaild_set_percetange) #Gussian distribution parameters train_data_size = X_train.shape[0] cnt1 = 0 cnt2 = 0 mu1 = np.zeros((1, X_train.shape[1])) mu2 = np.zeros((1, X_train.shape[1])) for i in range(train_data_size): if y_train[i] == 1: # &gt;50k mu1 += X_train[i] cnt1 += 1 else: mu2 += X_train[i] cnt2 += 1 mu1 /= cnt1 mu2 /= cnt2 sigma1 = np.zeros((X_train.shape[1], X_train.shape[1])) sigma2 = np.zeros((X_train.shape[1], X_train.shape[1])) for i in range(train_data_size): if y_train[i] == 1: sigma1 += np.dot(np.transpose(X_train[i].reshape(1,103) - mu1), X_train[i] - mu1) else: sigma2 += np.dot(np.transpose(X_train[i].reshape(1,103) - mu2), X_train[i] - mu2) sigma1 /= cnt1 sigma2 /= cnt2 shared_sigma = (float(cnt1) / train_data_size) * sigma1 + (float(cnt2) / train_data_size) * sigma2 N1 = cnt1 N2 = cnt2 return mu1, mu2, shared_sigma, N1, N2mu1, mu2, shared_sigma, N1, N2 = train(X_train, y_train)mu1.shape, mu2.shape, shared_sigma.shape, N1, N212345def sigmoid(z): res = 1 / (1.0 + np.exp(-z)) return np.clip(res, 1e-8, (1-(1e-8)))# from scipy.special import expit1234567891011121314151617def valid(funname, X, Y, mu1, mu2, shared_sigma, N1, N2): sigma_inv = inv(shared_sigma) w = np.dot((mu1-mu2), sigma_inv) X_t = X.T b = (-0.5) * np.dot(np.dot(mu1, sigma_inv), mu1.T) + (0.5) * np.dot(np.dot(mu2, sigma_inv), mu2.T) + np.log(float(N1)/N2) a = np.dot(w,X_t) + b y = sigmoid(a) y_ = np.around(y) result = (np.squeeze(Y) == y_) print(f'&#123;funname&#125; acc = %f' % (float(result.sum()) / X.shape[0]))valid("train", X_train, y_train, mu1, mu2, shared_sigma, N1, N2)valid("valid", X_valid, y_valid, mu1, mu2, shared_sigma, N1, N2)valid("test", X_test, y_test, mu1, mu2, shared_sigma, N1, N2)# train acc = 0.837140# valid acc = 0.851459# test acc = 0.838572Discriminative方法mini_batch1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# mini_batch def train(X_train, y_train): w = np.zeros((1, len(X_train[0]))) l_rate = 0.0001 batch_size = 32 train_dataz_size = len(X_train) step_num = int(floor(train_dataz_size / batch_size)) epoch_num = 300 list_cost = [] total_loss = 0.0 for epoch in range(1, epoch_num): total_loss = 0.0 X_train, y_train = _shuffle(X_train, y_train) for idx in range(1, step_num): X = X_train[idx*batch_size:(idx+1)*batch_size] #32*104 Y = y_train[idx*batch_size:(idx+1)*batch_size] #32*1 s_grad = np.zeros((1,len(X[0]))) z = np.dot(X, w.T) # 32*104*104*1 y = sigmoid(z) # squeeze 函数：从数组的形状中删除单维度条目，即把shape中为1的维度去掉 # loss = y - np.squeeze(Y) loss = y - Y cross_entropy = -1 * (np.dot(Y.T, np.log(y)) + np.dot((1 - Y.T),\ np.log(1 - y)))/ len(Y) total_loss += cross_entropy[0][0] #grad = np.sum(-1 * X * (np.squeeze(Y) - y).reshape((batch_size, 1)), axis=0) grad = np.sum(np.dot((y - Y).T, X), axis=0) #1*32*32*104 # grad = np.dot(X.T, loss) w = w - l_rate * grad # s_grad += grad ** 2 # ada = np.sqrt(s_grad) # w = w - l_rate * grad / ada list_cost.append(total_loss) # valid(X_valid, Y_valid, w) plt.plot(np.arange(len(list_cost)), list_cost) plt.title("Train Process") plt.xlabel("epoch_num") plt.ylabel("Cost Function (Cross Entropy)") plt.show() return wdef valid(funname, X, Y, w): a = np.dot(w, X.T) y = sigmoid(a) y_ = np.around(y) result = (np.squeeze(Y) == y_) print(f'&#123;funname&#125; acc = %f' % (float(result.sum()) / X.shape[0]))X_train_logi = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)X_valid_logi = np.concatenate((np.ones((X_valid.shape[0], 1)), X_valid), axis=1)X_test_logi = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)w_train = train(X_train_logi, y_train)valid("train", X_train_logi, y_train, w_train)valid("valid", X_valid_logi, y_valid, w_train)valid("test", X_test_logi, y_test, w_train)Ada12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# Adadef train(X_train, y_train): w = np.zeros((1, len(X_train[0]))) s_grad = np.zeros((1, len(X_train[0]))) l_rate = 0.1 epoch_num = 10000 list_cost = [] total_loss = 0.0 X = X_train y = y_train for epoch in range(epoch_num): z = np.dot(X, w.T) # n*104*104*1 Y = sigmoid(z) loss = Y - y cross_entropy = -1 * (np.dot(y.T, np.log(Y)) + np.dot((1 - y.T),\ np.log(1 - Y)))/ len(y) if abs(total_loss - cross_entropy[0][0]) &lt; 10**-9: break else: total_loss = cross_entropy[0][0] list_cost.append(total_loss) grad = np.sum(np.dot((Y - y).T, X), axis=0) #1*104 s_grad += grad**2 ada = np.sqrt(s_grad) w = w - l_rate * grad / ada print("times:", len(list_cost)) plt.plot(np.arange(len(list_cost)), list_cost) plt.title("Train Process") plt.xlabel("epoch_num") plt.ylabel("Cost Function (Cross Entropy)") plt.show() return wX_train_logi = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)X_valid_logi = np.concatenate((np.ones((X_valid.shape[0], 1)), X_valid), axis=1)X_test_logi = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)w_train = train(X_train_logi, y_train)valid("train", X_train_logi, y_train, w_train)valid("valid", X_valid_logi, y_valid, w_train)valid("test", X_test_logi, y_test, w_train)使用Kreas12from keras.models import Sequentialfrom keras.layers import Dense, Activation1234567891011121314151617model = Sequential()model.add(Dense(units=600, activation='sigmoid', input_dim=103))model.add(Dense(units=600, activation='sigmoid'))model.add(Dense(units=1, activation='sigmoid'))model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])model.fit(X_train, y_train, batch_size=32, epochs=50)score = model.evaluate(X_test, y_test)result = np.squeeze(model.predict(X_test))# print('Total loss on Testing set: ', score[0])# print('Accuracy of Testing set: ', score[1])# Total loss on Testing set: 0.2154487861520091# Accuracy of Testing set: 0.90295736357400741234y_ = np.around(result).astype(np.int)result = (np.squeeze(y_test) == y_)print('Test acc = %f' % (float(result.sum()) / X_test.shape[0]))# Test acc = 0.902957使用Tensorflow搭建3层神经网络1import tensorflow as tf12345678910111213141516171819xs = tf.placeholder(tf.float32, [None, 103])ys = tf.placeholder(tf.float32, [None, 1])w1 = tf.Variable(tf.random_normal([103, 600], stddev=1, seed=1))w2 = tf.Variable(tf.random_normal([600, 600], stddev=1, seed=1))w3 = tf.Variable(tf.random_normal([600, 1], stddev=1, seed=1))a = tf.nn.relu(tf.matmul(xs, w1))b = tf.sigmoid(tf.matmul(a, w2))y = tf.sigmoid(tf.matmul(b, w3))# a = tf.matmul(xs, w1)# y = tf.matmul(a, w2)y_ = tf.round(y)cross_entropy = -tf.reduce_mean(ys*tf.log(tf.clip_by_value(y,1e-10,1.0)))train_step = tf.train.GradientDescentOptimizer(0.001).minimize(cross_entropy)correct_prediction = tf.equal(y_, ys)accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))1234567891011result = Nonewith tf.Session() as sess: init = tf.global_variables_initializer() sess.run(init) for step in range(101): # training sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train&#125;) result = sess.run(y, feed_dict=&#123;xs: X_train, ys: y_train&#125;) if step % 100 == 0: print("accuracy:",sess.run(accuracy, feed_dict=&#123;xs: X_train, ys: y_train&#125;))# accuracy: 0.7285788随机森林和XGBoost1234# 随机森林from sklearn import metricsfrom sklearn.ensemble import RandomForestClassifierrfc=RandomForestClassifier()123# XGBoostfrom xgboost import XGBClassifierxgbc=XGBClassifier()123456# 选取k-1折的数据进行模型训练import warningswarnings.filterwarnings("ignore")from sklearn.model_selection import cross_val_score cross_val_score(rfc,X_train, y_train.ravel(),cv=5).mean(), cross_val_score(xgbc,X_train, y_train.ravel(),cv=5).mean()# (0.8434758405223647, 0.8613788898712886)12345#默认随机森林预测rfc.fit(X_train, y_train)rfc_y_predict = rfc.predict(X_valid)rfc.score(X_valid, y_valid)# 0.839854111405835512345# XGBoost预测xgbc.fit(X_train, y_train)xgbc_y_predict = xgbc.predict(X_valid)xgbc.score(X_valid, y_valid)# 0.862068965517241312345from sklearn.metrics import classification_reportprint('随机森林的预测准确率:')print(classification_report(y_valid, rfc_y_predict, target_names=['result'])) print('XGBoost的预测准确率:') print(classification_report(y_valid, xgbc_y_predict, target_names=['result']))保存数据1234# df = pd.DataFrame(&#123;"id": np.arange(1, 16282), "label": y_&#125;)# if not os.path.exists(output_dir):# os.mkdir(output_dir)# df.to_csv(os.path.join(output_dir + 'nn_output.csv'), sep='\t', index=False)参考文献http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.htmlhttps://github.com/maplezzz/NTU_ML2017_Hung-yi-Lee_HW]]></content>
      <categories>
        <category>李宏毅机器学习作业</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Homework 1 - PM2.5 Prediction]]></title>
    <url>%2F2019%2F03%2F25%2FHomework-1-PM2-5-Prediction%2F</url>
    <content type="text"><![CDATA[摘要李宏毅机器学习作业一，PM2.5预测PM2.5预测1. 导入相关库123import numpy as npimport pandas as pdimport matplotlib.pyplot as plt2. 数据处理1234train_data = pd.read_csv("train.csv")# pm2_5 = train_data[train_data['Obvservations']=='PM2.5'].iloc[:,3:]# pm2_5.info()train_data.info() # 240条数据，每条数据18个feature12345# 将RAINFALL值为NR的数据置0train_data[train_data[train_data['Obvservations']=='RAINFALL'].iloc[:,3:] == 'NR'] = 0# pm2_5 = train_data[train_data['Obvservations']=='PM2.5'].iloc[:,3:]# pm2_5.info()train_data[train_data['Obvservations']=='RAINFALL'].head()123456789101112tempxlist = [] tempylist = [] # 一天内总共有24-10+1 =15条记录for j in range(0, 240): for i in range(15): tempx = np.array(train_data.iloc[j*18:(j+1)*18,3:].iloc[:, i:i+9], float).reshape(1, 18*9) tempy = np.array(train_data.iloc[j*18+9:j*18+10,3:].iloc[:, i+9], float) # tempx = pm2_5.iloc[:,i:i+9] #使用前9小时数据作为feature # tempy = pm2_5.iloc[:,i+9] #使用第10个小数数据作为lable tempxlist.append(tempx) tempylist.append(tempy)12345678# X = np.array(pd.concat(tempxlist), float)X = np.concatenate(tempxlist, axis=0)# 插入列向量[1;1;...;1;]X = np.insert(X, 0, values=np.ones((1, X.shape[0])), axis=1)# y = np.array(pd.concat(tempylist), float)y = np.concatenate(tempylist, axis=0)y = y.reshape(y.shape[0], 1)X, y12X.shape, y.shape# ((3600, 163), (3600, 1))12# 特征归一化(Feature Scaling)X = (X - X.mean()) / X.std()12345# 代价函数def cost(y, w): temp = np.dot(X,w) loss = np.square(y - temp) return np.sum(loss)/len(y)3. 开始训练adagrad123456789101112131415161718192021# adagraddef ada(X, y, w, lr, iteration, lambdaL2): list_cost = [] s_grad = np.zeros([len(X[0]), 1]) for i in range(iteration): hypo = np.dot(X,w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) grad = np.dot(X.T, loss)/len(X) + lambdaL2*w s_grad += grad**2 ada = np.sqrt(s_grad) w = w - lr*grad/ada return w, list_costlr_ada = 10w_ada = np.zeros([X.shape[1], 1])w_ada, list_cost_ada = ada(X, y, w_ada, lr_ada, 10000, 0.)cost(y, w_ada)# 38.190334888655144SGD1234567891011121314151617181920# SGDdef SGD(X, y, w, lr, iteration, lambdaL2): list_cost = [] for i in range(iteration): hypo = np.dot(X,w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) rand = np.random.randint(0, len(X)) grad = X[rand].reshape(X.shape[1], 1)*loss[rand].reshape(loss.shape[1], 1)/len(X) + lambdaL2*w w = w - lr*grad return w, list_costw_sgd = np.zeros([X.shape[1], 1])lr_sgd = 0.1w_sgd, list_cost_sgd = SGD(X, y, w_sgd, lr_sgd, 10000, 0.)cost(y, w_sgd)# 209.00938349734912GD1234567891011121314151617def GD(X, y, w, lr, iteration, lambdaL2): list_cost = [] for i in range(iteration): hypo = np.dot(X, w) loss = hypo - y cost = np.sum(loss**2)/len(X) list_cost.append(cost) grad = np.dot(X.T, loss)/len(X) + lambdaL2 * w w = w - lr*grad return w, list_costw_gd = np.zeros([X.shape[1], 1])lr_gd = 0.01w_gd, list_cost_gd = GD(X, y, w_gd, lr_gd, 10000, 0.)cost(y, w_gd)# 44.80863160107732正规方程1234#close formw_cf = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)cost_wcf = np.sum((X.dot(w_cf)-y)**2) / len(X)hori = [cost_wcf for i in range(10000-3)]12345678910fig = plt.figure(figsize=(12,8))plt.plot(np.arange(len(list_cost_ada[3:])), list_cost_ada[3:], 'b', label="ada")plt.plot(np.arange(len(list_cost_sgd[3:])), list_cost_sgd[3:], 'g', label='sgd')plt.plot(np.arange(len(list_cost_gd[3:])), list_cost_gd[3:], 'r', label='gd')plt.plot(np.arange(len(list_cost_ada[3:])), hori, 'y--', label='close-form')plt.title('Train Process')plt.xlabel('Iteration')plt.ylabel('Loss Function(Quadratic)')plt.legend()plt.show()4. 使用Sklearn1234from sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import cross_val_predictfrom sklearn import metrics1234567X_train, X_test, y_train, y_test = train_test_split(X[:, 1:], y, random_state=1)linreg = LinearRegression()linreg.fit(X_train, y_train)predicted = cross_val_predict(linreg, X, y, cv=10)# linreg.intercept_, linreg.coef_metrics.mean_squared_error(y, predicted)# 42.576599287222915. 使用Tensorflow1import tensorflow as tf1234567891011121314151617181920212223242526272829303132333435363738def linear_regression(X_data, y_data, alpha, epoch, optimizer=tf.train.GradientDescentOptimizer): tf.reset_default_graph() xs = tf.placeholder(tf.float32, [None, X_data.shape[1]]) ys = tf.placeholder(tf.float32, [None, 1]) W = tf.Variable(tf.random_uniform([X_data.shape[1], 1], -10.0, 10.0)) y_pred = tf.matmul(xs, W) loss = tf.reduce_mean(tf.square(ys - y_pred)) if optimizer == tf.train.GradientDescentOptimizer: alpha = 0.01 elif optimizer == tf.train.AdagradOptimizer: alpha = 10 elif optimizer == tf.train.AdamOptimizer: alpha = 0.1 elif optimizer == tf.train.FtrlOptimizer: alpha = 10 elif optimizer == tf.train.RMSPropOptimizer: alpha = 10 opt = optimizer(learning_rate=alpha) opt_operation = opt.minimize(loss) # run the session with tf.Session() as sess: sess.run(tf.global_variables_initializer()) loss_data = [] for i in range(epoch): _, loss_val, W_val = sess.run([opt_operation, loss, W], feed_dict=&#123;xs: X_data, ys: y_data&#125;) loss_data.append(loss_val) if len(loss_data) &gt; 1 and np.abs(loss_data[-1] - loss_data[-2]) &lt; 10 ** -9: break tf.reset_default_graph() return &#123;'loss': loss_data, 'parameters': W_val&#125; # just want to return in row vector format123456789101112131415161718epoch = 10000alpha = 0.0001# 各种优化函数optimizer_dict=&#123;'GD': tf.train.GradientDescentOptimizer, 'Adagrad': tf.train.AdagradOptimizer, #'Adam': tf.train.AdamOptimizer, #'Ftrl': tf.train.FtrlOptimizer, #'RMS': tf.train.RMSPropOptimizer #'Momentum': tf.train.MomentumOptimizer两个参数 &#125;results = []t_loss = dict()for name in optimizer_dict: # 这里X应该是X[:, 1:] res = linear_regression(X, y, alpha, epoch, optimizer=optimizer_dict[name]) res['name'] = name t_loss[name] = res results.append(res)1234567891011121314fig, ax = plt.subplots(figsize=(16, 9))for res in results: loss_data = res['loss'] ax.plot(np.arange(len(loss_data[10:])), loss_data[10:], label=res['name'])ax.plot(np.arange(len(list_cost_ada[10:])), hori[0: 9990], label='close-form') ax.set_xlabel('epoch', fontsize=18)ax.set_ylabel('cost', fontsize=18)ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)ax.set_title('different optimizer', fontsize=18)plt.show()12t_loss["Adagrad"]["loss"][-1], t_loss["GD"]["loss"][-1]# (39.07505, 168.04092)6. 保存数据123test_data = pd.read_csv("test.csv")# pm2_5_test = test_data[test_data['AMB_TEMP'] == 'PM2.5'].iloc[:,2:]# pm2_5_test.info()1234# 对X_test进行一系列数据处理即可# X_test = np.array(pm2_5_test, float)# X_test = np.insert(X_test, 0, values=np.ones((1, X_test.shape[0])), axis=1)X_test12345#预测y_star = np.dot(X_test, w)y_pre = pd.read_csv("sampleSubmission.csv")y_pre.value = y_stary_pre.to_csv('predict.csv', index=False)参考文献http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.htmlhttps://github.com/maplezzz/NTU_ML2017_Hung-yi-Lee_HW]]></content>
      <categories>
        <category>李宏毅机器学习作业</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好]]></title>
    <url>%2F2019%2F02%2F28%2F%E4%BD%A0%E5%A5%BD%2F</url>
    <content type="text"><![CDATA[那些都是很好很好的，可我偏偏不喜欢。]]></content>
  </entry>
</search>
