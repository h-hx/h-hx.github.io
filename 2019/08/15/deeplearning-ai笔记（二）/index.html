<!-- build time:Thu Aug 15 2019 18:03:39 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|DejaVu Sans Mono for Powerline:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-flower.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-flower.png?v=5.1.4"><meta name="keywords" content="深度学习,"><meta name="description" content="浅层神经网络(Shallow neural networks)神经网络概述 （Neural Network Overview）\left.     \begin{array}{l}     x\\     w\\     b     \end{array}     \right\}     \implies{z={w}^Tx+b}如上所示，首先你需要输入特征$x$，参数$w$和$b$，通过这些你就"><meta name="keywords" content="深度学习"><meta property="og:type" content="article"><meta property="og:title" content="deeplearning.ai笔记（二）"><meta property="og:url" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/index.html"><meta property="og:site_name"><meta property="og:description" content="浅层神经网络(Shallow neural networks)神经网络概述 （Neural Network Overview）\left.     \begin{array}{l}     x\\     w\\     b     \end{array}     \right\}     \implies{z={w}^Tx+b}如上所示，首先你需要输入特征$x$，参数$w$和$b$，通过这些你就"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565854903617.png"><meta property="og:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565854858312.png"><meta property="og:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565854984520.png"><meta property="og:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565855871587.png"><meta property="og:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565856001087.png"><meta property="og:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565856269494.png"><meta property="og:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565856571918.png"><meta property="og:updated_time" content="2019-08-15T10:03:29.053Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="deeplearning.ai笔记（二）"><meta name="twitter:description" content="浅层神经网络(Shallow neural networks)神经网络概述 （Neural Network Overview）\left.     \begin{array}{l}     x\\     w\\     b     \end{array}     \right\}     \implies{z={w}^Tx+b}如上所示，首先你需要输入特征$x$，参数$w$和$b$，通过这些你就"><meta name="twitter:image" content="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/1565854903617.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"right",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/"><title>deeplearning.ai笔记（二） |</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title"></span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/15/deeplearning-ai笔记（二）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Kikyō"><meta itemprop="description" content><meta itemprop="image" content="/images/kikyo.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content></span><header class="post-header"><h1 class="post-title" itemprop="name headline">deeplearning.ai笔记（二）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-15T15:29:58+08:00">2019-08-15 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deeplearning-ai笔记/" itemprop="url" rel="index"><span itemprop="name">deeplearning.ai笔记</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">3.3k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">15</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="浅层神经网络-Shallow-neural-networks"><a href="#浅层神经网络-Shallow-neural-networks" class="headerlink" title="浅层神经网络(Shallow neural networks)"></a>浅层神经网络(Shallow neural networks)</h2><h3 id="神经网络概述-（Neural-Network-Overview）"><a href="#神经网络概述-（Neural-Network-Overview）" class="headerlink" title="神经网络概述 （Neural Network Overview）"></a>神经网络概述 （Neural Network Overview）</h3><p><img src="/2019/08/15/deeplearning-ai笔记（二）/1565854903617.png" alt="1565854903617"></p><script type="math/tex;mode=display">\left.
    \begin{array}{l}
    x\\
    w\\
    b
    \end{array}
    \right\}
    \implies{z={w}^Tx+b}</script><p>如上所示，首先你需要输入特征$x$，参数$w$和$b$，通过这些你就可以计算出$z$：</p><script type="math/tex;mode=display">\left.
    \begin{array}{l}
    x\\
    w\\
    b
    \end{array}
    \right\}
    \implies{z={w}^Tx+b}
    \implies{\alpha = \sigma(z)}\\ 
    \implies{L}(a,y)</script><p>接下来使用$z$就可以计算出$a$。我们将的符号换为表示输出$\hat{y}\implies{a = \sigma(z)}$,然后可以计算出<strong>loss function</strong> $L(a,y)$</p><p><img src="/2019/08/15/deeplearning-ai笔记（二）/1565854858312.png" alt="1565854858312"></p><p>在这个神经网络对应的3个节点，首先计算第一层网络中的各个节点相关的数$z^{[1]}$，接着计算$\alpha^{[1]}$，在计算下一层网络同理；<br>符号$^{[m]}$表示第$m$层网络中节点相关的数，这些节点的集合被称为第$m$层网络。</p><script type="math/tex;mode=display">\left.
    \begin{array}{r}
    {x }\\
    {W^{[1]}}\\
    {b^{[1]}}
    \end{array}
    \right\}
    \implies{z^{[1]}=W^{[1]}x+b^{[1]}}
    \implies{a^{[1]} = \sigma(z^{[1]})}</script><script type="math/tex;mode=display">\left.
    \begin{array}{r}
    \text{$a^{[1]} = \sigma(z^{[1]})$}\\
    \text{$W^{[2]}$}\\
    \text{$b^{[2]}$}\\
    \end{array}
    \right\}
    \implies{z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}}
    \implies{a^{[2]} = \sigma(z^{[2]})}\\ 
    \implies{L\left(a^{[2]},y \right)}</script><p>类似逻辑回归，在计算后需要使用计算，接下来你需要使用另外一个线性方程对应的参数计算$z^{[2]}$，<br>计算$a^{[2]}$，此时$a^{[2]}$就是整个神经网络最终的输出，用 $\hat{y}$表示网络的输出。</p><script type="math/tex;mode=display">\left.
    \begin{array}{r}
    {x }\\
    {dW^{[1]}}\\
    {db^{[1]}}
    \end{array}
    \right\}
    \impliedby{dz^{[1]}={d}(W^{[1]}x+b^{[1]})}
    \impliedby{d\alpha^{[1]} = { d}\sigma(z^{[1]})}</script><script type="math/tex;mode=display">\left.
    \begin{array}{r}
    {da^{[1]} = {d}\sigma(z^{[1]})}\\
    {dW^{[2]}}\\
    {db^{[2]}}\\
    \end{array}
    \right\}
    \impliedby{dz^{[2]}={d}(W^{[2]}\alpha^{[1]}+b^{[2]}})
    \impliedby{da^{[2]} = {d}\sigma(z^{[2]})}\\
    \impliedby{dL\left(a^{[2]},y \right)}</script><h3 id="神经网络的表示（Neural-Network-Representation）"><a href="#神经网络的表示（Neural-Network-Representation）" class="headerlink" title="神经网络的表示（Neural Network Representation）"></a>神经网络的表示（Neural Network Representation）</h3><p>只有一个隐藏层的简单两层<strong>神经网络结构</strong>：</p><p><img src="/2019/08/15/deeplearning-ai笔记（二）/1565854984520.png" alt="1565854984520"></p><ul><li>输入层和隐藏层之间<ul><li>$w^{[1]}-&gt;(4,3)$：前面的4是隐层神经元的个数，后面的3是输入层神经元的个数；</li><li>$b^{[1]}-&gt;(4,1)$：和隐藏层的神经元个数相同；</li></ul></li><li>隐藏层和输出层之间<ul><li>$w^{[2]}-&gt;(1,4)$：前面的1是输出层神经元的个数，后面的4是隐层神经元的个数；</li><li>$b^{[2]}-&gt;(1,1)$：和输出层的神经元个数相同；</li></ul></li></ul><p>两层之间的$w$参数矩阵大小为$(n_{out}, n_{in})$，$b$参数矩阵大小为$(n_{out},1)$，这里是作为$z = wX+b$</p><p>的线性关系来说明的，在神经网络中，$w^{[i]}=w^T$。</p><p>在logistic regression中，一般我们都会用$(n_{in}, n_{out})$来表示参数大小，计算使用的公式为：$z = w^{T}X+b$</p><h3 id="神经网络的输出（Computing-a-Neural-Network’s-output）"><a href="#神经网络的输出（Computing-a-Neural-Network’s-output）" class="headerlink" title="神经网络的输出（Computing a Neural Network’s output）"></a>神经网络的输出（Computing a Neural Network’s output）</h3><p><img src="/2019/08/15/deeplearning-ai笔记（二）/1565855871587.png" alt="1565855871587"></p><p>$z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1, a^{[1]}_1 = \sigma(z^{[1]}_1)$</p><p>$z^{[1]}_2 = w^{[1]T}_2x + b^{[1]}_2, a^{[1]}_2 = \sigma(z^{[1]}_2)$</p><p>$z^{[1]}_3 = w^{[1]T}_3x + b^{[1]}_3, a^{[1]}_3 = \sigma(z^{[1]}_3)$</p><p>$z^{[1]}_4 = w^{[1]T}_4x + b^{[1]}_4, a^{[1]}_4 = \sigma(z^{[1]}_4)$</p><h4 id="向量化计算"><a href="#向量化计算" class="headerlink" title="向量化计算"></a>向量化计算</h4><p>$z^{[n]} = w^{[n]}x + b^{[n]}$</p><p>$a^{[n]}=\sigma(z^{[n]})$</p><script type="math/tex;mode=display">a^{[1]} =
    \left[
        \begin{array}{c}
        a^{[1]}_{1}\\
        a^{[1]}_{2}\\
        a^{[1]}_{3}\\
        a^{[1]}_{4}
        \end{array}
        \right]
        = \sigma(z^{[1]})</script><script type="math/tex;mode=display">\left[
        \begin{array}{c}
        z^{[1]}_{1}\\
        z^{[1]}_{2}\\
        z^{[1]}_{3}\\
        z^{[1]}_{4}\\
        \end{array}
        \right]
         =
    \overbrace{
    \left[
        \begin{array}{c}
        ...W^{[1]T}_{1}...\\
        ...W^{[1]T}_{2}...\\
        ...W^{[1]T}_{3}...\\
        ...W^{[1]T}_{4}...
        \end{array}
        \right]
        }^{W^{[1]}}
        *
    \overbrace{
    \left[
        \begin{array}{c}
        x_1\\
        x_2\\
        x_3\\
        \end{array}
        \right]
        }^{input}
        +
    \overbrace{
    \left[
        \begin{array}{c}
        b^{[1]}_1\\
        b^{[1]}_2\\
        b^{[1]}_3\\
        b^{[1]}_4\\
        \end{array}
        \right]
        }^{b^{[1]}}</script><p>对于神经网络的第一层，给予一个输入$x$，得到$a^{[1]}$，$x$可以表示为$a^{[0]}$。后一层的表示同样可以写成类似的形式，得到$a^{[2]}$，$\hat{y} = a^{[2]}$。</p><p><img src="/2019/08/15/deeplearning-ai笔记（二）/1565856001087.png" alt="1565856001087"></p><h4 id="多样本向量化"><a href="#多样本向量化" class="headerlink" title="多样本向量化"></a>多样本向量化</h4><p>如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让$i$从1到$m$实现这四个等式：</p><p>$z^{<a href="i">1</a>}=W^{<a href="i">1</a>}x^{(i)}+b^{<a href="i">1</a>}$</p><p>$a^{<a href="i">1</a>}=\sigma(z^{<a href="i">1</a>})$</p><p>$z^{<a href="i">2</a>}=W^{<a href="i">2</a>}a^{<a href="i">1</a>}+b^{<a href="i">2</a>}$</p><p>$a^{<a href="i">2</a>}=\sigma(z^{<a href="i">2</a>})$</p><p>对于上面的这个方程中的$^{(i)}$，是所有依赖于训练样本的变量，即将$(i)$添加到$x$，$z$和$a$。如果想计算$m$个训练样本上的所有输出，就应该向量化整个计算，以简化这列。</p><script type="math/tex;mode=display">x =
    \left[
        \begin{array}{c}
        \vdots & \vdots & \vdots & \vdots\\
        x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\
        \vdots & \vdots & \vdots & \vdots\\
        \end{array}
        \right]</script><script type="math/tex;mode=display">Z^{[1]} =
    \left[
        \begin{array}{c}
        \vdots & \vdots & \vdots & \vdots\\
        z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)}\\
        \vdots & \vdots & \vdots & \vdots\\
        \end{array}
        \right]</script><script type="math/tex;mode=display">A^{[1]} =
    \left[
        \begin{array}{c}
        \vdots & \vdots & \vdots & \vdots\\
        \alpha^{[1](1)} & \alpha^{[1](2)} & \cdots & \alpha^{[1](m)}\\
        \vdots & \vdots & \vdots & \vdots\\
        \end{array}
        \right]</script><script type="math/tex;mode=display">\left.
        \begin{array}{r}
        \text{$z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1]}$}\\
        \text{$\alpha^{[1](i)} = \sigma(z^{[1](i)})$}\\
        \text{$z^{[2](i)} = W^{[2](i)}\alpha^{[1](i)} + b^{[2]}$}\\
        \text{$\alpha^{[2](i)} = \sigma(z^{[2](i)})$}\\
        \end{array}
        \right\}
        \implies
        \begin{cases}
        \text{$A^{[1]} = \sigma(z^{[1]})$}\\
        \text{$z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$}\\ 
        \text{$A^{[2]} = \sigma(z^{[2]})$}\\ 
        \end{cases}</script><p><img src="/2019/08/15/deeplearning-ai笔记（二）/1565856269494.png" alt="1565856269494"></p><p>在$m$个训练样本中，每次计算都是在重复相同的过程，均得到同样大小和结构的输出，所以利用向量化的思想将单个样本合并到一个矩阵中，其大小为$(x_n,m)$，其中$x_n$表示每个样本输入网络的神经元个数，也可以认为是单个样本的特征数，$m$表示训练样本的个数。</p><h3 id="激活函数（Activation-functions）"><a href="#激活函数（Activation-functions）" class="headerlink" title="激活函数（Activation functions）"></a>激活函数（Activation functions）</h3><p><strong>sigmoid</strong>函数：$a = \sigma(z) = \frac{1}{1 + e^{- z}}$，$a’=a(1-a)$</p><p><strong>tanh</strong>函数：$a= tanh(z) = \frac{e^{z} - e^{- z}}{e^{z} + e^{- z}}$，$a’=1-a^{2}$</p><p><strong>ReLu</strong>函数：$ a =max( 0,z) $</p><p><strong>Leaky ReLU</strong>：$a = max( 0.01z,z)$</p><p><img src="/2019/08/15/deeplearning-ai笔记（二）/1565856571918.png" alt="1565856571918"></p><p><strong>sigmoid</strong>激活函数：</p><ul><li>函数计算量大</li><li>反向传播时，很容易就会出现梯度消失的情况</li><li>只有正数输出</li></ul><p><strong>sigmoid</strong>和<strong>tanh</strong>函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而<strong>Relu</strong>和<strong>Leaky ReLu</strong>函数大于0部分都为常熟，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong>进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而<strong>Leaky ReLu</strong>不会有这问题)</p><h3 id="神经网络的梯度下降（Gradient-descent-for-neural-networks）"><a href="#神经网络的梯度下降（Gradient-descent-for-neural-networks）" class="headerlink" title="神经网络的梯度下降（Gradient descent for neural networks）"></a>神经网络的梯度下降（Gradient descent for neural networks）</h3><p>你的单隐层神经网络会有$W^{[1]}$，$b^{[1]}$，$W^{[2]}$，$b^{[2]}$这些参数，还有个$n_x$表示输入特征的个数，$n^{[1]}$表示隐藏单元个数，$n^{[2]}$表示输出单元个数。</p><p>在我们的例子中，我们只介绍过的这种情况，那么参数:</p><p>矩阵$W^{[1]}$的维度就是($n^{[1]}, n^{[0]}$)，$b^{[1]}$就是$n^{[1]}$维向量，可以写成$(n^{[1]}, 1)$，就是一个的列向量。<br>矩阵$W^{[2]}$的维度就是($n^{[2]}, n^{[1]}$)，$b^{[2]}$的维度就是$(n^{[2]},1)$维度。</p><p>你还有一个神经网络的成本函数，假设你在做二分类任务，那么你的成本函数等于：</p><p><strong>Cost function</strong>:<br>公式：<br>$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = {\frac{1}{m}}\sum_{i=1}^mL(\hat{y}, y)$<br><strong>loss function</strong>和之前做<strong>logistic</strong>回归完全一样。</p><p>训练参数需要做梯度下降，在训练神经网络的时候，随机初始化参数很重要，而不是初始化成全零。当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值：</p><p>$\hat{y}^{(i)},(i=1,2,…,m)$<br>$dW^{[1]} = \frac{dJ}{dW^{[1]}},db^{[1]} = \frac{dJ}{db^{[1]}}$</p><p>${d}W^{[2]} = \frac{dJ}{dW^{[2]}},{d}b^{[2]} = \frac{dJ}{db^{[2]}}$</p><p>其中<br>$W^{[1]}\implies{W^{[1]} - adW^{[1]}},b^{[1]}\implies{b^{[1]} -adb^{[1]}}$</p><p>$W^{[2]}\implies{W^{[2]} - \alpha{\rm d}W^{[2]}},b^{[2]}\implies{b^{[2]} - \alpha{\rm d}b^{[2]}}$<br>正向传播方程如下（之前讲过）：<br><strong>forward propagation</strong>：<br>$z^{[1]} = W^{[1]}x + b^{[1]}$</p><p>$a^{[1]} = \sigma(z^{[1]})$</p><p>$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$</p><p>$a^{[2]} = g^{[2]}(z^{[z]}) = \sigma(z^{[2]})$</p><p>反向传播方程如下:</p><p><strong>back propagation</strong>：<br>$ dz^{[2]} = A^{[2]} - Y , Y = \begin{bmatrix}y^{[1]} &amp; y^{[2]} &amp; \cdots &amp; y^{[m]}\\ \end{bmatrix} $</p><p>$ dW^{[2]} = {\frac{1}{m}}dz^{[2]}A^{[1]T} $</p><p>$ {\rm d}b^{[2]} = {\frac{1}{m}}np.sum({d}z^{[2]},axis=1,keepdims=True)$</p><p>$\underbrace{dZ^{[1]}}_{(n^{[1]}, m)} = \underbrace{W^{[2]T}dZ^{[2]}}_{(n^{[1]}, m)}*\underbrace{g[1]^{‘}(Z^{[1]})}_{(n^{[1]}, m)}$</p><p>$dW^{[1]} = {\frac{1}{m}}dz^{[1]}x^{T}$</p><p>${\underbrace{db^{[1]}}_{(n^{[1]},1)}} = {\frac{1}{m}}np.sum(dz^{[1]},axis=1,keepdims=True)$</p><p>上述是反向传播的步骤，注：这些都是针对所有样本进行过向量化，$Y$是$1×m$的矩阵；这里<code>np.sum</code>是python的numpy命令，<code>axis=1</code>表示水平相加求和，<code>keepdims</code>是防止<strong>python</strong>输出那些古怪的秩数$(n,)$，加上这个确保阵矩阵$db^{[2]}$这个向量输出的维度为$(n,1)$这样标准的形式。</p><p>目前为止，我们计算的都和<strong>Logistic</strong>回归十分相似，但当你开始计算反向传播时，你需要计算，是隐藏层函数的导数，输出在使用<strong>sigmoid</strong>函数进行二元分类。这里是进行逐个元素乘积，因为$W^{[2]T}dz^{[2]}$和$(z^{[1]})$这两个都为$(n^{[1]},m)$矩阵；</p><p>还有一种防止<strong>python</strong>输出奇怪的秩数，需要显式地调用<code>reshape</code>把<code>np.sum</code>输出结果写成矩阵形式。</p><h3 id="直观理解反向传播（Backpropagation-intuition）"><a href="#直观理解反向传播（Backpropagation-intuition）" class="headerlink" title="直观理解反向传播（Backpropagation intuition）"></a>直观理解反向传播（Backpropagation intuition）</h3><p>下图是逻辑回归的推导：</p><script type="math/tex;mode=display">\left.
    \begin{array}{l}
    {x }\\
    {w }\\
    {b }
    \end{array}
    \right\}
    \implies{z={w}^Tx+b}
    \implies{\alpha = \sigma(z)} 
    \implies{L\left(a,y \right)}</script><p>所以回想当时我们讨论逻辑回归的时候，我们有这个正向传播步骤，其中我们计算$z$，然后$a$，然后损失函数$L$。</p><script type="math/tex;mode=display">\underbrace{
    \left.
    \begin{array}{l}
    {x }\\
    {w }\\
    {b }
    \end{array}
    \right\}
    }_{dw={dz}\cdot x, db =dz}
    \impliedby\underbrace{z={w}^Tx+b}_{d
    z=da\cdot g^{'}(z),
    g(z)=\sigma(z),
    {\frac{dL}{dz}}={\frac{dL}{da}}\cdot{\frac{da}{dz}},
    {\frac{d}{ dz}}g(z)=g^{'}(z)}
    \impliedby\underbrace{a = \sigma(z)} 
    \impliedby{L(a,y)}_{da={\frac{d}{da}}{L}\left(a,y \right)=(-y\log{a} - (1 - y)\log(1 - a))^{'}={-\frac{y}{a}} + {\frac{1 - y}{1 - a}{}} }</script><p>神经网络的计算中，与逻辑回归十分类似，但中间会有多层的计算。下图是一个双层神经网络，有一个输入层，一个隐藏层和一个输出层。</p><p>前向传播：</p><p>计算$z^{[1]}$，$a^{[1]}$，再计算$z^{[2]}$，$a^{[2]}$，最后得到<strong>loss function</strong>。</p><p>反向传播：</p><p>向后推算出$da^{[2]}$，然后推算出$dz^{[2]}$，接着推算出$da^{[1]}$，然后推算出$dz^{[1]}$。我们不需要对$x$求导，因为$x$是固定的，我们也不是想优化$x$。向后推算出$da^{[2]}$，然后推算出$dz^{[2]}$的步骤可以合为一步：<br>公式3.40：<br>$dz^{[2]}=a^{[2]}-y\;，\;dW^{[2]}=dz^{[2]}{a^{[1]}}^{T}$<br>(注意：逻辑回归中；为什么$a^{[1]T}$多了个转置：$dw$中的$W$(视频里是$W^{[2]}_i$)是一个列向量，而$W^{[2]}$是个行向量，故需要加个转置);<br>$db^{[2]}=dz^{[2]}$<br>$dz^{[1]} = W^{[2]T}dz^{[2]}* g[1]^{‘}(z^{[1]})$<br>注意：这里的矩阵：$W^{[2]}$的维度是：$(n^{[2]},n^{[1]})$。</p><p>$z^{[2]}$ ， $dz^{[2]}$的维度都是：$(n^{[2]},1)$，如果是二分类，那维度就是$(1,1)$。</p><p>$z^{[1]}$，$dz^{[1]}$的维度都是：$(n^{[1]},1)$。</p><p>证明过程：<br>其中$W^{[2]T}dz^{[2]}$维度为：$(n^{[1]},n^{[2]})$、$(n^{[2]},1)$相乘得到$(n^{[1]},1)$，和$z^{[1]}$维度相同，</p><p>$g[1]^{‘}(z^{[1]})$的维度为$(n^{[1]},1)$，这就变成了两个都是$(n^{[1]},1)$向量逐元素乘积。</p><p>实现后向传播有个技巧，就是要保证矩阵的维度相互匹配。最后得到$dW^{[1]}$和$db^{[1]}$<br>$dW^{[1]} =dz^{[1]}x^{T},db^{[1]} = dz^{[1]}$</p><p>可以看出$dW^{[1]}$ 和$dW^{[2]}$ 非常相似，其中$x$扮演了$a^{[0]}$的角色，$x^{T}$ 等同于$a^{[0]T}$。</p><p>由：<br>$Z^{[1]} = W^{[1]}x + b^{[1]}\;,\;a^{[1]}=g^{[1]}(Z^{[1]})$<br>得到：<br>$Z^{[1]} = W^{[1]}x + b^{[1]}, A^{[1]} = g^{[1]}(Z^{[1]})$</p><script type="math/tex;mode=display">Z^{[1]} =
    \left[
        \begin{array}{c}
        \vdots &\vdots & \vdots & \vdots \\
        z^{[1](1)} & z^{[1](2)} & \vdots & z^{[1](m)} \\
        \vdots &\vdots & \vdots & \vdots \\
        \end{array}
        \right]</script><p>注意：大写的$Z^{[1]}$表示$z^{<a href="1">1</a>},z^{<a href="2">1</a>},z^{<a href="3">1</a>}…z^{<a href="m">1</a>}$的列向量堆叠成的矩阵，以下类同。</p><p>下图写了主要的推导过程：<br>$dZ^{[2]}=A^{[2]}-Y\;，\;dW^{[2]}={\frac{1}{m}}dZ^{[2]}{A^{[1]}}^{T}$<br>$L = {\frac{1}{m}}\sum_i^n{L(\hat{y},y)}$<br>$db^{[2]} = {\frac{1}{m}}np.sum(dZ^{[2]},axis=1,keepdims=True)$<br>$\underbrace{dZ^{[1]}}_{(n^{[1]}, m)} = \underbrace{W^{[2]T}dZ^{[2]}}_{(n^{[1]}, m)}*\underbrace{g[1]^{‘}(Z^{[1]})}_{(n^{[1]}, m)}$<br>$dW^{[1]} = {\frac{1}{m}}dZ^{[1]}x^{T}$<br>$db^{[1]} = {\frac{1}{m}}np.sum(dZ^{[1]},axis=1,keepdims=True) $</p><h3 id="随机初始化（Random-Initialization）"><a href="#随机初始化（Random-Initialization）" class="headerlink" title="随机初始化（Random+Initialization）"></a>随机初始化（Random+Initialization）</h3><p>在初始化的时候，$W$参数要进行随机初始化，$b$则不存在对称性的问题它可以设置为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand((<span class="number">2</span>,<span class="number">2</span>))* <span class="number">0.01</span></span><br><span class="line">b = np.zero((<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>这里我们将$W$的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用<strong>sigmoid</strong>函数或者<strong>tanh</strong>函数作为激活函数时，$W$比较小，则$Z=WX+b$所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果$W$设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。</p><p><strong>ReLU</strong>和<strong>Leaky ReLU</strong>作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p><p><a href="https://blog.csdn.net/Koala_Tree/article/details/79913655" target="_blank" rel="noopener">https://blog.csdn.net/Koala_Tree/article/details/79913655</a></p><p><a href="https://www.coursera.org/learn/neural-networks-deep-learning/notebook/NI888/planar-data-classification-with-a-hidden-layer" target="_blank" rel="noopener">https://www.coursera.org/learn/neural-networks-deep-learning/notebook/NI888/planar-data-classification-with-a-hidden-layer</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/08/14/deeplearning-ai笔记（一）/" rel="next" title="deeplearning.ai笔记（一）"><i class="fa fa-chevron-left"></i> deeplearning.ai笔记（一）</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/kikyo.jpg" alt="Kikyō"><p class="site-author-name" itemprop="name">Kikyō</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">3</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#浅层神经网络-Shallow-neural-networks"><span class="nav-text">浅层神经网络(Shallow neural networks)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络概述-（Neural-Network-Overview）"><span class="nav-text">神经网络概述 （Neural Network Overview）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络的表示（Neural-Network-Representation）"><span class="nav-text">神经网络的表示（Neural Network Representation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络的输出（Computing-a-Neural-Network’s-output）"><span class="nav-text">神经网络的输出（Computing a Neural Network’s output）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#向量化计算"><span class="nav-text">向量化计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多样本向量化"><span class="nav-text">多样本向量化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数（Activation-functions）"><span class="nav-text">激活函数（Activation functions）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络的梯度下降（Gradient-descent-for-neural-networks）"><span class="nav-text">神经网络的梯度下降（Gradient descent for neural networks）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#直观理解反向传播（Backpropagation-intuition）"><span class="nav-text">直观理解反向传播（Backpropagation intuition）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机初始化（Random-Initialization）"><span class="nav-text">随机初始化（Random+Initialization）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Kikyō</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">28.8k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script></body></html><!-- rebuild by neat -->