<!-- build time:Sat Sep 14 2019 15:42:11 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|DejaVu Sans Mono for Powerline:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-flower.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-flower.png?v=5.1.4"><meta name="keywords" content="深度学习,"><meta name="description" content="深度学习的实践层面（Practical aspects of Deep Learning）1.1 训练，验证，测试集（Train / Dev / Test sets）在数据较少情况下，划分情况：无验证集：$0.7-0.3$有验证集：$0.6-0.2-0.2$数据较多情况下：$1,000,000$：$0.98-0.01-0.01$$&amp;gt;1,000,000$：$0.995-0.0025-0.002"><meta name="keywords" content="深度学习"><meta property="og:type" content="article"><meta property="og:title" content="deeplearning.ai笔记（2-1）"><meta property="og:url" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/index.html"><meta property="og:site_name"><meta property="og:description" content="深度学习的实践层面（Practical aspects of Deep Learning）1.1 训练，验证，测试集（Train / Dev / Test sets）在数据较少情况下，划分情况：无验证集：$0.7-0.3$有验证集：$0.6-0.2-0.2$数据较多情况下：$1,000,000$：$0.98-0.01-0.01$$&amp;gt;1,000,000$：$0.995-0.0025-0.002"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568429419214.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568429509580.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568429772271.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568430119498.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568430871362.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568430954822.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568431048524.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568431081856.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568431146690.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568431185134.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568443498914.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568443517139.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568444589277.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568444666614.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568444768091.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568444967814.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568445165761.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568445633011.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568445736240.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568445919895.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568446320047.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568446360324.png"><meta property="og:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568446685321.png"><meta property="og:updated_time" content="2019-09-14T07:41:39.203Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="deeplearning.ai笔记（2-1）"><meta name="twitter:description" content="深度学习的实践层面（Practical aspects of Deep Learning）1.1 训练，验证，测试集（Train / Dev / Test sets）在数据较少情况下，划分情况：无验证集：$0.7-0.3$有验证集：$0.6-0.2-0.2$数据较多情况下：$1,000,000$：$0.98-0.01-0.01$$&amp;gt;1,000,000$：$0.995-0.0025-0.002"><meta name="twitter:image" content="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/1568429419214.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"right",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/"><title>deeplearning.ai笔记（2-1） |</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title"></span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/14/deeplearning-ai笔记（2-1）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Kikyō"><meta itemprop="description" content><meta itemprop="image" content="/images/kikyo.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content></span><header class="post-header"><h1 class="post-title" itemprop="name headline">deeplearning.ai笔记（2-1）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-14T10:40:03+08:00">2019-09-14 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2019-09-14T15:41:39+08:00">2019-09-14 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deeplearning-ai笔记/" itemprop="url" rel="index"><span itemprop="name">deeplearning.ai笔记</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">4.8k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">19</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="深度学习的实践层面（Practical-aspects-of-Deep-Learning）"><a href="#深度学习的实践层面（Practical-aspects-of-Deep-Learning）" class="headerlink" title="深度学习的实践层面（Practical aspects of Deep Learning）"></a>深度学习的实践层面（Practical aspects of Deep Learning）</h2><h3 id="1-1-训练，验证，测试集（Train-Dev-Test-sets）"><a href="#1-1-训练，验证，测试集（Train-Dev-Test-sets）" class="headerlink" title="1.1 训练，验证，测试集（Train / Dev / Test sets）"></a>1.1 训练，验证，测试集（Train / Dev / Test sets）</h3><p>在数据较少情况下，划分情况：</p><ul><li>无验证集：$0.7-0.3$</li><li>有验证集：$0.6-0.2-0.2$</li></ul><p>数据较多情况下：</p><p>$1,000,000$：$0.98-0.01-0.01$</p><p>$&gt;1,000,000$：$0.995-0.0025-0.0025$ 或 $0.995-0.004-0.001$</p><h3 id="1-2-偏差，方差（Bias-Variance）"><a href="#1-2-偏差，方差（Bias-Variance）" class="headerlink" title="1.2 偏差，方差（Bias /Variance）"></a>1.2 偏差，方差（Bias /Variance）</h3><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568429419214.png" alt="1568429419214"></p><p>高偏差和高方差：</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568429509580.png" alt="1568429509580"></p><p>采用曲线函数或二次元函数会产生高方差，因为它曲线灵活性太高以致拟合了这两个错误样本和中间这些活跃数据。</p><p>这看起来有些不自然，从两个维度上看都不太自然，但对于高维数据，有些数据区域偏差高，有些数据区域方差高，所以在高维数据中采用这种分类器看起来就不会那么牵强了。</p><h3 id="1-3-机器学习基础（Basic-Recipe-for-Machine-Learning）"><a href="#1-3-机器学习基础（Basic-Recipe-for-Machine-Learning）" class="headerlink" title="1.3 机器学习基础（Basic Recipe for Machine Learning）"></a>1.3 机器学习基础（Basic Recipe for Machine Learning）</h3><p>在训练机器学习模型的过程中，解决High bias 和High variance 的过程：</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568429772271.png" alt="1568429772271"></p><ul><li>1.是否存在High bias?<ul><li>增加网络结构，如增加隐藏层数目；</li><li>训练更长时间；</li><li>寻找合适的网络架构，使用更大的NN结构；</li></ul></li><li>2.是否存在High variance？<ul><li>获取更多的数据；</li><li>正则化（ regularization）；</li><li>寻找合适的网络结构；</li></ul></li></ul><h3 id="1-4-正则化（Regularization）"><a href="#1-4-正则化（Regularization）" class="headerlink" title="1.4 正则化（Regularization）"></a>1.4 正则化（Regularization）</h3><h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a><strong>Logistic regression</strong></h4><p>加入正则化项的代价函数：</p><p>$J(w,b)=\dfrac{1}{m}\sum\limits_{i=1}^{m}l(\hat y^{(i)},y^{(i)})+\dfrac{\lambda}{2m}||w||_{2}^{2}$</p><p>为什么只正则化参数$w$？为什么不再加上参数 $b$ 呢？你可以这么做，只是我习惯省略不写，因为$w$通常是一个高维参数矢量，已经可以表达高偏差问题，$w$可能包含有很多参数，我们不可能拟合所有参数，而$b$只是单个数字，所以$w$几乎涵盖所有参数，而不是$b$，如果加了参数$b$，其实也没太大影响，因为$b$只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数，完全没问题。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568430119498.png" alt="1568430119498"></p><ul><li>L2正则化：$\dfrac{\lambda}{2m}||w||_{2}^{2} = \dfrac{\lambda}{2m}\sum\limits_{j=1}^{n_{x}} w_{j}^{2}=\dfrac{\lambda}{2m}w^{T}w$</li><li>L1正则化：$\dfrac{\lambda}{2m}||w||_{1}=\dfrac{\lambda}{2m}\sum\limits_{j=1}^{n_{x}}|w_{j}|$</li></ul><p>其中$λ$为正则化因子。</p><p><code>lambda</code>在python中属于保留字，所以在编程的时候，用“lambd”代表这里的正则化因子$λ$</p><h4 id="Neural-network"><a href="#Neural-network" class="headerlink" title="Neural network"></a><strong>Neural network</strong></h4><p>加入正则化项的代价函数：</p><p>$J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\dfrac{1}{m}\sum\limits_{i=1}^{m}l(\hat y^{(i)},y^{(i)})+\dfrac{\lambda}{2m}\sum\limits_{l=1}^{L}||w^{[l]}||_{F}^{2}$</p><p>其中$||w^{[l]}||_{F}^{2}=\sum\limits_{i=1}^{n^{[l-1]}}\sum\limits_{j=1}^{n^{[l]}}(w_{ij}^{[l]})^{2}$，因为$w$的大小为$(n^{[l-1]},n^{[l]})$，该矩阵范数被称为“Frobenius norm”（“弗罗贝尼乌斯范数”，用下标$F$标注”）</p><p>使用该范数实现梯度下降：</p><p>$dW^{[l]} = (form_backprop)+\dfrac{\lambda}{m}W^{[l]}$</p><p>$W^{[l]}:= W^{[l]}-\alpha dW^{[l]}$</p><p>$\begin{align}W^{[l]}:&amp;= W^{[l]}-\alpha [ (form_backprop)+\dfrac{\lambda}{m}W^{[l]}]\\ &amp;= W^{[l]}-\alpha\dfrac{\lambda}{m}W^{[l]} -\alpha(form_backprop)\\&amp;=(1-\dfrac{\alpha\lambda}{m})W^{[l]}-\alpha(form_backprop)\end{align}$</p><p>该正则项说明，不论$W^{[l]}$是什么，我们都试图让它变得更小，实际上，相当于我们给矩阵W乘以$(1 -a\frac{\lambda}{m})$倍的权重，矩阵$W$减去$\alpha\frac{\lambda}{m}$倍的它，也就是用这个系数$(1-a\frac{\lambda}{m})$乘以矩阵$W$，该系数小于1，因此$L2$范数正则化也被称为“权重衰减”，因为它就像一般的梯度下降，$W$被更新为少了$a$乘以<strong>backprop</strong>输出的最初梯度值，同时$W$也乘以了这个系数，这个系数小于1，因此$L2$正则化也被称为“权重衰减”（Weight decay）。</p><h3 id="1-5-为什么正则化有利于预防过拟合呢？（Why-regularization-reduces-overfitting-）"><a href="#1-5-为什么正则化有利于预防过拟合呢？（Why-regularization-reduces-overfitting-）" class="headerlink" title="1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）"></a>1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</h3><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568430871362.png" alt="1568430871362"></p><p>左图是高偏差，右图是高方差，中间是<strong>Just Right</strong>。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568430954822.png" alt="1568430954822"></p><p>直观理解就是$\lambda$增加到足够大，$W$会接近于0，实际上是不会发生这种情况的，我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网络的所有隐藏单元依然存在，但是它们的影响变得更小了。神经网络变得更简单了，貌似这样更不容易发生过拟合。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568431048524.png" alt="1568431048524"></p><p>假设我们用的是这样的双曲线激活函数：</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568431081856.png" alt="1568431081856"></p><p>用$g(z)$表示$tanh(z)$,那么我们发现，只要$z$非常小，如果$z$只涉及少量参数，这里我们利用了双曲正切函数的线性状态，只要$z$可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568431146690.png" alt="1568431146690"></p><p>如果正则化参数λ很大，激活函数的参数会相对较小，因为代价函数中的参数变大了，如果$W$很小，相对来说，$z$也会很小。</p><p>特别是，如果$z$的值最终在这个范围内，都是相对较小的值，$g(z)$大致呈线性，每层几乎都是线性的，和线性回归函数一样。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568431185134.png" alt="1568431185134"></p><p>如果每层都是线性的，那么整个网络就是一个线性网络，即使是一个非常深的深层网络，因具有线性激活函数的特征，最终我们只能计算线性函数。</p><h3 id="1-6-dropout-正则化（Dropout-Regularization）"><a href="#1-6-dropout-正则化（Dropout-Regularization）" class="headerlink" title="1.6 dropout 正则化（Dropout Regularization）"></a>1.6 dropout 正则化（Dropout Regularization）</h3><p><strong>Dropout</strong>（随机失活）：</p><p>为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568443498914.png" alt="1568443498914"></p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568443517139.png" alt="1568443517139"></p><h4 id="inverted-dropout（反向随机失活）"><a href="#inverted-dropout（反向随机失活）" class="headerlink" title="inverted dropout（反向随机失活）"></a><strong>inverted dropout</strong>（反向随机失活）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span>  <span class="comment"># 设置神经元保留概率</span></span><br><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>], a3.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">a3 = np.multiply(a3, d3)</span><br><span class="line">a3 /= keep_prob</span><br></pre></td></tr></table></figure><p>$z^{[4]} = w^{[4]} a^{[3]} + b^{[4]}$，我们的预期是，$a^{[3]}$减少20%，也就是说$a^{[3]}$中有20%的元素被归零，为了不影响$z^{\lbrack4]}$的期望值，我们需要用$w^{[4]} a^{[3]}/0.8$，它将会修正或弥补我们所需的那20%，$a^{[3]}$的期望值不会变。</p><p><strong>在测试阶段不要用dropout，因为那样会使得预测结果变得随机。</strong></p><h3 id="1-7-理解-dropout（Understanding-Dropout）"><a href="#1-7-理解-dropout（Understanding-Dropout）" class="headerlink" title="1.7 理解 dropout（Understanding Dropout）"></a>1.7 理解 dropout（Understanding Dropout）</h3><p>这里我们以单个神经元入手，单个神经元的工作就是接收输入，并产生一些有意义的输出，但是加入了Dropout以后，输入的特征都是有可能会被随机清除的，所以该神经元不会再特别依赖于任何一个输入特征，也就是说不会给任何一个输入设置太大的权重。</p><p>所以通过传播过程，dropout将产生和L2范数相同的收缩权重的效果。</p><p><strong>dropout</strong>的功能类似于$L2$正则化，与$L2$正则化不同的是，被应用的方式不同，<strong>dropout</strong>也会有所不同，甚至更适用于不同的输入范围。</p><p>对于不同的层，设置的<strong>keep_prob</strong>也不同，一般来说神经元较少的层，会设<strong>keep_prob =1.0</strong>，神经元多的层，则会将<strong>keep_prob</strong>设置的较小。</p><p>如果你担心某些层比其它层更容易发生过拟合，可以把某些层的<strong>keep-prob</strong>值设置得比其它层更低，缺点是为了使用交叉验证，你要搜索更多的超级参数，另一种方案是在一些层上应用<strong>dropout</strong>，而有些层不用<strong>dropout</strong>，应用<strong>dropout</strong>的层只含有一个超级参数，就是<strong>keep-prob</strong>。</p><p><strong>dropout</strong>一大缺点就是代价函数$J$不再被明确定义，每次迭代，都会随机移除一些节点，无法绘制出每次迭代$J$的图。</p><p><strong>使用Dropout：</strong></p><ul><li>关闭dropout功能，即设置<strong>keep_prob = 1.0</strong>；</li><li>运行代码，确保$J(W，b)$函数单调递减；</li><li>再打开dropout函数。</li></ul><h3 id="1-8-其他正则化方法（Other-regularization-methods）"><a href="#1-8-其他正则化方法（Other-regularization-methods）" class="headerlink" title="1.8 其他正则化方法（Other regularization methods）"></a>1.8 其他正则化方法（Other regularization methods）</h3><h4 id="数据扩增（Data-augmentation）"><a href="#数据扩增（Data-augmentation）" class="headerlink" title="数据扩增（Data augmentation）"></a>数据扩增（Data augmentation）</h4><p>通过图片的一些变换，得到更多的训练集和验证集</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568444589277.png" alt="1568444589277"></p><h4 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h4><p>在交叉验证集的误差上升之前的点停止迭代，避免过拟合。这种方法的缺点是无法同时解决bias和variance之间的最优。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568444666614.png" alt="1568444666614"></p><p>如果不用<strong>early stopping</strong>，另一种方法就是$L2$正则化，训练神经网络的时间就可能很长。这导致超级参数搜索空间更容易分解，也更容易搜索，但是缺点在于，你必须尝试很多正则化参数$\lambda$的值，这也导致搜索大量$\lambda$值的计算代价太高。</p><p><strong>Early stopping</strong>的优点是，只运行一次梯度下降，你可以找出$w$的较小值，中间值和较大值，而无需尝试$L2$正则化超级参数$\lambda$的很多值。</p><h3 id="1-9-归一化输入（Normalizing-inputs）"><a href="#1-9-归一化输入（Normalizing-inputs）" class="headerlink" title="1.9 归一化输入（Normalizing inputs）"></a>1.9 归一化输入（Normalizing inputs）</h3><p>训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：</p><ol><li><p>零均值</p></li><li><p>归一化方差；</p><p>我们希望无论是训练集和测试集都是通过相同的$μ$和$σ^2$定义的数据转换，这两个是由训练集得出来的。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568444768091.png" alt="1568444768091"></p></li></ol><ul><li>计算每个特征所有样本数据的均值：$\mu = \dfrac{1}{m}\sum\limits_{i=1}^{m}x^{(i)}$</li><li>减去均值得到对称的分布：$x : =x-\mu$</li><li>归一化方差：$\sigma^{2} = \dfrac{1}{m}\sum\limits_{i=1}^{m}x^{(i)^{2}}$</li></ul><h4 id="使用归一化的原因："><a href="#使用归一化的原因：" class="headerlink" title="使用归一化的原因："></a><strong>使用归一化的原因：</strong></h4><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568444967814.png" alt="1568444967814"></p><p>在不使用归一化的代价函数中，如果我们设置一个较小的学习率，那么很可能我们需要很多次迭代才能到达代价函数全局最优解；如果使用了归一化，那么无论从哪个位置开始迭代，我们都能以相对很少的迭代次数找到全局最优解。</p><h3 id="1-10-梯度消失-梯度爆炸（Vanishing-Exploding-gradients）"><a href="#1-10-梯度消失-梯度爆炸（Vanishing-Exploding-gradients）" class="headerlink" title="1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）"></a>1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</h3><p>假设你正在训练这样一个极深的神经网络：</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568445165761.png" alt="1568445165761"></p><p>对于目标输出有：$\hat y = W^{[L]}W^{[L-1]}\cdots W^{[2]}W^{[1]}X$</p><p>假设每个权重矩阵$W^{[l]} = \begin{bmatrix} 1.5 &amp; 0 \\0 &amp; 1.5 \\\end{bmatrix}$，从技术上来讲，最后一项有不同维度，可能它就是余下的权重矩阵，$y= W^{[1]}\begin{bmatrix} 1.5 &amp; 0 \\ 0 &amp; 1.5 \\\end{bmatrix}^{(L -1)}x$，因为我们假设所有矩阵都等于它，它是1.5倍的单位矩阵，最后的计算结果就是$\hat{y}$，$\hat{y}$也就是等于${1.5}^{(L-1)}x$。如果对于一个深度神经网络来说$L$值较大，那么$\hat{y}$的值也会非常大，实际上它呈指数级增长的，它增长的比率是${1.5}^{L}$，因此对于一个深度神经网络，$y$的值将爆炸式增长。</p><p>相反的，如果权重是0.5，$W^{[l]} = \begin{bmatrix} 0.5&amp; 0 \\ 0 &amp; 0.5 \\ \end{bmatrix}$，它比1小，这项也就变成了${0.5}^{L}$，矩阵$y= W^{[1]}\begin{bmatrix} 0.5 &amp; 0 \\ 0 &amp; 0.5 \\\end{bmatrix}^{(L - 1)}x$，再次忽略$W^{[L]}$，因此每个矩阵都小于1，假设$x_{1}$和$x_{2}$都是1，激活函数将变成$\frac{1}{2}$，$\frac{1}{2}$，$\frac{1}{4}$，$\frac{1}{4}$，$\frac{1}{8}$，$\frac{1}{8}$等，直到最后一项变成$\frac{1}{2^{L}}$，所以作为自定义函数，激活函数的值将以指数级下降，它是与网络层数数量$L$相关的函数，在深度网络中，激活函数以指数级递减。</p><p>上面的情况对于导数也是同样的道理，所以在计算梯度时，根据情况的不同，梯度函数会以指数级递增或者递减，导致训练导数难度上升，梯度下降算法的步长会变得非常非常小，需要训练的时间将会非常长。</p><p>在梯度函数上出现的以指数级递增或者递减的情况就分别称为梯度爆炸或者梯度消失。</p><h3 id="1-11-神经网络的权重初始化（Weight-Initialization-for-Deep-NetworksVanishing-Exploding-gradients）"><a href="#1-11-神经网络的权重初始化（Weight-Initialization-for-Deep-NetworksVanishing-Exploding-gradients）" class="headerlink" title="1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing / Exploding gradients）"></a>1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing / Exploding gradients）</h3><p>以一个单个神经元为例子：</p><p>单个神经元可能有4个输入特征，从$x_{1}$到$x_{4}$，经过$a=g(z)$处理，最终得到$\hat{y}$，稍后讲深度网络时，这些输入表示为$a^{[l]}$，暂时我们用$x$表示。</p><p>$z = w_{1}x_{1} + w_{2}x_{2} + \ldots +w_{n}x_{n}$，$b=0$，暂时忽略$b$，为了预防$z$值过大或过小，你可以看到$n$越大，你希望$w_{i}$越小，因为$z$是$w_{i}x_{i}$的和，如果你把很多此类项相加，希望每项值更小，最合理的方法就是设置$w_{i}=\frac{1}{n}$，$n$表示神经元的输入特征数量，实际上，你要做的就是设置某层权重矩阵$w^{[l]} = np.random.randn( \text{shape})*\text{np.}\text{sqrt}(\frac{1}{n^{[l-1]}})$，$n^{[l - 1]}$就是我喂给第$l$层神经单元的数量（即第$l-1$层神经元数量）。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568445633011.png" alt="1568445633011"></p><p>结果，如果你是用的是<strong>Relu</strong>激活函数，而不是$\frac{1}{n}$，方差设置为$\frac{2}{n}$，效果会更好。你常常发现，初始化时，尤其是使用<strong>Relu</strong>激活函数时，$g^{[l]}(z) =Relu(z)$,它取决于你对随机变量的熟悉程度，这是高斯随机变量，然后乘以它的平方根，也就是引用这个方差$\frac{2}{n}$。这里，我用的是$n^{[l - 1]}$，因为本例中，逻辑回归的特征是不变的。但一般情况下$l$层上的每个神经元都有$n^{[l - 1]}$个输入。如果激活函数的输入特征被零均值和标准方差化，方差是1，$z$也会调整到相似范围，这就没解决问题（梯度消失和爆炸问题）。但它确实降低了梯度消失和爆炸问题，因为它给权重矩阵$w$设置了合理值，你也知道，它不能比1大很多，也不能比1小很多，所以梯度没有爆炸或消失过快。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568445736240.png" alt="1568445736240"></p><p>对于几个其它变体函数，如<strong>tanh</strong>激活函数，有篇论文提到，常量1比常量2的效率更高，对于<strong>tanh</strong>函数来说，它是$\sqrt{\frac{1}{n^{[l-1]}}}$，这里平方根的作用与这个公式作用相同($\text{np.}\text{sqrt}(\frac{1}{n^{[l-1]}})$)，它适用于<strong>tanh</strong>激活函数，被称为<strong>Xavier</strong>初始化。<strong>Yoshua Bengio</strong>和他的同事还提出另一种方法，你可能在一些论文中看到过，它们使用的是公式$\sqrt{\frac{2}{n^{[l-1]} + n^{\left[l\right]}}}$。其它理论已对此证明，但如果你想用<strong>Relu</strong>激活函数，也就是最常用的激活函数，我会用这个公式$\text{np.}\text{sqrt}(\frac{2}{n^{[l-1]}})$，如果使用<strong>tanh</strong>函数，可以用公式$\sqrt{\frac{1}{n^{[l-1]}}}$，有些作者也会使用这个函数。</p><p>实际上，我认为所有这些公式只是给你一个起点，它们给出初始化权重矩阵的方差的默认值，如果你想添加方差，方差参数则是另一个你需要调整的超级参数，可以给公式$\text{np.}\text{sqrt}(\frac{2}{n^{[l-1]}})$添加一个乘数参数，调优作为超级参数激增一份子的乘子参数。有时调优该超级参数效果一般，这并不是我想调优的首要超级参数，但我发现调优过程中产生的问题，虽然调优该参数能起到一定作用，但考虑到相比调优，其它超级参数的重要性，我通常把它的优先级放得比较低。</p><h3 id="1-12-梯度的数值逼近（Numerical-approximation-of-gradients）"><a href="#1-12-梯度的数值逼近（Numerical-approximation-of-gradients）" class="headerlink" title="1.12 梯度的数值逼近（Numerical approximation of gradients）"></a>1.12 梯度的数值逼近（Numerical approximation of gradients）</h3><p>使用双边误差的方法去逼近导数：</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568445919895.png" alt="1568445919895"></p><p>这两个宽度都是ε，所以三角形的宽度是$2\varepsilon$，高宽比值为$\frac{f(\theta + \varepsilon ) - (\theta -\varepsilon)}{2\varepsilon}$，它的期望值接近$g( \theta)$，$f( \theta)=\theta^{3}$传入参数值，$\frac {f\left( \theta + \varepsilon \right) - f(\theta -\varepsilon)}{2\varepsilon} = \frac{(1.01)^{3} - {(0.99)}^{3}}{2 \times0.01}=3.0001$，当$\theta =1$时，$g( \theta)=3\theta^{2} =3$，所以这两个$g(\theta)$值非常接近，逼近误差为0.0001。单边误差，即从$\theta $到$\theta +\varepsilon$之间的误差，$g( \theta)$的值为3.0301。</p><ul><li>双边导数：</li></ul><p>$f’(\theta) = \lim\limits_{\varepsilon \to 0}=\dfrac{f(\theta+\varepsilon)-(\theta-\varepsilon)}{2\varepsilon}$ 误差：$O(\varepsilon^{2})$</p><ul><li>单边导数：</li></ul><p>$f’(\theta) = \lim\limits_{\varepsilon \to 0}=\dfrac{f(\theta+\varepsilon)-(\theta)}{\varepsilon}$ 误差：$O(\varepsilon)$</p><h3 id="1-13-梯度检验（Gradient-checking）"><a href="#1-13-梯度检验（Gradient-checking）" class="headerlink" title="1.13 梯度检验（Gradient checking）"></a>1.13 梯度检验（Gradient checking）</h3><p>假设你的网络中含有下列参数，$W^{[1]}$和$b^{[1]}$……$W^{[l]}$和$b^{[l]}$，为了执行梯度检验，首先要做的就是，把所有参数转换成一个巨大的向量数据，你要做的就是把矩阵$W$转换成一个向量，把所有$W$矩阵转换成向量之后，做连接运算，得到一个巨型向量$\theta$，该向量表示为参数$\theta$，代价函数$J$是所有$W$和$b$的函数，现在你得到了一个$\theta$的代价函数$J$（即$J(\theta)$）。接着，你得到与$W$和$b$顺序相同的数据，你同样可以把$dW^{[1]}$和${db}^{[1]}$……${dW}^{[l]}$和${db}^{[l]}$转换成一个新的向量，用它们来初始化大向量$d\theta$，它与$\theta$具有相同维度。</p><p>同样的，把$dW^{[1]}$转换成矩阵，$db^{[1]}$已经是一个向量了，直到把${dW}^{[l]}$转换成矩阵，这样所有的$dW$都已经是矩阵，注意$dW^{[1]}$与$W^{[1]}$具有相同维度，$db^{[1]}$与$b^{[1]}$具有相同维度。经过相同的转换和连接运算操作之后，你可以把所有导数转换成一个大向量$d\theta$，它与$\theta$具有相同维度，现在的问题是$d\theta$和代价函数$J$的梯度或坡度有什么关系？</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568446320047.png" alt="1568446320047"></p><p>这就是实施梯度检验的过程，英语里通常简称为“<strong>grad check</strong>”，首先，我们要清楚$J$是超参数$\theta$的一个函数，你也可以将J函数展开为$J(\theta_{1},\theta_{2},\theta_{3},\ldots\ldots)$，不论超级参数向量$\theta$的维度是多少，为了实施梯度检验，你要做的就是循环执行，从而对每个$i$也就是对每个$\theta$组成元素计算$d\theta_{\text{approx}}[i]$的值，我使用双边误差，也就是</p><p>$d\theta_{\text{approx}}\left[i \right] = \frac{J\left( \theta_{1},\theta_{2},\ldots\theta_{i} + \varepsilon,\ldots \right) - J\left( \theta_{1},\theta_{2},\ldots\theta_{i} - \varepsilon,\ldots \right)}{2\varepsilon}$</p><p>只对$\theta_{i}$增加$\varepsilon$，其它项保持不变，因为我们使用的是双边误差，对另一边做同样的操作，只不过是减去$\varepsilon$，$\theta$其它项全都保持不变。</p><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568446360324.png" alt="1568446360324"></p><p>判断$d\theta_{approx}\approx d\theta$是否接近</p><p>$\dfrac {||d\theta_{approx}-d\theta||_{2}}{||d\theta_{approx}||_{2}+||d\theta||_{2}}$</p><p>$||\cdot ||_{2}$表示欧几里得范数，它是误差平方之和，然后求平方根，得到的欧氏距离。</p><h3 id="1-14-梯度检验应用的注意事项（Gradient-Checking-Implementation-Notes）"><a href="#1-14-梯度检验应用的注意事项（Gradient-Checking-Implementation-Notes）" class="headerlink" title="1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）"></a>1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</h3><p><img src="/2019/09/14/deeplearning-ai笔记（2-1）/1568446685321.png" alt="1568446685321"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p><p><a href="https://blog.csdn.net/Koala_Tree/article/details/79913655" target="_blank" rel="noopener">https://blog.csdn.net/Koala_Tree/article/details/79913655</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/09/09/第四章-朴素贝叶斯法/" rel="next" title="第四章-朴素贝叶斯法"><i class="fa fa-chevron-left"></i> 第四章-朴素贝叶斯法</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/kikyo.jpg" alt="Kikyō"><p class="site-author-name" itemprop="name">Kikyō</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">3</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习的实践层面（Practical-aspects-of-Deep-Learning）"><span class="nav-text">深度学习的实践层面（Practical aspects of Deep Learning）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-训练，验证，测试集（Train-Dev-Test-sets）"><span class="nav-text">1.1 训练，验证，测试集（Train / Dev / Test sets）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-偏差，方差（Bias-Variance）"><span class="nav-text">1.2 偏差，方差（Bias /Variance）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-机器学习基础（Basic-Recipe-for-Machine-Learning）"><span class="nav-text">1.3 机器学习基础（Basic Recipe for Machine Learning）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-正则化（Regularization）"><span class="nav-text">1.4 正则化（Regularization）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic-regression"><span class="nav-text">Logistic regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural-network"><span class="nav-text">Neural network</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-为什么正则化有利于预防过拟合呢？（Why-regularization-reduces-overfitting-）"><span class="nav-text">1.5 为什么正则化有利于预防过拟合呢？（Why regularization reduces overfitting?）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-dropout-正则化（Dropout-Regularization）"><span class="nav-text">1.6 dropout 正则化（Dropout Regularization）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#inverted-dropout（反向随机失活）"><span class="nav-text">inverted dropout（反向随机失活）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-理解-dropout（Understanding-Dropout）"><span class="nav-text">1.7 理解 dropout（Understanding Dropout）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-其他正则化方法（Other-regularization-methods）"><span class="nav-text">1.8 其他正则化方法（Other regularization methods）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据扩增（Data-augmentation）"><span class="nav-text">数据扩增（Data augmentation）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Early-stopping"><span class="nav-text">Early stopping</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-归一化输入（Normalizing-inputs）"><span class="nav-text">1.9 归一化输入（Normalizing inputs）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#使用归一化的原因："><span class="nav-text">使用归一化的原因：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-梯度消失-梯度爆炸（Vanishing-Exploding-gradients）"><span class="nav-text">1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-11-神经网络的权重初始化（Weight-Initialization-for-Deep-NetworksVanishing-Exploding-gradients）"><span class="nav-text">1.11 神经网络的权重初始化（Weight Initialization for Deep NetworksVanishing / Exploding gradients）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-12-梯度的数值逼近（Numerical-approximation-of-gradients）"><span class="nav-text">1.12 梯度的数值逼近（Numerical approximation of gradients）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-13-梯度检验（Gradient-checking）"><span class="nav-text">1.13 梯度检验（Gradient checking）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-14-梯度检验应用的注意事项（Gradient-Checking-Implementation-Notes）"><span class="nav-text">1.14 梯度检验应用的注意事项（Gradient Checking Implementation Notes）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Kikyō</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">51.8k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script></body></html><!-- rebuild by neat -->