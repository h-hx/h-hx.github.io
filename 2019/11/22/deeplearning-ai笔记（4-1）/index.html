<!-- build time:Mon Nov 01 2021 12:05:46 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|DejaVu Sans Mono for Powerline:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-flower.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-flower.png?v=5.1.4"><meta name="keywords" content="深度学习,"><meta name="description" content="卷积神经网络（Foundations of Convolutional Neural Networks）1.1 计算机视觉（Computer vision）1.2 边缘检测示例（Edge detection example）卷积运算是卷积神经网络最基本的组成部分。给了这样一张图片，让电脑去搞清楚这张照片里有什么物体，你可能做的第一件事是检测图片中的垂直边缘。比如说，在这张图片中的栏杆就对应垂直线，"><meta name="keywords" content="深度学习"><meta property="og:type" content="article"><meta property="og:title" content="deeplearning-ai笔记（4-1）"><meta property="og:url" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/index.html"><meta property="og:site_name"><meta property="og:description" content="卷积神经网络（Foundations of Convolutional Neural Networks）1.1 计算机视觉（Computer vision）1.2 边缘检测示例（Edge detection example）卷积运算是卷积神经网络最基本的组成部分。给了这样一张图片，让电脑去搞清楚这张照片里有什么物体，你可能做的第一件事是检测图片中的垂直边缘。比如说，在这张图片中的栏杆就对应垂直线，"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574425806653.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574425992554.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574426148469.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574426265432.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574426535785.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574427192455.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574427247883.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574427419645.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574427479369.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574428027302.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574581233649.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574581311100.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574581434218.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574581616769.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574581757040.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574581944179.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574582026932.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574582305146.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574582709587.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575426404894.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575426742430.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575426830176.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575426877726.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575426914512.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575427062597.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575427163972.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575427788393.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575427930692.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575428016532.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575428030916.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575428248803.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575428263412.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575428281536.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575428337870.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575428362014.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575429027395.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575429120219.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575429320207.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575429359087.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575873725540.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575873842965.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575873963955.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575874777911.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/Convolution_schematic.gif"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575875000031.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575875378455.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575876789888.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575877402479.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575878492910.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575879142012.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575879266978.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575879505718.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575981281395.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575981326134.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575981404092.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575981906114.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575982216129.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575982514022.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575983453744.png"><meta property="og:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1575983473339.png"><meta property="og:updated_time" content="2019-12-11T01:10:16.035Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="deeplearning-ai笔记（4-1）"><meta name="twitter:description" content="卷积神经网络（Foundations of Convolutional Neural Networks）1.1 计算机视觉（Computer vision）1.2 边缘检测示例（Edge detection example）卷积运算是卷积神经网络最基本的组成部分。给了这样一张图片，让电脑去搞清楚这张照片里有什么物体，你可能做的第一件事是检测图片中的垂直边缘。比如说，在这张图片中的栏杆就对应垂直线，"><meta name="twitter:image" content="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/1574425806653.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"right",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/"><title>deeplearning-ai笔记（4-1） |</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title"></span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-essay"><a href="/essays/" rel="section"><i class="menu-item-icon fa fa-fw fa-essay"></i><br>随笔</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/22/deeplearning-ai笔记（4-1）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Kikyō"><meta itemprop="description" content><meta itemprop="image" content="/images/kikyo.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content></span><header class="post-header"><h1 class="post-title" itemprop="name headline">deeplearning-ai笔记（4-1）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-22T20:05:16+08:00">2019-11-22 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2019-12-11T09:10:16+08:00">2019-12-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deeplearning-ai笔记/" itemprop="url" rel="index"><span itemprop="name">deeplearning.ai笔记</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">12.7k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">69</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="卷积神经网络（Foundations-of-Convolutional-Neural-Networks）"><a href="#卷积神经网络（Foundations-of-Convolutional-Neural-Networks）" class="headerlink" title="卷积神经网络（Foundations of Convolutional Neural Networks）"></a>卷积神经网络（Foundations of Convolutional Neural Networks）</h2><h3 id="1-1-计算机视觉（Computer-vision）"><a href="#1-1-计算机视觉（Computer-vision）" class="headerlink" title="1.1 计算机视觉（Computer vision）"></a>1.1 计算机视觉（Computer vision）</h3><h3 id="1-2-边缘检测示例（Edge-detection-example）"><a href="#1-2-边缘检测示例（Edge-detection-example）" class="headerlink" title="1.2 边缘检测示例（Edge detection example）"></a>1.2 边缘检测示例（Edge detection example）</h3><p>卷积运算是卷积神经网络最基本的组成部分。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574425806653.png" alt="1574425806653"></p><p>给了这样一张图片，让电脑去搞清楚这张照片里有什么物体，你可能做的第一件事是检测图片中的垂直边缘。比如说，在这张图片中的栏杆就对应垂直线，与此同时，这些行人的轮廓线某种程度上也是垂线，这些线是垂直边缘检测器的输出。同样，你可能也想检测水平边缘，比如说这些栏杆就是很明显的水平线，它们也能被检测到，结果在这。所以如何在图像中检测这些边缘？</p><h4 id="垂直边缘检测"><a href="#垂直边缘检测" class="headerlink" title="垂直边缘检测"></a><strong>垂直边缘检测</strong></h4><p>一个6×6的灰度图像，6×6×1的矩阵。为了检测图像中的垂直边缘，可以构造一个3×3矩阵（称为过滤器，有时候也称作“核”）。然后对这个6×6的图像进行卷积运算，卷积运算用“$*$”来表示，用3×3的过滤器对其进行卷积。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574425992554.png" alt="1574425992554"></p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574426148469.png" alt="1574426148469"></p><p>这个卷积运算的输出将会是一个4×4（6-3+1）的矩阵，你可以将它看成一个4×4的图像。</p><p>计算过程：</p><p>$\begin{bmatrix} 3 \times 1 &amp; 0 \times 0 &amp; 1 \times \left(1 \right) \\ 1 \times 1 &amp; 5 \times 0 &amp; 8 \times \left( - 1 \right) \\ 2 \times1 &amp; 7 \times 0 &amp; 2 \times \left( - 1 \right) \\ \end{bmatrix} = \begin{bmatrix}3 &amp; 0 &amp; - 1 \\ 1 &amp; 0 &amp; - 8 \\ 2 &amp; 0 &amp; - 2 \\\end{bmatrix}$，然后将该矩阵每个元素相加得到最左上角的元素，即$3+1+2+0+0 +0+(-1)+(-8) +(-2)=-5$。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574426265432.png" alt="1574426265432"></p><p>为什么这个可以做垂直边缘检测呢？</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574426535785.png" alt="1574426535785"></p><p>在这个例子中，在输出图像中间的亮处，表示在图像中间有一个特别明显的垂直边缘。从垂直边缘检测中可以得到的启发是，因为我们使用3×3的矩阵（过滤器），所以垂直边缘是一个3×3的区域，左边是明亮的像素，中间的并不需要考虑，右边是深色像素。在这个6×6图像的中间部分，明亮的像素在左边，深色的像素在右边，就被视为一个垂直边缘，卷积运算提供了一个方便的方法来发现图像中的垂直边缘。</p><h3 id="1-3-更多边缘检测内容（More-edge-detection）"><a href="#1-3-更多边缘检测内容（More-edge-detection）" class="headerlink" title="1.3 更多边缘检测内容（More edge detection）"></a>1.3 更多边缘检测内容（More edge detection）</h3><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574427192455.png" alt="1574427192455"></p><p>现在这幅图有什么变化呢？它的颜色被翻转了，变成了左边比较暗，而右边比较亮。现在亮度为10的点跑到了右边，为0的点则跑到了左边。如果你用它与相同的过滤器进行卷积，最后得到的图中间会是-30，而不是30。如果你将矩阵转换为图片，就会是该矩阵下面图片的样子。现在中间的过渡部分被翻转了，之前的30翻转成了-30，表明是由暗向亮过渡，而不是由亮向暗过渡。</p><h4 id="水平边缘检测"><a href="#水平边缘检测" class="headerlink" title="水平边缘检测"></a>水平边缘检测</h4><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574427247883.png" alt="1574427247883"></p><h4 id="更复杂的例子"><a href="#更复杂的例子" class="headerlink" title="更复杂的例子"></a>更复杂的例子</h4><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574427419645.png" alt="1574427419645"></p><p>这里的30（右边矩阵中绿色方框标记元素）代表了左边这块3×3的区域（左边矩阵绿色方框标记部分），这块区域确实是上边比较亮，而下边比较暗的，所以它在这里发现了一条正边缘。而这里的-30（右边矩阵中紫色方框标记元素）又代表了左边另一块区域（左边矩阵紫色方框标记部分），这块区域确实是底部比较亮，而上边则比较暗，所以在这里它是一条负边。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574427479369.png" alt="1574427479369"></p><p>我们现在所使用的都是相对很小的图片，仅有6×6。但这些中间的数值，比如说这个10（右边矩阵中黄色方框标记元素）代表的是左边这块区域（左边6×6矩阵中黄色方框标记的部分）。这块区域左边两列是正边，右边一列是负边，正边和负边的值加在一起得到了一个中间值。但假如这个一个非常大的1000×1000的类似这样棋盘风格的大图，就不会出现这些亮度为10的过渡带了，因为图片尺寸很大，这些中间值就会变得非常小。</p><h4 id="其他过滤器"><a href="#其他过滤器" class="headerlink" title="其他过滤器"></a>其他过滤器</h4><p>$\begin{bmatrix}1 &amp; 0 &amp; - 1 \\ 2 &amp; 0 &amp; - 2 \\ 1 &amp; 0 &amp; - 1 \\\end{bmatrix}$叫做<strong>Sobel</strong>的过滤器，它的优点在于增加了中间一行元素的权重，这使得结果的鲁棒性会更高一些。</p><p>$\begin{bmatrix} 3&amp; 0 &amp; - 3 \\ 10 &amp; 0 &amp; - 10 \\ 3 &amp; 0 &amp; - 3 \\\end{bmatrix}$叫做<strong>Scharr</strong>过滤器，它有着和之前完全不同的特性，实际上也是一种垂直边缘检测，如果你将其翻转90度，你就能得到对应水平边缘检测。</p><p>把这9个数字当成参数的过滤器，通过反向传播，可以学习这种$\begin{bmatrix}1 &amp; 0 &amp; - 1 \\ 1 &amp; 0 &amp; - 1 \\ 1 &amp; 0 &amp; - 1 \\\end{bmatrix}$的过滤器，或者<strong>Sobel</strong>过滤器和<strong>Scharr</strong>过滤器。</p><h3 id="1-4-Padding"><a href="#1-4-Padding" class="headerlink" title="1.4 Padding"></a>1.4 Padding</h3><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574428027302.png" alt="1574428027302"></p><p>用一个3×3的过滤器卷积一个6×6的图像，最后会得到一个4×4的输出。</p><p><strong>这样的话会有两个缺点：</strong></p><p>1.每次做卷积操作，你的图像就会缩小</p><p>2.角落或者边缘区域的像素点在输出中采用较少，意味着你丢掉了图像边缘位置的许多信息</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574581233649.png" alt="1574581233649"></p><p><strong>解决方法：</strong>沿着图像边缘填充像素</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574581311100.png" alt="1574581311100"></p><p>至于选择填充多少像素，通常有两个选择，分别叫做<strong>Valid</strong>卷积和<strong>Same</strong>卷积。</p><p><strong>Valid</strong>卷积意味着不填充，这样的话，如果你有一个$n×n$的图像，用一个$f×f$的过滤器卷积，它将会给你一个$(n-f+1)×(n-f+1)$维的输出。</p><p><strong>Same</strong>卷积，那意味你填充后，你的输出大小和输入大小是一样的。$p=(f-1)/2$</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574581434218.png" alt="1574581434218"></p><p>计算机视觉中，$f$通常是奇数，甚至可能都是这样。</p><p><strong>可能原因</strong>：</p><p>1.如果$f$是一个偶数，那么你只能使用一些不对称填充。只有$f$是奇数的情况下，<strong>Same</strong>卷积才会有自然的填充，我们可以以同样的数量填充四周，而不是左边填充多一点，右边填充少一点，这样不对称的填充。</p><p>2.当你有一个奇数维过滤器，比如3×3或者5×5的，它就有一个中心点。</p><h3 id="1-5-卷积步长（Strided-convolutions）"><a href="#1-5-卷积步长（Strided-convolutions）" class="headerlink" title="1.5 卷积步长（Strided convolutions）"></a>1.5 卷积步长（Strided convolutions）</h3><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574581616769.png" alt="1574581616769"></p><p>用3×3的矩阵卷积一个7×7的矩阵，得到一个3×3的输出。输入和输出的维度是由下面的公式决定的。如果你用一个$f×f$的过滤器卷积一个$n×n$的图像，你的<strong>padding</strong>为$p$，步幅为$s$，在这个例子中$s=2$，你会得到一个输出，因为现在你不是一次移动一个步子，而是一次移动$s$个步子，输出于是变为$\frac{n+2p - f}{s} + 1 \times \frac{n+2p - f}{s} + 1$</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574581757040.png" alt="1574581757040"></p><h4 id="互相关和卷积"><a href="#互相关和卷积" class="headerlink" title="互相关和卷积"></a>互相关和卷积</h4><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574581944179.png" alt="1574581944179"></p><p>在深度学习中，我们称为的卷积运算实则没有卷积核变换为镜像的这一步操作，因为在权重学习的角度，变换是没有必要的。深度学习的卷积操作在数学上准确度来说称为<strong>互相关</strong>（cross-correlation）。</p><h3 id="1-6-三维卷积（Convolutions-over-volumes）"><a href="#1-6-三维卷积（Convolutions-over-volumes）" class="headerlink" title="1.6 三维卷积（Convolutions over volumes）"></a>1.6 三维卷积（Convolutions over volumes）</h3><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574582026932.png" alt="1574582026932"></p><p>这个过滤器是3×3×3的，如果你想检测图像红色通道的边缘，那么你可以将第一个过滤器设为$\begin{bmatrix}1 &amp; 0 &amp; - 1 \\ 1 &amp; 0 &amp; - 1 \\ 1 &amp; 0 &amp; - 1 \\\end{bmatrix}$，和之前一样，而绿色通道全为0，$\begin{bmatrix} 0&amp; 0 &amp; 0 \\ 0 &amp;0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\\end{bmatrix}$，蓝色也全为0。如果你把这三个堆叠在一起形成一个3×3×3的过滤器，那么这就是一个检测垂直边界的过滤器，但只对红色通道有用。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574582305146.png" alt="1574582305146"></p><p>如果你有一个$n \times n \times n_{c}$（通道数）的输入图像，这里的$n_{c}$就是通道数目，然后卷积上一个$f×f×n_{c}$，然后你就得到了$（n-f+1）×（n-f+1）×n_{c^{‘}}$，这里$n_{c^{‘}}$其实就是下一层的通道数。</p><h3 id="1-7-单层卷积网络（One-layer-of-a-convolutional-network）"><a href="#1-7-单层卷积网络（One-layer-of-a-convolutional-network）" class="headerlink" title="1.7 单层卷积网络（One layer of a convolutional network）"></a>1.7 单层卷积网络（One layer of a convolutional network）</h3><p>和普通的神经网络单层前向传播的过程类似，卷积神经网络也是一个先由输入和权重及偏置做线性运算，然后得到的结果输入一个激活函数中，得到最终的输出：</p><script type="math/tex;mode=display">z^{[1]}=w^{[1]}a^{[0]}+b^{[1]}\\a^{[1]}=g(z^{[1]})</script><p>不同点是在卷积神经网络中，权重和输入进行的是卷积运算。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1574582709587.png" alt="1574582709587"></p><p>在一个卷积层中，如果我们有10个 3×3×3 大小的卷积核，那么加上每个卷积核对应的偏置，则对于一个卷积层，我们共有的参数个数为 ：</p><p>$(3\times3\times3+1)\times10 = 280$</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575426404894.png" alt="1575426404894"></p><p>不论输入图片有多大，参数始终都是280个。用这10个过滤器来提取特征，如垂直边缘，水平边缘和其它特征。即使这些图片很大，参数却很少，这就是卷积神经网络的一个特征，叫作“<strong>避免过拟合</strong>”。</p><p><strong>卷积层的各种标记的总结：</strong></p><p>用$f^{[l]}$表示过滤器大小，上标$\lbrack l\rbrack$表示$l$层中过滤器大小为$f×f$。</p><p>用$p^{[l]}$来标记<strong>padding</strong>的数量，用$s^{[l]}$标记步幅。</p><p>图片大小为$n_{H}^{\left\lbrack l - 1 \right\rbrack} \times n_{W}^{\left\lbrack l - 1 \right\rbrack} \times n_{c}^{\left\lbrack l - 1\right\rbrack}$，$l$层的输入就是上一层的输出，因此上标要用$\lbrack l - 1\rbrack$。</p><p>权重为$f^{[l]}\times f^{[l]}\times n^{[l-1]}_{C}\times n^{[l]}_{C}$</p><p>$n_{H}^{[l]} = \lfloor\frac{n_{H}^{\left\lbrack l - 1 \right\rbrack} +2p^{[l]} - f^{[l]}}{s^{[l]}} +1\rfloor$，$n_{W}^{[l]} = \lfloor\frac{n_{W}^{\left\lbrack l - 1 \right\rbrack} +2p^{[l]} - f^{[l]}}{s^{[l]}} +1\rfloor$。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575426742430.png" alt="1575426742430"></p><h3 id="1-8-简单卷积网络示例（A-simple-convolution-network-example）"><a href="#1-8-简单卷积网络示例（A-simple-convolution-network-example）" class="headerlink" title="1.8 简单卷积网络示例（A simple convolution network example）"></a>1.8 简单卷积网络示例（A simple convolution network example）</h3><p>假如有大小是39×39×3的图片，用于辨别图片中有没有猫，用0或1表示。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575426830176.png" alt="1575426830176"></p><p>假设第一层我们用一个3×3的过滤器来提取特征，那么$f^{[1]} = 3$，因为过滤器时3×3的矩阵。$s^{[1]} = 1$，$p^{[1]} =0$，所以高度和宽度使用<strong>valid</strong>卷积。如果有10个过滤器，神经网络下一层的激活值为37×37×10。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575426877726.png" alt="1575426877726"></p><p>假设还有另外一个卷积层，采用过滤器是5×5的矩阵，即$f^{\left\lbrack 2 \right\rbrack} = 5$步幅为2，即$s^{\left\lbrack 2 \right\rbrack} = 2$。<strong>padding</strong>为0，即$p^{\left\lbrack 2 \right\rbrack} = 0$，且有20个过滤器。所以其输出结果为17×17×20。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575426914512.png" alt="1575426914512"></p><p>构建最后一个卷积层，假设过滤器还是5×5，步幅为2，即$f^{\left\lbrack 2 \right\rbrack} = 5$，$s^{\left\lbrack 3 \right\rbrack} = 2$，假设使用了40个过滤器。<strong>padding</strong>为0，最后结果为7×7×40。</p><p>为图片提取了7×7×40个特征后进行平滑处理，然后把这个长向量填充到<strong>softmax</strong>回归函数中。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575427062597.png" alt="1575427062597"></p><p>一个典型的卷积神经网络通常有三层，一个是卷积层（<strong>COV</strong>），一个是池化层(<strong>POOL</strong>)，最后一个是全连接层（<strong>FC</strong>）。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575427163972.png" alt="1575427163972"></p><h3 id="1-9-池化层（Pooling-layers）"><a href="#1-9-池化层（Pooling-layers）" class="headerlink" title="1.9 池化层（Pooling layers）"></a>1.9 池化层（Pooling layers）</h3><p>执行最大池化的树池是一个2×2矩阵。执行过程非常简单，把4×4的输入拆分成不同的区域，我把这个区域用不同颜色来标记。对于2×2的输出，输出的每个元素都是其对应颜色区域中的最大元素值。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575427788393.png" alt="1575427788393"></p><p>这是一个2×2矩阵，即$f=2$，步幅是2，即$s=2$。</p><p>最大池化的输入就是$n_{H} \times n_{W} \times n_{c}$，假设没有<strong>padding</strong>，则输出$\lfloor\frac{n_{H} - f}{s} +1\rfloor \times \lfloor\frac{n_{w} - f}{s} + 1\rfloor \times n_{c}$。输入通道与输出通道个数相同，因为我们对每个通道都做了池化。需要注意的一点是，池化过程中没有需要学习的参数。执行反向传播时，反向传播没有参数适用于最大池化。只有这些设置过的超参数，可能是手动设置的，也可能是通过交叉验证设置的。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575427930692.png" alt="1575427930692"></p><h3 id="1-10-卷积神经网络示例（Convolutional-neural-network-example）"><a href="#1-10-卷积神经网络示例（Convolutional-neural-network-example）" class="headerlink" title="1.10 卷积神经网络示例（Convolutional neural network example）"></a>1.10 卷积神经网络示例（Convolutional neural network example）</h3><p>有一张大小为32×32×3的输入图片，用于手写体数字识别。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575428016532.png" alt="1575428016532"></p><p>第一层使用过滤器大小为5×5，步幅是1，<strong>paddin</strong>g是0，过滤器个数为6，那么输出为28×28×6。将这层标记为<strong>CONV1</strong>，它用了6个过滤器，增加了偏差，应用了非线性函数，可能是<strong>ReLU</strong>非线性函数，最后输出<strong>CONV1</strong>的结果。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575428030916.png" alt="1575428030916"></p><p>然后构建一个池化层，选择用最大池化，参数$f=2$，$s=2$，因此，28×28变成了14×14，通道数量保持不变，所以最终输出为14×14×6，将该输出标记为<strong>POOL1</strong>。</p><p>一类卷积是一个卷积层和一个池化层一起作为一层，这就是神经网络的<strong>Layer1</strong>。另一类卷积是把卷积层作为一层，而池化层单独作为一层。人们在计算神经网络有多少层时，通常只统计具有权重和参数的层。因为池化层没有权重和参数，只有一些超参数。这里，我们把<strong>CONV1</strong>和<strong>POOL1</strong>共同作为一个卷积，并标记为<strong>Layer1</strong>。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575428248803.png" alt="1575428248803"></p><p>再为它构建一个卷积层，过滤器大小为5×5，步幅为1，使用用16个过滤器，最后输出一个10×10×16的矩阵，标记为<strong>CONV2</strong>。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575428263412.png" alt="1575428263412"></p><p>然后做最大池化，超参数$f=2$，$s=2$。你大概可以猜出结果，$f=2$，$s=2$，高度和宽度会减半，最后输出为5×5×16，标记为<strong>POOL2</strong>，这就是神经网络的第二个卷积层，即<strong>Layer2</strong>。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575428281536.png" alt="1575428281536"></p><p>5×5×16矩阵包含400个元素，现在将<strong>POOL2</strong>平整化为一个大小为400的一维向量，然后利用这400个单元构建下一层。下一层含有120个单元，这是第一个全连接层，标记为<strong>FC3</strong>。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575428337870.png" alt="1575428337870"></p><p>然后我们对这个120个单元再添加一个全连接层，这层更小，假设它含有84个单元，标记为<strong>FC4</strong>。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575428362014.png" alt="1575428362014"></p><p>最后，用这84个单元填充一个<strong>softmax</strong>单元。如果我们想通过手写数字识别来识别手写0-9这10个数字，这个<strong>softmax</strong>就会有10个输出。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575429027395.png" alt="1575429027395"></p><p>池化层和最大池化层没有参数；卷积层的参数相对较少，全连接层有着大量参数。</p><h3 id="1-11-为什么使用卷积？（Why-convolutions-）"><a href="#1-11-为什么使用卷积？（Why-convolutions-）" class="headerlink" title="1.11 为什么使用卷积？（Why convolutions?）"></a>1.11 为什么使用卷积？（Why convolutions?）</h3><p>和只用全连接层相比，卷积层的两个主要优势在于参数共享和稀疏连接，举例说明一下。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575429120219.png" alt="1575429120219"></p><p>卷积神经网络仅有$6×(5×5+1)=156$个参数，而普通的全连接网络有$3072×4704≈14M$个参数。</p><p>卷积网络映射这么少参数有两个原因：</p><p><strong>参数共享</strong>： 一个特征检测器（filter）对图片的一部分有用的同时也有可能对图片的另外一部分有用。</p><p><strong>连接的稀疏性</strong>：在每一层中，每个输出值只取决于少量的输入。</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575429320207.png" alt="1575429320207"></p><h4 id="训练卷积神经网络"><a href="#训练卷积神经网络" class="headerlink" title="训练卷积神经网络"></a>训练卷积神经网络</h4><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575429359087.png" alt="1575429359087"></p><p>$x$表示一张图片，$\hat{y}$是二进制标记或某个重要标记。我们选定了一个卷积神经网络，输入图片，增加卷积层和池化层，然后添加全连接层，最后输出一个<strong>softmax</strong>，即$\hat{y}$。卷积层和全连接层有不同的参数$w$和偏差$b$。</p><p>$\text{Cost}\ J = \frac{1}{m}\sum\limits_{i = 1}^{m}{L(\hat{y}^{(i)},y^{(i)})}$</p><h2 id="Part-1：Convolutional-Model-step-by-step"><a href="#Part-1：Convolutional-Model-step-by-step" class="headerlink" title="Part 1：Convolutional Model: step by step"></a>Part 1：Convolutional Model: step by step</h2><p>Welcome to Course 4’s first assignment! In this assignment, you will implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and (optionally) backward propagation.</p><p><strong>Notation</strong>:</p><ul><li>Superscript $[l]$ denotes an object of the $l^{th}$ layer.<ul><li>Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.</li></ul></li></ul><ul><li><p>Superscript $(i)$ denotes an object from the $i^{th}$ example.</p><ul><li>Example: $x^{(i)}$ is the $i^{th}$ training example input.</li></ul></li><li><p>Subscript $i$ denotes the $i^{th}$ entry of a vector.</p><ul><li>Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$, assuming this is a fully connected (FC) layer.</li></ul></li><li><p>$n_H$, $n_W$ and $n_C$ denote respectively the height, width and number of channels of a given layer. If you want to reference a specific layer $l$, you can also write $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$.</p></li><li>$n_{H_{prev}}$, $n_{W_{prev}}$ and $n_{C_{prev}}$ denote respectively the height, width and number of channels of the previous layer. If referencing a specific layer $l$, this could also be denoted $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$.</li></ul><p>We assume that you are already familiar with <code>numpy</code> and/or have completed the previous courses of the specialization. Let’s get started!</p><h3 id="Updates"><a href="#Updates" class="headerlink" title="Updates"></a>Updates</h3><h4 id="If-you-were-working-on-the-notebook-before-this-update…"><a href="#If-you-were-working-on-the-notebook-before-this-update…" class="headerlink" title="If you were working on the notebook before this update…"></a>If you were working on the notebook before this update…</h4><ul><li>The current notebook is version “v2a”.</li><li>You can find your original work saved in the notebook with the previous version name (“v2”)</li><li>To view the file directory, go to the menu “File-&gt;Open”, and this will open a new tab that shows the file directory.</li></ul><h4 id="List-of-updates"><a href="#List-of-updates" class="headerlink" title="List of updates"></a>List of updates</h4><ul><li>clarified example used for padding function. Updated starter code for padding function.</li><li><code>conv_forward</code> has additional hints to help students if they’re stuck.</li><li><code>conv_forward</code> places code for <code>vert_start</code> and <code>vert_end</code> within the <code>for h in range(...)</code> loop; to avoid redundant calculations. Similarly updated <code>horiz_start</code> and <code>horiz_end</code>. <strong>Thanks to our mentor Kevin Brown for pointing this out.</strong></li><li><code>conv_forward</code> breaks down the <code>Z[i, h, w, c]</code> single line calculation into 3 lines, for clarity.</li><li><code>conv_forward</code> test case checks that students don’t accidentally use n_H_prev instead of n_H, use n_W_prev instead of n_W, and don’t accidentally swap n_H with n_W</li><li><code>pool_forward</code> properly nests calculations of <code>vert_start</code>, <code>vert_end</code>, <code>horiz_start</code>, and <code>horiz_end</code> to avoid redundant calculations.</li><li><code>pool_forward</code> has two new test cases that check for a correct implementation of stride (the height and width of the previous layer’s activations should be large enough relative to the filter dimensions so that a stride can take place).</li><li><code>conv_backward</code>: initialize <code>Z</code> and <code>cache</code> variables within unit test, to make it independent of unit testing that occurs in the <code>conv_forward</code> section of the assignment.</li><li><strong>Many thanks to our course mentor, Paul Mielke, for proposing these test cases.</strong></li></ul><h3 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h3><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><a href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://matplotlib.org" target="_blank" rel="noopener">matplotlib</a> is a library to plot graphs in Python.</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="2-Outline-of-the-Assignment"><a href="#2-Outline-of-the-Assignment" class="headerlink" title="2 - Outline of the Assignment"></a>2 - Outline of the Assignment</h3><p>You will be implementing the building blocks of a convolutional neural network! Each function you will implement will have detailed instructions that will walk you through the steps needed:</p><ul><li>Convolution functions, including:<ul><li>Zero Padding</li><li>Convolve window</li><li>Convolution forward</li><li>Convolution backward (optional)</li></ul></li><li>Pooling functions, including:<ul><li>Pooling forward</li><li>Create mask</li><li>Distribute value</li><li>Pooling backward (optional)</li></ul></li></ul><p>This notebook will ask you to implement these functions from scratch in <code>numpy</code>. In the next notebook, you will use the TensorFlow equivalents of these functions to build the following model:</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575873725540.png" alt="1575873725540"></p><p><strong>Note</strong> that for every forward function, there is its corresponding backward equivalent. Hence, at every step of your forward module you will store some parameters in a cache. These parameters are used to compute gradients during backpropagation.</p><h3 id="3-Convolutional-Neural-Networks"><a href="#3-Convolutional-Neural-Networks" class="headerlink" title="3 - Convolutional Neural Networks"></a>3 - Convolutional Neural Networks</h3><p>Although programming frameworks make convolutions easy to use, they remain one of the hardest concepts to understand in Deep Learning. A convolution layer transforms an input volume into an output volume of different size, as shown below.</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575873842965.png" alt="1575873842965"></p><p>In this part, you will build every step of the convolution layer. You will first implement two helper functions: one for zero padding and the other for computing the convolution function itself.</p><h4 id="3-1-Zero-Padding"><a href="#3-1-Zero-Padding" class="headerlink" title="3.1 - Zero-Padding"></a>3.1 - Zero-Padding</h4><p>Zero-padding adds zeros around the border of an image:</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575873963955.png" alt="1575873963955"></p><p>The main benefits of padding are the following:</p><ul><li><p>It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the “same” convolution, in which the height/width is exactly preserved after one layer.</p></li><li><p>It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.</p></li></ul><p><strong>Exercise</strong>: Implement the following function, which pads all the images of a batch of examples X with zeros. <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html" target="_blank" rel="noopener">Use np.pad</a>. Note if you want to pad the array “a” of shape $(5,5,5,5,5)$ with <code>pad = 1</code> for the 2nd dimension, <code>pad = 3</code> for the 4th dimension and <code>pad = 0</code> for the rest, you would do:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.pad(a, ((<span class="number">0</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">0</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">0</span>,<span class="number">0</span>)), mode=<span class="string">'constant'</span>, constant_values = (<span class="number">0</span>,<span class="number">0</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: zero_pad</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, </span></span><br><span class="line"><span class="string">    as illustrated in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images</span></span><br><span class="line"><span class="string">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>), (pad,pad), (pad,pad), (<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">x_pad = zero_pad(x, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x.shape =\n"</span>, x.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x_pad.shape =\n"</span>, x_pad.shape)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x[1,1] =\n"</span>, x[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"x_pad[1,1] =\n"</span>, x_pad[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">axarr[<span class="number">0</span>].set_title(<span class="string">'x'</span>)</span><br><span class="line">axarr[<span class="number">0</span>].imshow(x[<span class="number">0</span>,:,:,<span class="number">0</span>])</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">'x_pad'</span>)</span><br><span class="line">axarr[<span class="number">1</span>].imshow(x_pad[<span class="number">0</span>,:,:,<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575874777911.png" alt="1575874777911"></p><h4 id="3-2-Single-step-of-convolution"><a href="#3-2-Single-step-of-convolution" class="headerlink" title="3.2 - Single step of convolution"></a>3.2 - Single step of convolution</h4><p>In this part, implement a single step of convolution, in which you apply the filter to a single position of the input. This will be used to build a convolutional unit, which:</p><ul><li>Takes an input volume</li><li>Applies a filter at every position of the input</li><li>Outputs another volume (usually of different size)</li></ul><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/Convolution_schematic.gif" alt="Convolution_schematic"></p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575875000031.png" alt="1575875000031"></p><p>In a computer vision application, each value in the matrix on the left corresponds to a single pixel value, and we convolve a 3x3 filter with the image by multiplying its values element-wise with the original matrix, then summing them up and adding a bias. In this first step of the exercise, you will implement a single step of convolution, corresponding to applying a filter to just one of the positions to get a single real-valued output.</p><p>Later in this notebook, you’ll apply this function to multiple positions of the input to implement the full convolutional operation.</p><p><strong>Exercise</strong>: Implement conv_single_step(). <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html" target="_blank" rel="noopener">Hint</a>.</p><p><strong>Note</strong>: The variable b will be passed in as a numpy array. If we add a scalar (a float or integer) to a numpy array, the result is a numpy array. In the special case when a numpy array contains a single value, we can cast it as a float to convert it to a scalar.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_single_step</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span><span class="params">(a_slice_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation </span></span><br><span class="line"><span class="string">    of the previous layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Element-wise product between a_slice_prev and W. Do not add the bias yet.</span></span><br><span class="line">    s = a_slice_prev * W</span><br><span class="line">    <span class="comment"># Sum over all entries of the volume s.</span></span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    <span class="comment"># Add bias b to Z. Cast b to a float() so that Z results in a scalar value.</span></span><br><span class="line">    Z = Z + b</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">a_slice_prev = np.random.randn(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">W = np.random.randn(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Z = conv_single_step(a_slice_prev, W, b)</span><br><span class="line">print(<span class="string">"Z ="</span>, Z)</span><br><span class="line"><span class="comment"># Z = [[[-6.99908945]]]</span></span><br></pre></td></tr></table></figure><h4 id="3-3-Convolutional-Neural-Networks-Forward-pass"><a href="#3-3-Convolutional-Neural-Networks-Forward-pass" class="headerlink" title="3.3 - Convolutional Neural Networks - Forward pass"></a>3.3 - Convolutional Neural Networks - Forward pass</h4><p>In the forward pass, you will take many filters and convolve them on the input. Each ‘convolution’ gives you a 2D matrix output. You will then stack these outputs to get a 3D volume:</p><p><video width="100%" height="400" src="conv_kiank.mp4" controls></video><br><strong>Exercise</strong>:<br>Implement the function below to convolve the filters <code>W</code> on an input activation <code>A_prev</code>.<br>This function takes the following inputs:</p><ul><li><code>A_prev</code>, the activations output by the previous layer (for a batch of m inputs);</li><li>Weights are denoted by <code>W</code>. The filter window size is <code>f</code> by <code>f</code>.</li><li>The bias vector is <code>b</code>, where each filter has its own (single) bias.</li></ul><p>Finally you also have access to the hyperparameters dictionary which contains the stride and the padding.</p><p><strong>Hint</strong>:</p><ol><li>To select a 2x2 slice at the upper left corner of a matrix “a_prev” (shape (5,5,3)), you would do:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a_slice_prev = a_prev[<span class="number">0</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">2</span>,:]</span><br></pre></td></tr></table></figure></li></ol><p>Notice how this gives a 3D slice that has height 2, width 2, and depth 3. Depth is the number of channels.<br>This will be useful when you will define <code>a_slice_prev</code> below, using the <code>start/end</code> indexes you will define.</p><ol><li>To define a_slice you will need to first define its corners <code>vert_start</code>, <code>vert_end</code>, <code>horiz_start</code> and <code>horiz_end</code>. This figure may be helpful for you to find out how each of the corner can be defined using h, w, f and s in the code below.</li></ol><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575875378455.png" alt="1575875378455"></p><p><strong>Reminder</strong>:<br>The formulas relating the output shape of the convolution to the input shape is:</p><script type="math/tex;mode=display">n_H = \lfloor \frac{n_{H_{prev}} - f + 2 \times pad}{stride} \rfloor +1</script><script type="math/tex;mode=display">n_W = \lfloor \frac{n_{W_{prev}} - f + 2 \times pad}{stride} \rfloor +1</script><script type="math/tex;mode=display">n_C = \text{number of filters used in the convolution}</script><p>For this exercise, we won’t worry about vectorization, and will just implement everything with for-loops.</p><h5 id="Additional-Hints-if-you’re-stuck"><a href="#Additional-Hints-if-you’re-stuck" class="headerlink" title="Additional Hints if you’re stuck"></a>Additional Hints if you’re stuck</h5><ul><li>You will want to use array slicing (e.g.<code>varname[0:1,:,3:5]</code>) for the following variables:<br><code>a_prev_pad</code> ,<code>W</code>, <code>b</code><br>Copy the starter code of the function and run it outside of the defined function, in separate cells.<br>Check that the subset of each array is the size and dimension that you’re expecting.</li><li>To decide how to get the vert_start, vert_end; horiz_start, horiz_end, remember that these are indices of the previous layer.<br>Draw an example of a previous padded layer (8 x 8, for instance), and the current (output layer) (2 x 2, for instance).<br>The output layer’s indices are denoted by <code>h</code> and <code>w</code>.</li><li>Make sure that <code>a_slice_prev</code> has a height, width and depth.</li><li>Remember that <code>a_prev_pad</code> is a subset of <code>A_prev_pad</code>.<br>Think about which one should be used within the for loops.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_prev, W, b, hparameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- output activations of the previous layer, </span></span><br><span class="line"><span class="string">        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    b -- Biases, numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "stride" and "pad"</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward() function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape (≈1 line)  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape (≈1 line)</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters" (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    pad = hparameters[<span class="string">"pad"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the dimensions of the CONV output volume using the formula given above. </span></span><br><span class="line">    <span class="comment"># Hint: use int() to apply the 'floor' operation. (≈2 lines)</span></span><br><span class="line">    n_H = int((n_H_prev - f + <span class="number">2</span> * pad) / stride + <span class="number">1</span>)</span><br><span class="line">    n_W = int((n_W_prev - f + <span class="number">2</span> * pad) / stride + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the output volume Z with zeros. (≈1 line)</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create A_prev_pad by padding A_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):               <span class="comment"># loop over the batch of training examples</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i, :, :, :]  <span class="comment"># Select ith training example's padded activation</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):           <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="comment"># Find the vertical start and end of the current "slice" (≈2 lines)</span></span><br><span class="line">            vert_start = stride * h</span><br><span class="line">            vert_end = vert_start + f</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):       <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="comment"># Find the horizontal start and end of the current "slice" (≈2 lines)</span></span><br><span class="line">                horiz_start = stride * w</span><br><span class="line">                horiz_end = horiz_start + f</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):   <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                                        </span><br><span class="line">                    <span class="comment"># Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)</span></span><br><span class="line">                    weights = W[:, :, :, c]</span><br><span class="line">                    biases = b[:, :, :, c]</span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)</span><br><span class="line">                                        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save information in "cache" for the backprop</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">10</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">4</span>)</span><br><span class="line">W = np.random.randn(<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">8</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">8</span>)</span><br><span class="line">hparameters = &#123;<span class="string">"pad"</span> : <span class="number">1</span>,</span><br><span class="line">               <span class="string">"stride"</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">Z, cache_conv = conv_forward(A_prev, W, b, hparameters)</span><br><span class="line">print(<span class="string">"Z's mean =\n"</span>, np.mean(Z))</span><br><span class="line">print(<span class="string">"Z[3,2,1] =\n"</span>, Z[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"cache_conv[0][1][2][3] =\n"</span>, cache_conv[<span class="number">0</span>][<span class="number">1</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br><span class="line"><span class="comment"># (3,3,4)*W[:,:,:,0].shape</span></span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575876789888.png" alt="1575876789888"></p><p>Finally, CONV layer should also contain an activation, in which case we would add the following line of code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolve the window to get back one output neuron</span></span><br><span class="line">Z[i, h, w, c] = ...</span><br><span class="line"><span class="comment"># Apply activation</span></span><br><span class="line">A[i, h, w, c] = activation(Z[i, h, w, c])</span><br></pre></td></tr></table></figure><p>You don’t need to do it here.</p><h3 id="4-Pooling-layer"><a href="#4-Pooling-layer" class="headerlink" title="4 - Pooling layer"></a>4 - Pooling layer</h3><p>The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are:</p><ul><li>Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.</li><li>Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.</li></ul><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575877402479.png" alt="1575877402479"></p><p>These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size $f$. This specifies the height and width of the $f \times f$ window you would compute a <em>max</em> or <em>average</em> over.</p><h4 id="4-1-Forward-Pooling"><a href="#4-1-Forward-Pooling" class="headerlink" title="4.1 - Forward Pooling"></a>4.1 - Forward Pooling</h4><p>Now, you are going to implement MAX-POOL and AVG-POOL, in the same function.</p><p><strong>Exercise</strong>: Implement the forward pass of the pooling layer. Follow the hints in the comments below.</p><p><strong>Reminder</strong>:<br>As there’s no padding, the formulas binding the output shape of the pooling to the input shape is:</p><script type="math/tex;mode=display">n_H = \lfloor \frac{n_{H_{prev}} - f}{stride} \rfloor +1</script><script type="math/tex;mode=display">n_W = \lfloor \frac{n_{W_{prev}} - f}{stride} \rfloor +1</script><script type="math/tex;mode=display">n_C = n_{C_{prev}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pool_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_prev, hparameters, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "f" and "stride"</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from the input shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from "hparameters"</span></span><br><span class="line">    f = hparameters[<span class="string">"f"</span>]</span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the dimensions of the output</span></span><br><span class="line">    n_H = int(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = int(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize output matrix A</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                         <span class="comment"># loop over the training examples</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                     <span class="comment"># loop on the vertical axis of the output volume</span></span><br><span class="line">            <span class="comment"># Find the vertical start and end of the current "slice" (≈2 lines)</span></span><br><span class="line">            vert_start = h * stride</span><br><span class="line">            vert_end = vert_start + f</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                 <span class="comment"># loop on the horizontal axis of the output volume</span></span><br><span class="line">                <span class="comment"># Find the vertical start and end of the current "slice" (≈2 lines)</span></span><br><span class="line">                horiz_start = w * stride</span><br><span class="line">                horiz_end = horiz_start + f</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range (n_C):            <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)</span></span><br><span class="line">                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the pooling operation on the slice. </span></span><br><span class="line">                    <span class="comment"># Use an if statement to differentiate the modes. </span></span><br><span class="line">                    <span class="comment"># Use np.max and np.mean.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.max(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.mean(a_prev_slice)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Store the input and hparameters in "cache" for pool_backward()</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Case 1: stride of 1</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">hparameters = &#123;<span class="string">"stride"</span> : <span class="number">1</span>, <span class="string">"f"</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">A, cache = pool_forward(A_prev, hparameters)</span><br><span class="line">print(<span class="string">"mode = max"</span>)</span><br><span class="line">print(<span class="string">"A.shape = "</span> + str(A.shape))</span><br><span class="line">print(<span class="string">"A =\n"</span>, A)</span><br><span class="line">print()</span><br><span class="line">A, cache = pool_forward(A_prev, hparameters, mode = <span class="string">"average"</span>)</span><br><span class="line">print(<span class="string">"mode = average"</span>)</span><br><span class="line">print(<span class="string">"A.shape = "</span> + str(A.shape))</span><br><span class="line">print(<span class="string">"A =\n"</span>, A)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Case 2: stride of 2</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">hparameters = &#123;<span class="string">"stride"</span> : <span class="number">2</span>, <span class="string">"f"</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">A, cache = pool_forward(A_prev, hparameters)</span><br><span class="line">print(<span class="string">"mode = max"</span>)</span><br><span class="line">print(<span class="string">"A.shape = "</span> + str(A.shape))</span><br><span class="line">print(<span class="string">"A =\n"</span>, A)</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">A, cache = pool_forward(A_prev, hparameters, mode = <span class="string">"average"</span>)</span><br><span class="line">print(<span class="string">"mode = average"</span>)</span><br><span class="line">print(<span class="string">"A.shape = "</span> + str(A.shape))</span><br><span class="line">print(<span class="string">"A =\n"</span>, A)</span><br></pre></td></tr></table></figure><p>Congratulations! You have now implemented the forward passes of all the layers of a convolutional network.</p><p>The remainder of this notebook is optional, and will not be graded.</p><h3 id="5-Backpropagation-in-convolutional-neural-networks-OPTIONAL-UNGRADED"><a href="#5-Backpropagation-in-convolutional-neural-networks-OPTIONAL-UNGRADED" class="headerlink" title="5 - Backpropagation in convolutional neural networks (OPTIONAL / UNGRADED)"></a>5 - Backpropagation in convolutional neural networks (OPTIONAL / UNGRADED)</h3><p>In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers don’t need to bother with the details of the backward pass. The backward pass for convolutional networks is complicated. If you wish, you can work through this optional portion of the notebook to get a sense of what backprop in a convolutional network looks like.</p><p>When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in convolutional neural networks you can calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are not trivial and we did not derive them in lecture, but we will briefly present them below.</p><h4 id="5-1-Convolutional-layer-backward-pass"><a href="#5-1-Convolutional-layer-backward-pass" class="headerlink" title="5.1 - Convolutional layer backward pass"></a>5.1 - Convolutional layer backward pass</h4><p>Let’s start by implementing the backward pass for a CONV layer.</p><h5 id="5-1-1-Computing-dA"><a href="#5-1-1-Computing-dA" class="headerlink" title="5.1.1 - Computing dA:"></a>5.1.1 - Computing dA:</h5><p>This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:</p><script type="math/tex;mode=display">dA += \sum _{h=0} ^{n_H} \sum_{w=0} ^{n_W} W_c \times dZ_{hw} \tag{1}</script><p>Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down). Note that at each time, we multiply the the same filter $W_c$ by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices.</p><p>In code, inside the appropriate for-loops, this formula translates into:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><p></p><h5 id="5-1-2-Computing-dW"><a href="#5-1-2-Computing-dW" class="headerlink" title="5.1.2 - Computing dW:"></a>5.1.2 - Computing dW:</h5><p>This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:</p><script type="math/tex;mode=display">dW_c  += \sum _{h=0} ^{n_H} \sum_{w=0} ^ {n_W} a_{slice} \times dZ_{hw}  \tag{2}</script><p>Where $a_{slice}$ corresponds to the slice which was used to generate the activation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$.</p><p>In code, inside the appropriate for-loops, this formula translates into:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><p></p><h5 id="5-1-3-Computing-db"><a href="#5-1-3-Computing-db" class="headerlink" title="5.1.3 - Computing db:"></a>5.1.3 - Computing db:</h5><p>This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:</p><script type="math/tex;mode=display">db = \sum_h \sum_w dZ_{hw} \tag{3}</script><p>As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost.</p><p>In code, inside the appropriate for-loops, this formula translates into:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><p></p><p><strong>Exercise</strong>: Implement the <code>conv_backward</code> function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward(), output of conv_forward()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),</span></span><br><span class="line"><span class="string">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    dW -- gradient of the cost with respect to the weights of the conv layer (W)</span></span><br><span class="line"><span class="string">          numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    db -- gradient of the cost with respect to the biases of the conv layer (b)</span></span><br><span class="line"><span class="string">          numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters"</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from dZ's shape</span></span><br><span class="line">    (m, n_H, n_W, n_C) = dZ.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev, dW, db with the correct shapes</span></span><br><span class="line">    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           </span><br><span class="line">    dW = np.zeros((f, f, n_C_prev, n_C))</span><br><span class="line">    db = np.zeros((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, n_C))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pad A_prev and dA_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select ith training example from A_prev_pad and dA_prev_pad</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i, :, :, :]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i, :, :, :]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice"</span></span><br><span class="line">                    vert_start = h * stride </span><br><span class="line">                    vert_end = vert_start + f </span><br><span class="line">                    horiz_start = w * stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the slice from a_prev_pad</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update gradients for the window and the filter's parameters using the code formulas given above</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br><span class="line">                    db[:,:,:,c] += dZ[i, h, w, c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span></span><br><span class="line">        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We'll run conv_forward to initialize the 'Z' and 'cache_conv",</span></span><br><span class="line"><span class="comment"># which we'll use to test the conv_backward function</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">10</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">W = np.random.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">8</span>)</span><br><span class="line">hparameters = &#123;<span class="string">"pad"</span> : <span class="number">2</span>,</span><br><span class="line">               <span class="string">"stride"</span>: <span class="number">2</span>&#125;</span><br><span class="line">Z, cache_conv = conv_forward(A_prev, W, b, hparameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test conv_backward</span></span><br><span class="line">dA, dW, db = conv_backward(Z, cache_conv)</span><br><span class="line">print(<span class="string">"dA_mean ="</span>, np.mean(dA))</span><br><span class="line">print(<span class="string">"dW_mean ="</span>, np.mean(dW))</span><br><span class="line">print(<span class="string">"db_mean ="</span>, np.mean(db))</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575878492910.png" alt="1575878492910"></p><h4 id="5-2-Pooling-layer-backward-pass"><a href="#5-2-Pooling-layer-backward-pass" class="headerlink" title="5.2 Pooling layer - backward pass"></a>5.2 Pooling layer - backward pass</h4><p>Next, let’s implement the backward pass for the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer.</p><h5 id="5-2-1-Max-pooling-backward-pass"><a href="#5-2-1-Max-pooling-backward-pass" class="headerlink" title="5.2.1 Max pooling - backward pass"></a>5.2.1 Max pooling - backward pass</h5><p>Before jumping into the backpropagation of the pooling layer, you are going to build a helper function called <code>create_mask_from_window()</code> which does the following:</p><script type="math/tex;mode=display">X = \begin{bmatrix}
1 && 3 \\
4 && 2
\end{bmatrix} \quad \rightarrow  \quad M =\begin{bmatrix}
0 && 0 \\
1 && 0
\end{bmatrix}\tag{4}</script><p>As you can see, this function creates a “mask” matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). You’ll see later that the backward pass for average pooling will be similar to this but using a different mask.</p><p><strong>Exercise</strong>: Implement <code>create_mask_from_window()</code>. This function will be helpful for pooling backward.<br>Hints:</p><ul><li><a href>np.max()</a> may be helpful. It computes the maximum of an array.</li><li><p>If you have a matrix X and a scalar x: <code>A = (X == x)</code> will return a matrix A of the same size as X such that:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A[i,j] = True if X[i,j] = x</span><br><span class="line">A[i,j] = False if X[i,j] != x</span><br></pre></td></tr></table></figure></li><li><p>Here, you don’t need to consider cases where there are several maxima in a matrix.</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a mask from an input matrix x, to identify the max entry of x.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Array of shape (f, f)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    mask = (x == np.max(x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">mask = create_mask_from_window(x)</span><br><span class="line">print(<span class="string">'x = '</span>, x)</span><br><span class="line">print(<span class="string">"mask = "</span>, mask)</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575879142012.png" alt="1575879142012"></p><p>Why do we keep track of the position of the max? It’s because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will “propagate” the gradient back to this particular input value that had influenced the cost.</p><h5 id="5-2-2-Average-pooling-backward-pass"><a href="#5-2-2-Average-pooling-backward-pass" class="headerlink" title="5.2.2 - Average pooling - backward pass"></a>5.2.2 - Average pooling - backward pass</h5><p>In max pooling, for each input window, all the “influence” on the output came from a single input value—the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.</p><p>For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you’ll use for the backward pass will look like:</p><script type="math/tex;mode=display">dZ = 1 \quad \rightarrow  \quad dZ =\begin{bmatrix}
1/4 && 1/4 \\
1/4 && 1/4
\end{bmatrix}\tag{5}</script><p>This implies that each position in the $dZ$ matrix contributes equally to output because in the forward pass, we took an average.</p><p><strong>Exercise</strong>: Implement the function below to equally distribute a value dz through a matrix of dimension shape. <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html" target="_blank" rel="noopener">Hint</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz, shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Distributes the input value in the matrix of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dz -- input scalar</span></span><br><span class="line"><span class="string">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Array of size (n_H, n_W) for which we distributed the value of dz</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shape (≈1 line)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the value to distribute on the matrix (≈1 line)</span></span><br><span class="line">    average = dz / (n_H * n_W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a matrix where every entry is the "average" value (≈1 line)</span></span><br><span class="line">    a = average * np.ones(shape)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = distribute_value(<span class="number">2</span>, (<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">print(<span class="string">'distributed value ='</span>, a)</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575879266978.png" alt="1575879266978"></p><h5 id="5-2-3-Putting-it-together-Pooling-backward"><a href="#5-2-3-Putting-it-together-Pooling-backward" class="headerlink" title="5.2.3 Putting it together: Pooling backward"></a>5.2.3 Putting it together: Pooling backward</h5><p>You now have everything you need to compute backward propagation on a pooling layer.</p><p><strong>Exercise</strong>: Implement the <code>pool_backward</code> function in both modes (<code>&quot;max&quot;</code> and <code>&quot;average&quot;</code>). You will once again use 4 for-loops (iterating over training examples, height, width, and channels). You should use an <code>if/elif</code> statement to see if the mode is equal to <code>&#39;max&#39;</code> or <code>&#39;average&#39;</code>. If it is equal to ‘average’ you should use the <code>distribute_value()</code> function you implemented above to create a matrix of the same shape as <code>a_slice</code>. Otherwise, the mode is equal to ‘<code>max</code>‘, and you will create a mask with <code>create_mask_from_window()</code> and multiply it by the corresponding value of dA.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA, cache, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A</span></span><br><span class="line"><span class="string">    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters </span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from cache (≈1 line)</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from "hparameters" (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    f = hparameters[<span class="string">'f'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape</span><br><span class="line">    m, n_H, n_W, n_C = dA.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev with zeros (≈1 line)</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select training example from A_prev (≈1 line)</span></span><br><span class="line">        a_prev = A_prev[i, :, :, :]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop on the vertical axis</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop on the horizontal axis</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels (depth)</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride </span><br><span class="line">                    vert_end = vert_start + f </span><br><span class="line">                    horiz_start = w * stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line"></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the backward propagation in both modes.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Use the corners and "c" to define the current slice from a_prev (≈1 line)</span></span><br><span class="line">                        a_prev_slice =  a_prev[vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                        <span class="comment"># Create the mask from a_prev_slice (≈1 line)</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        <span class="comment"># Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Get the value a from dA (≈1 line)</span></span><br><span class="line">                        da = dA[i, h, w, c]</span><br><span class="line">                        <span class="comment"># Define the shape of the filter as fxf (≈1 line)</span></span><br><span class="line">                        shape = (f, f)</span><br><span class="line">                        <span class="comment"># Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line">    <span class="comment">### END CODE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">hparameters = &#123;<span class="string">"stride"</span> : <span class="number">1</span>, <span class="string">"f"</span>: <span class="number">2</span>&#125;</span><br><span class="line">A, cache = pool_forward(A_prev, hparameters)</span><br><span class="line">dA = np.random.randn(<span class="number">5</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">dA_prev = pool_backward(dA, cache, mode = <span class="string">"max"</span>)</span><br><span class="line">print(<span class="string">"mode = max"</span>)</span><br><span class="line">print(<span class="string">'mean of dA = '</span>, np.mean(dA))</span><br><span class="line">print(<span class="string">'dA_prev[1,1] = '</span>, dA_prev[<span class="number">1</span>,<span class="number">1</span>])  </span><br><span class="line">print()</span><br><span class="line">dA_prev = pool_backward(dA, cache, mode = <span class="string">"average"</span>)</span><br><span class="line">print(<span class="string">"mode = average"</span>)</span><br><span class="line">print(<span class="string">'mean of dA = '</span>, np.mean(dA))</span><br><span class="line">print(<span class="string">'dA_prev[1,1] = '</span>, dA_prev[<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575879505718.png" alt="1575879505718"></p><h3 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations !"></a>Congratulations !</h3><p>Congratulations on completing this assignment. You now understand how convolutional neural networks work. You have implemented all the building blocks of a neural network. In the next assignment you will implement a ConvNet using TensorFlow.</p><h2 id="Part-2：Convolutional-Neural-Networks-Application"><a href="#Part-2：Convolutional-Neural-Networks-Application" class="headerlink" title="Part 2：Convolutional Neural Networks: Application"></a>Part 2：Convolutional Neural Networks: Application</h2><p>Welcome to Course 4’s second assignment! In this notebook, you will:</p><ul><li>Implement helper functions that you will use when implementing a TensorFlow model</li><li>Implement a fully functioning ConvNet using TensorFlow</li></ul><p><strong>After this assignment you will be able to:</strong></p><ul><li>Build and train a ConvNet in TensorFlow for a classification problem</li></ul><p>We assume here that you are already familiar with TensorFlow. If you are not, please refer the <em>TensorFlow Tutorial</em> of the third week of Course 2 (“<em>Improving deep neural networks</em>“).</p><h3 id="Updates-to-Assignment"><a href="#Updates-to-Assignment" class="headerlink" title="Updates to Assignment"></a>Updates to Assignment</h3><h4 id="If-you-were-working-on-a-previous-version"><a href="#If-you-were-working-on-a-previous-version" class="headerlink" title="If you were working on a previous version"></a>If you were working on a previous version</h4><ul><li>The current notebook filename is version “1a”.</li><li>You can find your work in the file directory as version “1”.</li><li>To view the file directory, go to the menu “File-&gt;Open”, and this will open a new tab that shows the file directory.</li></ul><h4 id="List-of-Updates"><a href="#List-of-Updates" class="headerlink" title="List of Updates"></a>List of Updates</h4><ul><li><code>initialize_parameters</code>: added details about tf.get_variable, <code>eval</code>. Clarified test case.</li><li>Added explanations for the kernel (filter) stride values, max pooling, and flatten functions.</li><li>Added details about softmax cross entropy with logits.</li><li>Added instructions for creating the Adam Optimizer.</li><li>Added explanation of how to evaluate tensors (optimizer and cost).</li><li><code>forward_propagation</code>: clarified instructions, use “F” to store “flatten” layer.</li><li>Updated print statements and ‘expected output’ for easier visual comparisons.</li><li>Many thanks to Kevin P. Brown (mentor for the deep learning specialization) for his suggestions on the assignments in this course!</li></ul><h3 id="1-0-TensorFlow-model"><a href="#1-0-TensorFlow-model" class="headerlink" title="1.0 - TensorFlow model"></a>1.0 - TensorFlow model</h3><p>In the previous assignment, you built helper functions using numpy to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call.</p><p>As usual, we will start by loading in the packages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> cnn_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>Run the next cell to load the “SIGNS” dataset you are going to use.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (signs)</span></span><br><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.</p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575981281395.png" alt="1575981281395"></p><p>The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of <code>index</code> below and re-run to see different examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">6</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575981326134.png" alt="1575981326134"></p><p>In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.</p><p>To get started, let’s examine the shapes of your data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br><span class="line">conv_layers = &#123;&#125;</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575981404092.png" alt="1575981404092"></p><h3 id="1-1-Create-placeholders"><a href="#1-1-Create-placeholders" class="headerlink" title="1.1 - Create placeholders"></a>1.1 - Create placeholders</h3><p>TensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session.</p><p><strong>Exercise</strong>: Implement the function below to create placeholders for the input image X and the output Y. You should not define the number of training examples for the moment. To do so, you could use “None” as the batch size, it will give you the flexibility to choose it later. Hence X should be of dimension <strong>[None, n_H0, n_W0, n_C0]</strong> and Y should be of dimension <strong>[None, n_y]</strong>. <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder" target="_blank" rel="noopener">Hint: search for the tf.placeholder documentation”</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: create_placeholders</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_H0, n_W0, n_C0, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_H0 -- scalar, height of an input image</span></span><br><span class="line"><span class="string">    n_W0 -- scalar, width of an input image</span></span><br><span class="line"><span class="string">    n_C0 -- scalar, number of channels of the input</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    X = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, n_H0, n_W0, n_C0])</span><br><span class="line">    Y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, n_y])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X, Y = create_placeholders(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X = "</span> + str(X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y = "</span> + str(Y))</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575981906114.png" alt="1575981906114"></p><h3 id="1-2-Initialize-parameters"><a href="#1-2-Initialize-parameters" class="headerlink" title="1.2 - Initialize parameters"></a>1.2 - Initialize parameters</h3><p>You will initialize weights/filters $W1$ and $W2$ using <code>tf.contrib.layers.xavier_initializer(seed = 0)</code>. You don’t need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment.</p><p><strong>Exercise:</strong> Implement initialize_parameters(). The dimensions for each group of filters are provided below. Reminder - to initialize a parameter $W$ of shape [1,2,3,4] in Tensorflow, use:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.get_variable(<span class="string">"W"</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], initializer = ...)</span><br></pre></td></tr></table></figure><p></p><h4 id="tf-get-variable"><a href="#tf-get-variable" class="headerlink" title="tf.get_variable()"></a>tf.get_variable()</h4><p><a href="https://www.tensorflow.org/api_docs/python/tf/get_variable" target="_blank" rel="noopener">Search for the tf.get_variable documentation</a>. Notice that the documentation says:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Gets an existing variable with these parameters or create a new one.</span><br></pre></td></tr></table></figure><p>So we can use this function to create a tensorflow variable with the specified name, but if the variables already exist, it will get the existing variable with that same name.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [4, 4, 3, 8]</span></span><br><span class="line"><span class="string">                        W2 : [2, 2, 8, 16]</span></span><br><span class="line"><span class="string">    Note that we will hard code the shape values in the function to make the grading simpler.</span></span><br><span class="line"><span class="string">    Normally, functions should take values as inputs rather than hard coding.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, W2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                              <span class="comment"># so that your "random" numbers match ours</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines of code)</span></span><br><span class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">16</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess_test:</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess_test.run(init)</span><br><span class="line">    print(<span class="string">"W1[1,1,1] = \n"</span> + str(parameters[<span class="string">"W1"</span>].eval()[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">    print(<span class="string">"W1.shape: "</span> + str(parameters[<span class="string">"W1"</span>].shape))</span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line">    print(<span class="string">"W2[1,1,1] = \n"</span> + str(parameters[<span class="string">"W2"</span>].eval()[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">    print(<span class="string">"W2.shape: "</span> + str(parameters[<span class="string">"W2"</span>].shape))</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575982216129.png" alt="1575982216129"></p><h3 id="1-3-Forward-propagation"><a href="#1-3-Forward-propagation" class="headerlink" title="1.3 - Forward propagation"></a>1.3 - Forward propagation</h3><p>In TensorFlow, there are built-in functions that implement the convolution steps for you.</p><ul><li><p><strong>tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input $X$ and a group of filters $W$, this function convolves $W$’s filters on X. The third parameter ([1,s,s,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). Normally, you’ll choose a stride of 1 for the number of examples (the first value) and for the channels (the fourth value), which is why we wrote the value as <code>[1,s,s,1]</code>. You can read the full documentation on <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">conv2d</a>.</p></li><li><p><strong>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. For max pooling, we usually operate on a single example at a time and a single channel at a time. So the first and fourth value in <code>[1,f,f,1]</code> are both 1. You can read the full documentation on <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener">max_pool</a>.</p></li><li><p><strong>tf.nn.relu(Z):</strong> computes the elementwise ReLU of Z (which can be any shape). You can read the full documentation on <a href="https://www.tensorflow.org/api_docs/python/tf/nn/relu" target="_blank" rel="noopener">relu</a>.</p></li><li><p><strong>tf.contrib.layers.flatten(P)</strong>: given a tensor “P”, this function takes each training (or test) example in the batch and flattens it into a 1D vector.</p><ul><li>If a tensor P has the shape (m,h,w,c), where m is the number of examples (the batch size), it returns a flattened tensor with shape (batch_size, k), where $k=h \times w \times c$. “k” equals the product of all the dimension sizes other than the first dimension.</li><li>For example, given a tensor with dimensions [100,2,3,4], it flattens the tensor to be of shape [100, 24], where 24 = 2 <em>3 </em>4. You can read the full documentation on <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten" target="_blank" rel="noopener">flatten</a>.</li></ul></li><li><p><strong>tf.contrib.layers.fully_connected(F, num_outputs):</strong> given the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation on <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected" target="_blank" rel="noopener">full_connected</a>.</p></li></ul><p>In the last function above (<code>tf.contrib.layers.fully_connected</code>), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters.</p><h4 id="Window-kernel-filter"><a href="#Window-kernel-filter" class="headerlink" title="Window, kernel, filter"></a>Window, kernel, filter</h4><p>The words “window”, “kernel”, and “filter” are used to refer to the same thing. This is why the parameter <code>ksize</code> refers to “kernel size”, and we use <code>(f,f)</code> to refer to the filter size. Both “kernel” and “filter” refer to the “window.”</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note that for simplicity and grading purposes, we'll hard-code some values</span></span><br><span class="line"><span class="string">    such as the stride and kernel (filter) sizes. </span></span><br><span class="line"><span class="string">    Normally, functions should take these values as function parameters.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "W2"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># CONV2D: stride of 1, padding 'SAME'</span></span><br><span class="line">    Z1 = tf.nn.conv2d(X, W1, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 8x8, stride 8, padding 'SAME'</span></span><br><span class="line">    P1 = tf.nn.max_pool(A1, ksize=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># CONV2D: filters W2, stride 1, padding 'SAME'</span></span><br><span class="line">    Z2 = tf.nn.conv2d(P1, W2, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 4x4, stride 4, padding 'SAME'</span></span><br><span class="line">    P2 = tf.nn.max_pool(A2, ksize=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># FLATTEN</span></span><br><span class="line">    F = tf.contrib.layers.flatten(P2)</span><br><span class="line">    <span class="comment"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span></span><br><span class="line">    <span class="comment"># 6 neurons in output layer. Hint: one of the arguments should be "activation_fn=None" </span></span><br><span class="line">    Z3 = tf.contrib.layers.fully_connected(F, <span class="number">6</span>, activation_fn=<span class="literal">None</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    X, Y = create_placeholders(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    a = sess.run(Z3, &#123;X: np.random.randn(<span class="number">2</span>,<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>), Y: np.random.randn(<span class="number">2</span>,<span class="number">6</span>)&#125;)</span><br><span class="line">    print(<span class="string">"Z3 = \n"</span> + str(a))</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575982514022.png" alt="1575982514022"></p><h3 id="1-4-Compute-cost"><a href="#1-4-Compute-cost" class="headerlink" title="1.4 - Compute cost"></a>1.4 - Compute cost</h3><p>Implement the compute cost function below. Remember that the cost function helps the neural network see how much the model’s predictions differ from the correct labels. By adjusting the weights of the network to reduce the cost, the neural network can improve its predictions.</p><p>You might find these two functions helpful:</p><ul><li><strong>tf.nn.softmax_cross_entropy_with_logits(logits = Z, labels = Y):</strong> computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits" target="_blank" rel="noopener">softmax_cross_entropy_with_logits</a>.</li><li><strong>tf.reduce_mean:</strong> computes the mean of elements across dimensions of a tensor. Use this to calculate the sum of the losses over all the examples to get the overall cost. You can check the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean" target="_blank" rel="noopener">reduce_mean</a>.</li></ul><h4 id="Details-on-softmax-cross-entropy-with-logits-optional-reading"><a href="#Details-on-softmax-cross-entropy-with-logits-optional-reading" class="headerlink" title="Details on softmax_cross_entropy_with_logits (optional reading)"></a>Details on softmax_cross_entropy_with_logits (optional reading)</h4><ul><li>Softmax is used to format outputs so that they can be used for classification. It assigns a value between 0 and 1 for each category, where the sum of all prediction values (across all possible categories) equals 1.</li><li>Cross Entropy is compares the model’s predicted classifications with the actual labels and results in a numerical value representing the “loss” of the model’s predictions.</li><li>“Logits” are the result of multiplying the weights and adding the biases. Logits are passed through an activation function (such as a relu), and the result is called the “activation.”</li><li>The function is named <code>softmax_cross_entropy_with_logits</code> takes logits as input (and not activations); then uses the model to predict using softmax, and then compares the predictions with the true labels using cross entropy. These are done with a single function to optimize the calculations.</li></ul><p><strong>Exercise</strong>: Compute the cost below using the function above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (number of examples, 6)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    X, Y = create_placeholders(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    a = sess.run(cost, &#123;X: np.random.randn(<span class="number">4</span>,<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>), Y: np.random.randn(<span class="number">4</span>,<span class="number">6</span>)&#125;)</span><br><span class="line">    print(<span class="string">"cost = "</span> + str(a))</span><br><span class="line"><span class="comment"># cost = 2.91034</span></span><br></pre></td></tr></table></figure><h3 id="1-5-Model"><a href="#1-5-Model" class="headerlink" title="1.5 Model"></a>1.5 Model</h3><p>Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset.</p><p><strong>Exercise</strong>: Complete the function below.</p><p>The model below should:</p><ul><li>create placeholders</li><li>initialize parameters</li><li>forward propagate</li><li>compute the cost</li><li>create an optimizer</li></ul><p>Finally you will create a session and run a for loop for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. <a href="https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer" target="_blank" rel="noopener">Hint for initializing the variables</a></p><h4 id="Adam-Optimizer"><a href="#Adam-Optimizer" class="headerlink" title="Adam Optimizer"></a>Adam Optimizer</h4><p>You can use <code>tf.train.AdamOptimizer(learning_rate = ...)</code> to create the optimizer. The optimizer has a <code>minimize(loss=...)</code> function that you’ll call to set the cost function that the optimizer will minimize.</p><p>For details, check out the documentation for <a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" target="_blank" rel="noopener">Adam Optimizer</a></p><h4 id="Random-mini-batches"><a href="#Random-mini-batches" class="headerlink" title="Random mini batches"></a>Random mini batches</h4><p>If you took course 2 of the deep learning specialization, you implemented <code>random_mini_batches()</code> in the “Optimization” programming assignment. This function returns a list of mini-batches. It is already implemented in the <code>cnn_utils.py</code> file and imported here, so you can call it like this:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">minibatches = random_mini_batches(X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p></p><p>(You will want to choose the correct variable names when you use it in your code).</p><h4 id="Evaluating-the-optimizer-and-cost"><a href="#Evaluating-the-optimizer-and-cost" class="headerlink" title="Evaluating the optimizer and cost"></a>Evaluating the optimizer and cost</h4><p>Within a loop, for each mini-batch, you’ll use the <code>tf.Session</code> object (named <code>sess</code>) to feed a mini-batch of inputs and labels into the neural network and evaluate the tensors for the optimizer as well as the cost. Remember that we built a graph data structure and need to feed it inputs and labels and use <code>sess.run()</code> in order to get values for the optimizer and cost.</p><p>You’ll use this kind of syntax:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output_for_var1, output_for_var2 = sess.run(</span><br><span class="line">                                                fetches=[var1, var2],</span><br><span class="line">                                                feed_dict=&#123;var_inputs: the_batch_of_inputs,</span><br><span class="line">                                                           var_labels: the_batch_of_labels&#125;</span><br><span class="line">                                                )</span><br></pre></td></tr></table></figure><p></p><ul><li>Notice that <code>sess.run</code> takes its first argument <code>fetches</code> as a list of objects that you want it to evaluate (in this case, we want to evaluate the optimizer and the cost).</li><li>It also takes a dictionary for the <code>feed_dict</code> parameter.</li><li>The keys are the <code>tf.placeholder</code> variables that we created in the <code>create_placeholders</code> function above.</li><li>The values are the variables holding the actual numpy arrays for each mini-batch.</li><li>The sess.run outputs a tuple of the evaluated tensors, in the same order as the list given to <code>fetches</code>.</li></ul><p>For more information on how to use sess.run, see the documentation <a href="https://www.tensorflow.org/api_docs/python/tf/Session#run" target="_blank" rel="noopener">tf.Sesssion#run</a> documentation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.009</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs = <span class="number">100</span>, minibatch_size = <span class="number">64</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer ConvNet in Tensorflow:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    train_accuracy -- real number, accuracy on the train set (X_train)</span></span><br><span class="line"><span class="string">    test_accuracy -- real number, testing accuracy on the test set (X_test)</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    ops.reset_default_graph()                         <span class="comment"># to be able to rerun the model without overwriting tf variables</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                             <span class="comment"># to keep results consistent (tensorflow seed)</span></span><br><span class="line">    seed = <span class="number">3</span>                                          <span class="comment"># to keep results consistent (numpy seed)</span></span><br><span class="line">    (m, n_H0, n_W0, n_C0) = X_train.shape             </span><br><span class="line">    n_y = Y_train.shape[<span class="number">1</span>]                            </span><br><span class="line">    costs = []                                        <span class="comment"># To keep track of the cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Placeholders of the correct shape</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Forward propagation: Build the forward propagation in the tensorflow graph</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cost function: Add cost function to tensorflow graph</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize all the variables globally</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">     </span><br><span class="line">    <span class="comment"># Start the session to compute the tensorflow graph</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Run the initialization</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Do the training loop</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">            minibatch_cost = <span class="number">0.</span></span><br><span class="line">            num_minibatches = int(m / minibatch_size) <span class="comment"># number of minibatches of size minibatch_size in the train set</span></span><br><span class="line">            seed = seed + <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Select a minibatch</span></span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                <span class="string">"""</span></span><br><span class="line"><span class="string">                # IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line"><span class="string">                # Run the session to execute the optimizer and the cost.</span></span><br><span class="line"><span class="string">                # The feedict should contain a minibatch for (X,Y).</span></span><br><span class="line"><span class="string">                """</span></span><br><span class="line">                <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">                _ , temp_cost = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br><span class="line">                <span class="comment">### END CODE HERE ###</span></span><br><span class="line">                </span><br><span class="line">                minibatch_cost += temp_cost / num_minibatches</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print the cost every epoch</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, minibatch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(minibatch_cost)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        predict_op = tf.argmax(Z3, <span class="number">1</span>)</span><br><span class="line">        correct_prediction = tf.equal(predict_op, tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">        print(accuracy)</span><br><span class="line">        train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;)</span><br><span class="line">        test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;)</span><br><span class="line">        print(<span class="string">"Train Accuracy:"</span>, train_accuracy)</span><br><span class="line">        print(<span class="string">"Test Accuracy:"</span>, test_accuracy)</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> train_accuracy, test_accuracy, parameters</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model for 100 epochs. Check if your cost after epoch 0 and 5 matches our output. If not, stop the cell and go back to your code!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_, _, parameters = model(X_train, Y_train, X_test, Y_test)</span><br></pre></td></tr></table></figure><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575983453744.png" alt="1575983453744"></p><p><img src="/2019/11/22/deeplearning-ai笔记（4-1）/1575983473339.png" alt="1575983473339"></p><p>Congratulations! You have finished the assignment and built a model that recognizes SIGN language with almost 80% accuracy on the test set. If you wish, feel free to play around with this dataset further. You can actually improve its accuracy by spending more time tuning the hyperparameters, or using regularization (as this model clearly has a high variance).</p><p>Once again, here’s a thumbs up for your work!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fname = <span class="string">"images/thumbs_up.jpg"</span></span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="literal">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line">plt.imshow(my_image)</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p><p><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">https://www.coursera.org/specializations/deep-learning</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p><p><a href="https://blog.csdn.net/Koala_Tree/article/details/79913655" target="_blank" rel="noopener">https://blog.csdn.net/Koala_Tree/article/details/79913655</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2019/11/22/deeplearning-ai笔记（3-2）/" rel="next" title="deeplearning-ai笔记（3-2）"><i class="fa fa-chevron-left"></i> deeplearning-ai笔记（3-2）</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/01/04/deeplearning-ai笔记（4-2）/" rel="prev" title="deeplearning-ai笔记（4-2）">deeplearning-ai笔记（4-2） <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/kikyo.jpg" alt="Kikyō"><p class="site-author-name" itemprop="name">Kikyō</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">40</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积神经网络（Foundations-of-Convolutional-Neural-Networks）"><span class="nav-text">卷积神经网络（Foundations of Convolutional Neural Networks）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-计算机视觉（Computer-vision）"><span class="nav-text">1.1 计算机视觉（Computer vision）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-边缘检测示例（Edge-detection-example）"><span class="nav-text">1.2 边缘检测示例（Edge detection example）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#垂直边缘检测"><span class="nav-text">垂直边缘检测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-更多边缘检测内容（More-edge-detection）"><span class="nav-text">1.3 更多边缘检测内容（More edge detection）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#水平边缘检测"><span class="nav-text">水平边缘检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#更复杂的例子"><span class="nav-text">更复杂的例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他过滤器"><span class="nav-text">其他过滤器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Padding"><span class="nav-text">1.4 Padding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-卷积步长（Strided-convolutions）"><span class="nav-text">1.5 卷积步长（Strided convolutions）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#互相关和卷积"><span class="nav-text">互相关和卷积</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-三维卷积（Convolutions-over-volumes）"><span class="nav-text">1.6 三维卷积（Convolutions over volumes）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-单层卷积网络（One-layer-of-a-convolutional-network）"><span class="nav-text">1.7 单层卷积网络（One layer of a convolutional network）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-简单卷积网络示例（A-simple-convolution-network-example）"><span class="nav-text">1.8 简单卷积网络示例（A simple convolution network example）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-池化层（Pooling-layers）"><span class="nav-text">1.9 池化层（Pooling layers）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-卷积神经网络示例（Convolutional-neural-network-example）"><span class="nav-text">1.10 卷积神经网络示例（Convolutional neural network example）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-11-为什么使用卷积？（Why-convolutions-）"><span class="nav-text">1.11 为什么使用卷积？（Why convolutions?）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#训练卷积神经网络"><span class="nav-text">训练卷积神经网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-1：Convolutional-Model-step-by-step"><span class="nav-text">Part 1：Convolutional Model: step by step</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Updates"><span class="nav-text">Updates</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#If-you-were-working-on-the-notebook-before-this-update…"><span class="nav-text">If you were working on the notebook before this update…</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#List-of-updates"><span class="nav-text">List of updates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Packages"><span class="nav-text">1 - Packages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Outline-of-the-Assignment"><span class="nav-text">2 - Outline of the Assignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Convolutional-Neural-Networks"><span class="nav-text">3 - Convolutional Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Zero-Padding"><span class="nav-text">3.1 - Zero-Padding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Single-step-of-convolution"><span class="nav-text">3.2 - Single step of convolution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Convolutional-Neural-Networks-Forward-pass"><span class="nav-text">3.3 - Convolutional Neural Networks - Forward pass</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Additional-Hints-if-you’re-stuck"><span class="nav-text">Additional Hints if you’re stuck</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Pooling-layer"><span class="nav-text">4 - Pooling layer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Forward-Pooling"><span class="nav-text">4.1 - Forward Pooling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Backpropagation-in-convolutional-neural-networks-OPTIONAL-UNGRADED"><span class="nav-text">5 - Backpropagation in convolutional neural networks (OPTIONAL / UNGRADED)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-Convolutional-layer-backward-pass"><span class="nav-text">5.1 - Convolutional layer backward pass</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-1-Computing-dA"><span class="nav-text">5.1.1 - Computing dA:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-2-Computing-dW"><span class="nav-text">5.1.2 - Computing dW:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-1-3-Computing-db"><span class="nav-text">5.1.3 - Computing db:</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-Pooling-layer-backward-pass"><span class="nav-text">5.2 Pooling layer - backward pass</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-1-Max-pooling-backward-pass"><span class="nav-text">5.2.1 Max pooling - backward pass</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-2-Average-pooling-backward-pass"><span class="nav-text">5.2.2 - Average pooling - backward pass</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-3-Putting-it-together-Pooling-backward"><span class="nav-text">5.2.3 Putting it together: Pooling backward</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Congratulations"><span class="nav-text">Congratulations !</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-2：Convolutional-Neural-Networks-Application"><span class="nav-text">Part 2：Convolutional Neural Networks: Application</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Updates-to-Assignment"><span class="nav-text">Updates to Assignment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#If-you-were-working-on-a-previous-version"><span class="nav-text">If you were working on a previous version</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#List-of-Updates"><span class="nav-text">List of Updates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-0-TensorFlow-model"><span class="nav-text">1.0 - TensorFlow model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Create-placeholders"><span class="nav-text">1.1 - Create placeholders</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Initialize-parameters"><span class="nav-text">1.2 - Initialize parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-get-variable"><span class="nav-text">tf.get_variable()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Forward-propagation"><span class="nav-text">1.3 - Forward propagation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Window-kernel-filter"><span class="nav-text">Window, kernel, filter</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Compute-cost"><span class="nav-text">1.4 - Compute cost</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Details-on-softmax-cross-entropy-with-logits-optional-reading"><span class="nav-text">Details on softmax_cross_entropy_with_logits (optional reading)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Model"><span class="nav-text">1.5 Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam-Optimizer"><span class="nav-text">Adam Optimizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Random-mini-batches"><span class="nav-text">Random mini batches</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Evaluating-the-optimizer-and-cost"><span class="nav-text">Evaluating the optimizer and cost</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Kikyō</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">165.6k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script></body></html><!-- rebuild by neat -->