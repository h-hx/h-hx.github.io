<!-- build time:Fri Jun 04 2021 16:27:46 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|DejaVu Sans Mono for Powerline:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-flower.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-flower.png?v=5.1.4"><meta name="keywords" content="框架,"><meta name="description" content="6.2 CNN卷积层和池化层通过过滤器将一个2x2x3的节点矩阵变化成一个1x1x5的单位节点矩阵总共需要参数：2x2x3x5+5=64个使用$w_{(x,y,z)}^i$来表示对于输出单位节点矩阵中的第$i$个节点，则第$i$个节点的取值为：$g(i)=f(\sum \limits _{x=1}^2\sum \limits _{y=1}^2\sum \limits _{z=1}^3a_{(x,y"><meta name="keywords" content="框架"><meta property="og:type" content="article"><meta property="og:title" content="实战Google深度学习框架-第6~9章"><meta property="og:url" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/index.html"><meta property="og:site_name"><meta property="og:description" content="6.2 CNN卷积层和池化层通过过滤器将一个2x2x3的节点矩阵变化成一个1x1x5的单位节点矩阵总共需要参数：2x2x3x5+5=64个使用$w_{(x,y,z)}^i$来表示对于输出单位节点矩阵中的第$i$个节点，则第$i$个节点的取值为：$g(i)=f(\sum \limits _{x=1}^2\sum \limits _{y=1}^2\sum \limits _{z=1}^3a_{(x,y"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/1580037853070.png"><meta property="og:image" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/1580039396384.png"><meta property="og:image" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/1580125587470.png"><meta property="og:image" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/1580125852519.png"><meta property="og:image" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/1580125991328.png"><meta property="og:image" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/1581079106994.png"><meta property="og:updated_time" content="2020-02-21T01:26:45.286Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="实战Google深度学习框架-第6~9章"><meta name="twitter:description" content="6.2 CNN卷积层和池化层通过过滤器将一个2x2x3的节点矩阵变化成一个1x1x5的单位节点矩阵总共需要参数：2x2x3x5+5=64个使用$w_{(x,y,z)}^i$来表示对于输出单位节点矩阵中的第$i$个节点，则第$i$个节点的取值为：$g(i)=f(\sum \limits _{x=1}^2\sum \limits _{y=1}^2\sum \limits _{z=1}^3a_{(x,y"><meta name="twitter:image" content="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/1580037853070.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"right",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/"><title>实战Google深度学习框架-第6~9章 |</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title"></span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/24/实战Google深度学习框架-第6-9章/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Kikyō"><meta itemprop="description" content><meta itemprop="image" content="/images/kikyo.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content></span><header class="post-header"><h1 class="post-title" itemprop="name headline">实战Google深度学习框架-第6~9章</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-24T18:18:56+08:00">2020-01-24 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-02-21T09:26:45+08:00">2020-02-21 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/实战Google深度学习框架笔记/" itemprop="url" rel="index"><span itemprop="name">实战Google深度学习框架笔记</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">10.9k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">52</span></div></div></header><div class="post-body" itemprop="articleBody"><h3 id="6-2-CNN"><a href="#6-2-CNN" class="headerlink" title="6.2 CNN"></a>6.2 CNN</h3><h4 id="卷积层和池化层"><a href="#卷积层和池化层" class="headerlink" title="卷积层和池化层"></a>卷积层和池化层</h4><p>通过过滤器将一个2x2x3的节点矩阵变化成一个1x1x5的单位节点矩阵总共需要参数：2x2x3x5+5=64个</p><p>使用$w_{(x,y,z)}^i$来表示对于输出单位节点矩阵中的第$i$个节点，则第$i$个节点的取值为：</p><p>$g(i)=f(\sum \limits _{x=1}^2\sum \limits _{y=1}^2\sum \limits _{z=1}^3a_{(x,y,z)\times w_{x,y,z}^{i}+b^i})$</p><p>$a$为过滤器，$f$为激活函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入矩阵</span></span><br><span class="line">M = np.array([</span><br><span class="line">        [[<span class="number">1</span>],[<span class="number">-1</span>],[<span class="number">0</span>]],</span><br><span class="line">        [[<span class="number">-1</span>],[<span class="number">2</span>],[<span class="number">1</span>]],</span><br><span class="line">        [[<span class="number">0</span>],[<span class="number">2</span>],[<span class="number">-2</span>]]</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Matrix shape is: "</span>, M.shape) <span class="comment"># Matrix shape is:  (3, 3, 1)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义卷积过滤器</span></span><br><span class="line">filter_weight = tf.get_variable(<span class="string">'weights'</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>], initializer = tf.constant_initializer([</span><br><span class="line">                                                                        [<span class="number">1</span>, <span class="number">-1</span>],</span><br><span class="line">                                                                        [<span class="number">0</span>, <span class="number">2</span>]]))</span><br><span class="line">biases = tf.get_variable(<span class="string">'biases'</span>, [<span class="number">1</span>], initializer = tf.constant_initializer(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">M = np.asarray(M, dtype=<span class="string">'float32'</span>)</span><br><span class="line">M = M.reshape(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算矩阵通过卷积层过滤器和池化层过滤器计算后的结果</span></span><br><span class="line">x = tf.placeholder(<span class="string">'float32'</span>, [<span class="number">1</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">conv = tf.nn.conv2d(x, filter_weight, strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">pool = tf.nn.avg_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    convoluted_M = sess.run(bias,feed_dict=&#123;x:M&#125;)</span><br><span class="line">    pooled_M = sess.run(pool,feed_dict=&#123;x:M&#125;)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"convoluted_M: \n"</span>, convoluted_M)</span><br><span class="line">    print(<span class="string">"pooled_M: \n"</span>, pooled_M)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">convoluted_M: </span><br><span class="line"> [[[[ <span class="number">7.</span>]</span><br><span class="line">   [ <span class="number">1.</span>]]</span><br><span class="line"></span><br><span class="line">  [[<span class="number">-1.</span>]</span><br><span class="line">   [<span class="number">-1.</span>]]]]</span><br><span class="line">pooled_M: </span><br><span class="line"> [[[[ <span class="number">0.25</span>]</span><br><span class="line">   [ <span class="number">0.5</span> ]]</span><br><span class="line"></span><br><span class="line">  [[ <span class="number">1.</span>  ]</span><br><span class="line">   [<span class="number">-2.</span>  ]]]]</span><br></pre></td></tr></table></figure><h3 id="6-4-经典卷积网络模型"><a href="#6-4-经典卷积网络模型" class="headerlink" title="6.4 经典卷积网络模型"></a>6.4 经典卷积网络模型</h3><h4 id="6-4-2-LeNet-5"><a href="#6-4-2-LeNet-5" class="headerlink" title="6.4.2 LeNet-5"></a>6.4.2 LeNet-5</h4><p><img src="/2020/01/24/实战Google深度学习框架-第6-9章/1580037853070.png" alt="1580037853070"></p><p>第一层的过滤器尺寸为5x5，深度为6，不使用全0填充，步长为1，有5x5x1x6+6=156个参数，下一层矩阵有28x28x6=4704个节点，故有4704x(25+1)=122304个连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置神经网络的参数</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">IMAGE_SIZE = <span class="number">28</span></span><br><span class="line">NUM_CHANNELS = <span class="number">1</span></span><br><span class="line">NUM_LABELS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">CONV1_DEEP = <span class="number">32</span></span><br><span class="line">CONV1_SIZE = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">CONV2_DEEP = <span class="number">64</span></span><br><span class="line">CONV2_SIZE = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">FC_SIZE = <span class="number">512</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, train, regularizer)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer1-conv1'</span>):</span><br><span class="line">        conv1_weights = tf.get_variable(</span><br><span class="line">            <span class="string">"weight"</span>, [CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_DEEP],</span><br><span class="line">            initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv1_biases = tf.get_variable(<span class="string">"bias"</span>, [CONV1_DEEP], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"layer2-pool1"</span>):</span><br><span class="line">        pool1 = tf.nn.max_pool(relu1, ksize = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],padding=<span class="string">"SAME"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer3-conv2"</span>):</span><br><span class="line">        conv2_weights = tf.get_variable(</span><br><span class="line">            <span class="string">"weight"</span>, [CONV2_SIZE, CONV2_SIZE, CONV1_DEEP, CONV2_DEEP],</span><br><span class="line">            initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        conv2_biases = tf.get_variable(<span class="string">"bias"</span>, [CONV2_DEEP], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">"layer4-pool2"</span>):</span><br><span class="line">        pool2 = tf.nn.max_pool(relu2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">        pool_shape = pool2.get_shape().as_list()</span><br><span class="line">        nodes = pool_shape[<span class="number">1</span>] * pool_shape[<span class="number">2</span>] * pool_shape[<span class="number">3</span>]</span><br><span class="line">        reshaped = tf.reshape(pool2, [pool_shape[<span class="number">0</span>], nodes])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer5-fc1'</span>):</span><br><span class="line">        fc1_weights = tf.get_variable(<span class="string">"weight"</span>, [nodes, FC_SIZE],</span><br><span class="line">                                      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        <span class="keyword">if</span> regularizer != <span class="literal">None</span>: tf.add_to_collection(<span class="string">'losses'</span>, regularizer(fc1_weights))</span><br><span class="line">        fc1_biases = tf.get_variable(<span class="string">"bias"</span>, [FC_SIZE], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line"></span><br><span class="line">        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)</span><br><span class="line">        <span class="keyword">if</span> train: fc1 = tf.nn.dropout(fc1, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'layer6-fc2'</span>):</span><br><span class="line">        fc2_weights = tf.get_variable(<span class="string">"weight"</span>, [FC_SIZE, NUM_LABELS],</span><br><span class="line">                                      initializer=tf.truncated_normal_initializer(stddev=<span class="number">0.1</span>))</span><br><span class="line">        <span class="keyword">if</span> regularizer != <span class="literal">None</span>: tf.add_to_collection(<span class="string">'losses'</span>, regularizer(fc2_weights))</span><br><span class="line">        fc2_biases = tf.get_variable(<span class="string">"bias"</span>, [NUM_LABELS], initializer=tf.constant_initializer(<span class="number">0.1</span>))</span><br><span class="line">        logit = tf.matmul(fc1, fc2_weights) + fc2_biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> logit</span><br></pre></td></tr></table></figure><h5 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> LeNet5_infernece</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.01</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line">REGULARIZATION_RATE = <span class="number">0.0001</span></span><br><span class="line">TRAINING_STEPS = <span class="number">6000</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    <span class="comment"># 定义输出为4维矩阵的placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [</span><br><span class="line">            BATCH_SIZE,</span><br><span class="line">            LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">            LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">            LeNet5_infernece.NUM_CHANNELS],</span><br><span class="line">        name=<span class="string">'x-input'</span>)</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, LeNet5_infernece.OUTPUT_NODE], name=<span class="string">'y-input'</span>)</span><br><span class="line">    </span><br><span class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">    y = LeNet5_infernece.inference(x,<span class="literal">False</span>,regularizer)</span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义损失函数、学习率、滑动平均操作以及训练过程。</span></span><br><span class="line">    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    variables_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line">    loss = cross_entropy_mean + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        LEARNING_RATE_BASE,</span><br><span class="line">        global_step,</span><br><span class="line">        mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY,</span><br><span class="line">        staircase=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">'train'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 初始化TensorFlow持久化类。</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">            reshaped_xs = np.reshape(xs, (</span><br><span class="line">                BATCH_SIZE,</span><br><span class="line">                LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">                LeNet5_infernece.IMAGE_SIZE,</span><br><span class="line">                LeNet5_infernece.NUM_CHANNELS))</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict=&#123;x: reshaped_xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"After %d training step(s), loss on training batch is %g."</span> % (step, loss_value))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主程序入口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv=None)</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"../../../datasets/MNIST_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><img src="/2020/01/24/实战Google深度学习框架-第6-9章/1580039396384.png" alt="1580039396384"></p><p>一般图片分类问题的卷积神经网络架构：</p><p>输入层$\rightarrow $(卷积层+ $\rightarrow $ 池化层?)$\rightarrow $全连接层+</p><h4 id="6-4-2-Inception-v3"><a href="#6-4-2-Inception-v3" class="headerlink" title="6.4.2 Inception-v3"></a>6.4.2 Inception-v3</h4><p>使用slim库</p><h3 id="6-5-迁移学习"><a href="#6-5-迁移学习" class="headerlink" title="6.5 迁移学习"></a>6.5 迁移学习</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line"><span class="comment"># wget http://download.tensorflow.org/example_images/flower_photos.tgz</span></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.platform <span class="keyword">import</span> gfile</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原始输入数据的目录，这个目录下有5个子目录，每个子目录底下保存这属于该</span></span><br><span class="line"><span class="comment"># 类别的所有图片。</span></span><br><span class="line">INPUT_DATA = <span class="string">'../../datasets/flower_photos'</span></span><br><span class="line"><span class="comment"># 输出文件地址。我们将整理后的图片数据通过numpy的格式保存。</span></span><br><span class="line">OUTPUT_FILE = <span class="string">'../../datasets/flower_processed_data.npy'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据和验证数据比例。</span></span><br><span class="line">VALIDATION_PERCENTAGE = <span class="number">10</span></span><br><span class="line">TEST_PERCENTAGE = <span class="number">10</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据并将数据分割成训练数据、验证数据和测试数据。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_image_lists</span><span class="params">(sess, testing_percentage, validation_percentage)</span>:</span></span><br><span class="line">    sub_dirs = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> os.walk(INPUT_DATA)]</span><br><span class="line">    is_root_dir = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化各个数据集。</span></span><br><span class="line">    training_images = []</span><br><span class="line">    training_labels = []</span><br><span class="line">    testing_images = []</span><br><span class="line">    testing_labels = []</span><br><span class="line">    validation_images = []</span><br><span class="line">    validation_labels = []</span><br><span class="line">    current_label = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 读取所有的子目录。</span></span><br><span class="line">    <span class="keyword">for</span> sub_dir <span class="keyword">in</span> sub_dirs:</span><br><span class="line">        <span class="keyword">if</span> is_root_dir:</span><br><span class="line">            is_root_dir = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取一个子目录中所有的图片文件。</span></span><br><span class="line">        extensions = [<span class="string">'jpg'</span>, <span class="string">'jpeg'</span>, <span class="string">'JPG'</span>, <span class="string">'JPEG'</span>]</span><br><span class="line">        file_list = []</span><br><span class="line">        dir_name = os.path.basename(sub_dir)</span><br><span class="line">        <span class="keyword">for</span> extension <span class="keyword">in</span> extensions:</span><br><span class="line">            file_glob = os.path.join(INPUT_DATA, dir_name, <span class="string">'*.'</span> + extension)</span><br><span class="line">            file_list.extend(glob.glob(file_glob))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> file_list: <span class="keyword">continue</span></span><br><span class="line">        print(<span class="string">"processing:"</span>, dir_name)</span><br><span class="line">        </span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 处理图片数据。</span></span><br><span class="line">        <span class="keyword">for</span> file_name <span class="keyword">in</span> file_list:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 读取并解析图片，将图片转化为299*299以方便inception-v3模型来处理。</span></span><br><span class="line">            image_raw_data = gfile.FastGFile(file_name, <span class="string">'rb'</span>).read()</span><br><span class="line">            image = tf.image.decode_jpeg(image_raw_data)</span><br><span class="line">            <span class="keyword">if</span> image.dtype != tf.float32:</span><br><span class="line">                image = tf.image.convert_image_dtype(image, dtype=tf.float32)</span><br><span class="line">            image = tf.image.resize_images(image, [<span class="number">299</span>, <span class="number">299</span>])</span><br><span class="line">            image_value = sess.run(image)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 随机划分数据聚。</span></span><br><span class="line">            chance = np.random.randint(<span class="number">100</span>)</span><br><span class="line">            <span class="keyword">if</span> chance &lt; validation_percentage:</span><br><span class="line">                validation_images.append(image_value)</span><br><span class="line">                validation_labels.append(current_label)</span><br><span class="line">            <span class="keyword">elif</span> chance &lt; (testing_percentage + validation_percentage):</span><br><span class="line">                testing_images.append(image_value)</span><br><span class="line">                testing_labels.append(current_label)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                training_images.append(image_value)</span><br><span class="line">                training_labels.append(current_label)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                print(i, <span class="string">"images processed."</span>)</span><br><span class="line">        current_label += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将训练数据随机打乱以获得更好的训练效果。</span></span><br><span class="line">    state = np.random.get_state()</span><br><span class="line">    np.random.shuffle(training_images)</span><br><span class="line">    np.random.set_state(state)</span><br><span class="line">    np.random.shuffle(training_labels)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> np.asarray([training_images, training_labels,</span><br><span class="line">                       validation_images, validation_labels,</span><br><span class="line">                       testing_images, testing_labels])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行数据处理过程</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    processed_data = create_image_lists(sess, TEST_PERCENTAGE, VALIDATION_PERCENTAGE)</span><br><span class="line">    <span class="comment"># 通过numpy格式保存处理后的数据。</span></span><br><span class="line">    np.save(OUTPUT_FILE, processed_data)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import glob</span><br><span class="line">import os.path</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.platform import gfile</span><br><span class="line">import tensorflow.contrib.slim as slim</span><br><span class="line"></span><br><span class="line"># 加载通过TensorFlow-Slim定义好的inception_v3模型。</span><br><span class="line">import tensorflow.contrib.slim.python.slim.nets.inception_v3 as inception_v3</span><br><span class="line"></span><br><span class="line"># 处理好之后的数据文件。</span><br><span class="line">INPUT_DATA = &apos;../../datasets/flower_processed_data.npy&apos;</span><br><span class="line"># 保存训练好的模型的路径。</span><br><span class="line">TRAIN_FILE = &apos;train_dir/model&apos;</span><br><span class="line"># 谷歌提供的训练好的模型文件地址。因为GitHub无法保存大于100M的文件，所以</span><br><span class="line"># 在运行时需要先自行从Google下载inception_v3.ckpt文件。</span><br><span class="line">CKPT_FILE = &apos;../../datasets/inception_v3.ckpt&apos;</span><br><span class="line"></span><br><span class="line"># 定义训练中使用的参数。</span><br><span class="line">LEARNING_RATE = 0.0001</span><br><span class="line">STEPS = 300</span><br><span class="line">BATCH = 32</span><br><span class="line">N_CLASSES = 5</span><br><span class="line"></span><br><span class="line"># 不需要从谷歌训练好的模型中加载的参数。</span><br><span class="line">CHECKPOINT_EXCLUDE_SCOPES = &apos;InceptionV3/Logits,InceptionV3/AuxLogits&apos;</span><br><span class="line"># 需要训练的网络层参数明层，在fine-tuning的过程中就是最后的全联接层。</span><br><span class="line">TRAINABLE_SCOPES=&apos;InceptionV3/Logits,InceptionV3/AuxLogit&apos;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取所有需要从谷歌训练好的模型中加载的参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tuned_variables</span><span class="params">()</span>:</span></span><br><span class="line">    exclusions = [scope.strip() <span class="keyword">for</span> scope <span class="keyword">in</span> CHECKPOINT_EXCLUDE_SCOPES.split(<span class="string">','</span>)]</span><br><span class="line"></span><br><span class="line">    variables_to_restore = []</span><br><span class="line">    <span class="comment"># 枚举inception-v3模型中所有的参数，然后判断是否需要从加载列表中移除。</span></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> slim.get_model_variables():</span><br><span class="line">        excluded = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> exclusion <span class="keyword">in</span> exclusions:</span><br><span class="line">            <span class="keyword">if</span> var.op.name.startswith(exclusion):</span><br><span class="line">                excluded = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> excluded:</span><br><span class="line">            variables_to_restore.append(var)</span><br><span class="line">    <span class="keyword">return</span> variables_to_restore</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取所有需要训练的变量列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_trainable_variables</span><span class="params">()</span>:</span>    </span><br><span class="line">    scopes = [scope.strip() <span class="keyword">for</span> scope <span class="keyword">in</span> TRAINABLE_SCOPES.split(<span class="string">','</span>)]</span><br><span class="line">    variables_to_train = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 枚举所有需要训练的参数前缀，并通过这些前缀找到所有需要训练的参数。</span></span><br><span class="line">    <span class="keyword">for</span> scope <span class="keyword">in</span> scopes:</span><br><span class="line">        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)</span><br><span class="line">        variables_to_train.extend(variables)</span><br><span class="line">    <span class="keyword">return</span> variables_to_train</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">def main():</span><br><span class="line">    # 加载预处理好的数据。</span><br><span class="line">    processed_data = np.load(INPUT_DATA)</span><br><span class="line">    training_images = processed_data[0]</span><br><span class="line">    n_training_example = len(training_images)</span><br><span class="line">    training_labels = processed_data[1]</span><br><span class="line">    </span><br><span class="line">    validation_images = processed_data[2]</span><br><span class="line">    validation_labels = processed_data[3]</span><br><span class="line">    </span><br><span class="line">    testing_images = processed_data[4]</span><br><span class="line">    testing_labels = processed_data[5]</span><br><span class="line">    print(&quot;%d training examples, %d validation examples and %d testing examples.&quot; % (</span><br><span class="line">        n_training_example, len(validation_labels), len(testing_labels)))</span><br><span class="line"></span><br><span class="line">    # 定义inception-v3的输入，images为输入图片，labels为每一张图片对应的标签。</span><br><span class="line">    images = tf.placeholder(tf.float32, [None, 299, 299, 3], name=&apos;input_images&apos;)</span><br><span class="line">    labels = tf.placeholder(tf.int64, [None], name=&apos;labels&apos;)</span><br><span class="line">    </span><br><span class="line">    # 定义inception-v3模型。因为谷歌给出的只有模型参数取值，所以这里</span><br><span class="line">    # 需要在这个代码中定义inception-v3的模型结构。虽然理论上需要区分训练和</span><br><span class="line">    # 测试中使用到的模型，也就是说在测试时应该使用is_training=False，但是</span><br><span class="line">    # 因为预先训练好的inception-v3模型中使用的batch normalization参数与</span><br><span class="line">    # 新的数据会有出入，所以这里直接使用同一个模型来做测试。</span><br><span class="line">    with slim.arg_scope(inception_v3.inception_v3_arg_scope()):</span><br><span class="line">        logits, _ = inception_v3.inception_v3(</span><br><span class="line">            images, num_classes=N_CLASSES, is_training=True)</span><br><span class="line">    </span><br><span class="line">    trainable_variables = get_trainable_variables()</span><br><span class="line">    # 定义损失函数和训练过程。</span><br><span class="line">    tf.losses.softmax_cross_entropy(</span><br><span class="line">        tf.one_hot(labels, N_CLASSES), logits, weights=1.0)</span><br><span class="line">    total_loss = tf.losses.get_total_loss()</span><br><span class="line">    train_step = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(total_loss)</span><br><span class="line">    </span><br><span class="line">    # 计算正确率。</span><br><span class="line">    with tf.name_scope(&apos;evaluation&apos;):</span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(logits, 1), labels)</span><br><span class="line">        evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">                </span><br><span class="line">    # 定义加载Google训练好的Inception-v3模型的Saver。</span><br><span class="line">    load_fn = slim.assign_from_checkpoint_fn(</span><br><span class="line">      CKPT_FILE,</span><br><span class="line">      get_tuned_variables(),</span><br><span class="line">      ignore_missing_vars=True)</span><br><span class="line">    </span><br><span class="line">    # 定义保存新模型的Saver。</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    </span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        # 初始化没有加载进来的变量。</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        # 加载谷歌已经训练好的模型。</span><br><span class="line">        print(&apos;Loading tuned variables from %s&apos; % CKPT_FILE)</span><br><span class="line">        load_fn(sess)</span><br><span class="line">            </span><br><span class="line">        start = 0</span><br><span class="line">        end = BATCH</span><br><span class="line">        for i in range(STEPS):            </span><br><span class="line">            _, loss = sess.run([train_step, total_loss], feed_dict=&#123;</span><br><span class="line">                images: training_images[start:end], </span><br><span class="line">                labels: training_labels[start:end]&#125;)</span><br><span class="line"></span><br><span class="line">            if i % 30 == 0 or i + 1 == STEPS:</span><br><span class="line">                saver.save(sess, TRAIN_FILE, global_step=i)</span><br><span class="line">                </span><br><span class="line">                validation_accuracy = sess.run(evaluation_step, feed_dict=&#123;</span><br><span class="line">                    images: validation_images, labels: validation_labels&#125;)</span><br><span class="line">                print(&apos;Step %d: Training loss is %.1f Validation accuracy = %.1f%%&apos; % (</span><br><span class="line">                    i, loss, validation_accuracy * 100.0))</span><br><span class="line">                            </span><br><span class="line">            start = end</span><br><span class="line">            if start == n_training_example:</span><br><span class="line">                start = 0</span><br><span class="line">            </span><br><span class="line">            end = start + BATCH</span><br><span class="line">            if end &gt; n_training_example: </span><br><span class="line">                end = n_training_example</span><br><span class="line">            </span><br><span class="line">        # 在最后的测试数据上测试正确率。</span><br><span class="line">        test_accuracy = sess.run(evaluation_step, feed_dict=&#123;</span><br><span class="line">            images: testing_images, labels: testing_labels&#125;)</span><br><span class="line">        print(&apos;Final test accuracy = %.1f%%&apos; % (test_accuracy * 100))</span><br></pre></td></tr></table></figure><h3 id="7-1-TFRecord"><a href="#7-1-TFRecord" class="headerlink" title="7.1 TFRecord"></a>7.1 TFRecord</h3><p>TFRecord文件中的数据都是通过tf.train.Example Protocol Buffer的格式存储的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义函数转化变量类型。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_int64_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bytes_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为tf.train.Example格式。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_make_example</span><span class="params">(pixels, label, image)</span>:</span></span><br><span class="line">    image_raw = image.tostring()</span><br><span class="line">    example = tf.train.Example(features=tf.train.Features(feature=&#123;</span><br><span class="line">        <span class="string">'pixels'</span>: _int64_feature(pixels),</span><br><span class="line">        <span class="string">'label'</span>: _int64_feature(np.argmax(label)),</span><br><span class="line">        <span class="string">'image_raw'</span>: _bytes_feature(image_raw)</span><br><span class="line">    &#125;))</span><br><span class="line">    <span class="keyword">return</span> example</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mnist训练数据。</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"../../datasets/MNIST_data"</span>,dtype=tf.uint8, one_hot=<span class="literal">True</span>)</span><br><span class="line">images = mnist.train.images</span><br><span class="line">labels = mnist.train.labels</span><br><span class="line">pixels = images.shape[<span class="number">1</span>]</span><br><span class="line">num_examples = mnist.train.num_examples</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出包含训练数据的TFRecord文件。</span></span><br><span class="line"><span class="keyword">with</span> tf.python_io.TFRecordWriter(<span class="string">"output.tfrecords"</span>) <span class="keyword">as</span> writer:</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(num_examples):</span><br><span class="line">        example = _make_example(pixels, labels[index], images[index])</span><br><span class="line">        writer.write(example.SerializeToString())</span><br><span class="line">print(<span class="string">"TFRecord训练文件已保存。"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取mnist测试数据。</span></span><br><span class="line">images_test = mnist.test.images</span><br><span class="line">labels_test = mnist.test.labels</span><br><span class="line">pixels_test = images_test.shape[<span class="number">1</span>] <span class="comment"># 784</span></span><br><span class="line">num_examples_test = mnist.test.num_examples</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出包含测试数据的TFRecord文件。</span></span><br><span class="line"><span class="keyword">with</span> tf.python_io.TFRecordWriter(<span class="string">"output_test.tfrecords"</span>) <span class="keyword">as</span> writer:</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(num_examples_test):</span><br><span class="line">        example = _make_example(</span><br><span class="line">            pixels_test, labels_test[index], images_test[index])</span><br><span class="line">        writer.write(example.SerializeToString())</span><br><span class="line">print(<span class="string">"TFRecord测试文件已保存。"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取文件。</span></span><br><span class="line">reader = tf.TFRecordReader()</span><br><span class="line">filename_queue = tf.train.string_input_producer([<span class="string">"output.tfrecords"</span>])</span><br><span class="line">_,serialized_example = reader.read(filename_queue)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析读取的样例。</span></span><br><span class="line">features = tf.parse_single_example(</span><br><span class="line">    serialized_example,</span><br><span class="line">    features=&#123;</span><br><span class="line">        <span class="string">'pixels'</span>:tf.FixedLenFeature([],tf.int64),</span><br><span class="line">        <span class="string">'label'</span>:tf.FixedLenFeature([],tf.int64),</span><br><span class="line">        <span class="string">'image_raw'</span>:tf.FixedLenFeature([],tf.string)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">images = tf.decode_raw(features[<span class="string">'image_raw'</span>],tf.uint8)</span><br><span class="line">labels = tf.cast(features[<span class="string">'label'</span>],tf.int32)</span><br><span class="line">pixels = tf.cast(features[<span class="string">'pixels'</span>],tf.int32)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动多线程处理输入数据。</span></span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">threads = tf.train.start_queue_runners(sess=sess,coord=coord)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    image, label, pixel = sess.run([images, labels, pixels])</span><br></pre></td></tr></table></figure><h3 id="7-2-图像数据处理"><a href="#7-2-图像数据处理" class="headerlink" title="7.2 图像数据处理"></a>7.2 图像数据处理</h3><h4 id="7-2-1-TensorFlow图像处理函数"><a href="#7-2-1-TensorFlow图像处理函数" class="headerlink" title="7.2.1 TensorFlow图像处理函数"></a>7.2.1 TensorFlow图像处理函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf   </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">image_raw_data = tf.gfile.FastGFile(<span class="string">"../../datasets/cat.jpg"</span>,<span class="string">'rb'</span>).read()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    img_data = tf.image.decode_jpeg(image_raw_data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出解码之后的三维矩阵。</span></span><br><span class="line">    print(img_data.eval())</span><br><span class="line">    img_data.set_shape([<span class="number">1797</span>, <span class="number">2673</span>, <span class="number">3</span>])</span><br><span class="line">    print(img_data.get_shape())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印图片</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    plt.imshow(img_data.eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/01/24/实战Google深度学习框架-第6-9章/1580125587470.png" alt="1580125587470"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 大小调整</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 如果直接以0-255范围的整数数据输入resize_images，那么输出将是0-255之间的实数，</span></span><br><span class="line">    <span class="comment"># 不利于后续处理。本书建议在调整图片大小前，先将图片转为0-1范围的实数。</span></span><br><span class="line">    image_float = tf.image.convert_image_dtype(img_data, tf.float32)</span><br><span class="line">    resized = tf.image.resize_images(image_float, [<span class="number">300</span>, <span class="number">300</span>], method=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    plt.imshow(resized.eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/01/24/实战Google深度学习框架-第6-9章/1580125852519.png" alt="1580125852519"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 裁剪和填充图片</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:    </span><br><span class="line">    croped = tf.image.resize_image_with_crop_or_pad(img_data, <span class="number">1000</span>, <span class="number">1000</span>)</span><br><span class="line">    padded = tf.image.resize_image_with_crop_or_pad(img_data, <span class="number">3000</span>, <span class="number">3000</span>)</span><br><span class="line">    plt.imshow(croped.eval())</span><br><span class="line">    plt.show()</span><br><span class="line">    plt.imshow(padded.eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/01/24/实战Google深度学习框架-第6-9章/1580125991328.png" alt="1580125991328"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 截取中间50%的图片</span></span><br><span class="line">central_cropped = tf.image.central_crop(img_data, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 翻转图片</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess: </span><br><span class="line">    <span class="comment"># 上下翻转</span></span><br><span class="line">    <span class="comment">#flipped1 = tf.image.flip_up_down(img_data)</span></span><br><span class="line">    <span class="comment"># 左右翻转</span></span><br><span class="line">    <span class="comment">#flipped2 = tf.image.flip_left_right(img_data)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#对角线翻转</span></span><br><span class="line">    transposed = tf.image.transpose_image(img_data)</span><br><span class="line">    plt.imshow(transposed.eval())</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 以一定概率上下翻转图片。</span></span><br><span class="line">    <span class="comment">#flipped = tf.image.random_flip_up_down(img_data)</span></span><br><span class="line">    <span class="comment"># 以一定概率左右翻转图片。</span></span><br><span class="line">    <span class="comment">#flipped = tf.image.random_flip_left_right(img_data)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片色彩调整</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 在进行一系列图片调整前，先将图片转换为实数形式，有利于保持计算精度。</span></span><br><span class="line">    image_float = tf.image.convert_image_dtype(img_data, tf.float32)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片的亮度-0.5。</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_brightness(image_float, -0.5)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片的亮度+0.5</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_brightness(image_float, 0.5)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在[-max_delta, max_delta)的范围随机调整图片的亮度。</span></span><br><span class="line">    adjusted = tf.image.random_brightness(image_float, max_delta=<span class="number">0.5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片的对比度-5</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_contrast(image_float, -5)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片的对比度+5</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_contrast(image_float, 5)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在[lower, upper]的范围随机调整图的对比度。</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.random_contrast(image_float, lower, upper)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在最终输出前，将实数取值截取到0-1范围内。</span></span><br><span class="line">    adjusted = tf.clip_by_value(adjusted, <span class="number">0.0</span>, <span class="number">1.0</span>)</span><br><span class="line">    plt.imshow(adjusted.eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加色相和饱和度</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 在进行一系列图片调整前，先将图片转换为实数形式，有利于保持计算精度。</span></span><br><span class="line">    image_float = tf.image.convert_image_dtype(img_data, tf.float32)</span><br><span class="line">    </span><br><span class="line">    adjusted = tf.image.adjust_hue(image_float, <span class="number">0.1</span>)</span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_hue(image_float, 0.3)</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_hue(image_float, 0.6)</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_hue(image_float, 0.9)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在[-max_delta, max_delta]的范围随机调整图片的色相。max_delta的取值在[0, 0.5]之间。</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.random_hue(image_float, max_delta)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片的饱和度-5。</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_saturation(image_float, -5)</span></span><br><span class="line">    <span class="comment"># 将图片的饱和度+5。</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.adjust_saturation(image_float, 5)</span></span><br><span class="line">    <span class="comment"># 在[lower, upper]的范围随机调整图的饱和度。</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.random_saturation(image_float, lower, upper)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将代表一张图片的三维矩阵中的数字均值变为0，方差变为1。</span></span><br><span class="line">    <span class="comment">#adjusted = tf.image.per_image_whitening(image_float)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在最终输出前，将实数取值截取到0-1范围内。</span></span><br><span class="line">    adjusted = tf.clip_by_value(adjusted, <span class="number">0.0</span>, <span class="number">1.0</span>)</span><br><span class="line">    plt.imshow(adjusted.eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加标注框并裁减</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:         </span><br><span class="line">    boxes = tf.constant([[[<span class="number">0.05</span>, <span class="number">0.05</span>, <span class="number">0.9</span>, <span class="number">0.7</span>], [<span class="number">0.35</span>, <span class="number">0.47</span>, <span class="number">0.5</span>, <span class="number">0.56</span>]]])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># sample_distorted_bounding_box要求输入图片必须是实数类型。</span></span><br><span class="line">    image_float = tf.image.convert_image_dtype(img_data, tf.float32)</span><br><span class="line">    </span><br><span class="line">    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(</span><br><span class="line">        tf.shape(image_float), bounding_boxes=boxes, min_object_covered=<span class="number">0.4</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 截取后的图片</span></span><br><span class="line">    distorted_image = tf.slice(image_float, begin, size)</span><br><span class="line">    plt.imshow(distorted_image.eval())</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在原图上用标注框画出截取的范围。由于原图的分辨率较大（2673x1797)，生成的标注框 </span></span><br><span class="line">    <span class="comment"># 在Jupyter Notebook上通常因边框过细而无法分辨，这里为了演示方便先缩小分辨率。</span></span><br><span class="line">    image_small = tf.image.resize_images(image_float, [<span class="number">180</span>, <span class="number">267</span>], method=<span class="number">0</span>)</span><br><span class="line">    batchced_img = tf.expand_dims(image_small, <span class="number">0</span>)</span><br><span class="line">    image_with_box = tf.image.draw_bounding_boxes(batchced_img, bbox_for_draw)</span><br><span class="line">    print(bbox_for_draw.eval())</span><br><span class="line">    plt.imshow(image_with_box[<span class="number">0</span>].eval())</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><h3 id="7-3-多线程输入数据处理框架"><a href="#7-3-多线程输入数据处理框架" class="headerlink" title="7.3 多线程输入数据处理框架"></a>7.3 多线程输入数据处理框架</h3><h4 id="7-3-1-队列与多线程"><a href="#7-3-1-队列与多线程" class="headerlink" title="7.3.1 队列与多线程"></a>7.3.1 队列与多线程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">q = tf.FIFOQueue(<span class="number">2</span>, <span class="string">"int32"</span>) <span class="comment"># 2为队列大小</span></span><br><span class="line">init = q.enqueue_many(([<span class="number">0</span>, <span class="number">10</span>],)) <span class="comment"># 初始化队列</span></span><br><span class="line">x = q.dequeue()</span><br><span class="line">y = x + <span class="number">1</span></span><br><span class="line">q_inc = q.enqueue([y]) <span class="comment"># +1后的数据放入队列</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        v, _ = sess.run([x, q_inc])</span><br><span class="line">        print(v)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每隔1秒判断是否需要停止并打印自己的ID</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MyLoop</span><span class="params">(coord, worker_id)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</span><br><span class="line">        <span class="keyword">if</span> np.random.rand()&lt;<span class="number">0.1</span>:</span><br><span class="line">            print(<span class="string">"Stoping from id: %d\n"</span> % worker_id)</span><br><span class="line">            <span class="comment"># 停止所有进程</span></span><br><span class="line">            coord.request_stop()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"Working on id: %d\n"</span> % worker_id)</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建、启动并退出线程</span></span><br><span class="line">coord = tf.train.Coordinator()</span><br><span class="line">threads = [threading.Thread(target=MyLoop, args=(coord, i, )) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>)]</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> threads:t.start()</span><br><span class="line">coord.join(threads)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多线程</span></span><br><span class="line">queue = tf.FIFOQueue(<span class="number">100</span>,<span class="string">"float"</span>)</span><br><span class="line"><span class="comment"># 定义队列入队操作</span></span><br><span class="line">enqueue_op = queue.enqueue([tf.random_normal([<span class="number">1</span>])])</span><br><span class="line">qr = tf.train.QueueRunner(queue, [enqueue_op] * <span class="number">5</span>)</span><br><span class="line">tf.train.add_queue_runner(qr)</span><br><span class="line"><span class="comment"># 定义队列出队操作</span></span><br><span class="line">out_tensor = queue.dequeue()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>): print(sess.run(out_tensor)[<span class="number">0</span>])</span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure><h4 id="7-3-2-输入文件队列"><a href="#7-3-2-输入文件队列" class="headerlink" title="7.3.2 输入文件队列"></a>7.3.2 输入文件队列</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成文件存储样例数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建TFRecord文件的辅助函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_int64_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))</span><br><span class="line"></span><br><span class="line">num_shards = <span class="number">2</span> <span class="comment"># 写入文件数</span></span><br><span class="line">instances_per_shard = <span class="number">2</span> <span class="comment"># 每个文件数据数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_shards):</span><br><span class="line">    filename = (<span class="string">'data.tfrecords-%.5d-of-%.5d'</span> % (i, num_shards)) </span><br><span class="line">    <span class="comment"># 将Example结构写入TFRecord文件。</span></span><br><span class="line">    writer = tf.python_io.TFRecordWriter(filename)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(instances_per_shard):</span><br><span class="line">    <span class="comment"># Example结构仅包含当前样例属于第几个文件以及是当前文件的第几个样本。</span></span><br><span class="line">        example = tf.train.Example(features=tf.train.Features(feature=&#123;</span><br><span class="line">            <span class="string">'i'</span>: _int64_feature(i),</span><br><span class="line">            <span class="string">'j'</span>: _int64_feature(j)&#125;))</span><br><span class="line">        writer.write(example.SerializeToString())</span><br><span class="line">    writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取文件</span></span><br><span class="line"></span><br><span class="line">files = tf.train.match_filenames_once(<span class="string">"data.tfrecords-*"</span>)</span><br><span class="line">filename_queue = tf.train.string_input_producer(files, shuffle=<span class="literal">False</span>) </span><br><span class="line"></span><br><span class="line">reader = tf.TFRecordReader()</span><br><span class="line">_, serialized_example = reader.read(filename_queue)</span><br><span class="line">features = tf.parse_single_example(</span><br><span class="line">      serialized_example,</span><br><span class="line">      features=&#123;</span><br><span class="line">          <span class="string">'i'</span>: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">          <span class="string">'j'</span>: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])</span><br><span class="line">    print(sess.run(files))</span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">        print(sess.run([features[<span class="string">'i'</span>], features[<span class="string">'j'</span>]]))</span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure><p>在不打乱文件列表的情况下，会依次读取输出样例数据的每一个样例，当所有样例读取完毕，程序会重头开始，如果限制$num_epochs$为1，程序将报错。</p><h4 id="7-3-3-组合训练数据"><a href="#7-3-3-组合训练数据" class="headerlink" title="7.3.3 组合训练数据"></a>7.3.3 组合训练数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">example, label = features[<span class="string">'i'</span>], features[<span class="string">'j'</span>]</span><br><span class="line"><span class="comment"># 一个batch中样例的个数</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">capacity = <span class="number">1000</span> + <span class="number">3</span> * batch_size</span><br><span class="line"></span><br><span class="line">example_batch, label_batch = tf.train.batch([example, label], batch_size=batch_size, capacity=capacity)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    tf.local_variables_initializer().run()</span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        cur_example_batch, cur_label_batch = sess.run([example_batch, label_batch])</span><br><span class="line">        print(cur_example_batch, cur_label_batch)</span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.train.shuffle_batch</span></span><br></pre></td></tr></table></figure><h4 id="7-3-4-输入数据处理框架"><a href="#7-3-4-输入数据处理框架" class="headerlink" title="7.3.4 输入数据处理框架"></a>7.3.4 输入数据处理框架</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件列表，通过文件列表创建输入文件队列</span></span><br><span class="line">files = tf.train.match_filenames_once(<span class="string">"output.tfrecords"</span>)</span><br><span class="line">filename_queue = tf.train.string_input_producer(files, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解析TFRecord文件里的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文件。</span></span><br><span class="line">reader = tf.TFRecordReader()</span><br><span class="line">_,serialized_example = reader.read(filename_queue)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析读取的样例。</span></span><br><span class="line">features = tf.parse_single_example(</span><br><span class="line">    serialized_example,</span><br><span class="line">    features=&#123;</span><br><span class="line">        <span class="string">'image_raw'</span>:tf.FixedLenFeature([],tf.string),</span><br><span class="line">        <span class="string">'pixels'</span>:tf.FixedLenFeature([],tf.int64),</span><br><span class="line">        <span class="string">'label'</span>:tf.FixedLenFeature([],tf.int64)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">decoded_images = tf.decode_raw(features[<span class="string">'image_raw'</span>],tf.uint8)</span><br><span class="line">retyped_images = tf.cast(decoded_images, tf.float32)</span><br><span class="line">labels = tf.cast(features[<span class="string">'label'</span>],tf.int32)</span><br><span class="line"><span class="comment">#pixels = tf.cast(features['pixels'],tf.int32)</span></span><br><span class="line">images = tf.reshape(retyped_images, [<span class="number">784</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将文件以100个为一组打包</span></span><br><span class="line">min_after_dequeue = <span class="number">10000</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">capacity = min_after_dequeue + <span class="number">3</span> * batch_size</span><br><span class="line"></span><br><span class="line">image_batch, label_batch = tf.train.shuffle_batch([images, labels], </span><br><span class="line">                                                    batch_size=batch_size, </span><br><span class="line">                                                    capacity=capacity, </span><br><span class="line">                                                    min_after_dequeue=min_after_dequeue)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(input_tensor, weights1, biases1, weights2, biases2)</span>:</span></span><br><span class="line">    layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)</span><br><span class="line">    <span class="keyword">return</span> tf.matmul(layer1, weights2) + biases2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型相关的参数</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line">REGULARAZTION_RATE = <span class="number">0.0001</span>   </span><br><span class="line">TRAINING_STEPS = <span class="number">5000</span>        </span><br><span class="line"></span><br><span class="line">weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">biases1 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[LAYER1_NODE]))</span><br><span class="line"></span><br><span class="line">weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">biases2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[OUTPUT_NODE]))</span><br><span class="line"></span><br><span class="line">y = inference(image_batch, weights1, biases1, weights2, biases2)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 计算交叉熵及其平均值</span></span><br><span class="line">cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=label_batch)</span><br><span class="line">cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 损失函数的计算</span></span><br><span class="line">regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)</span><br><span class="line">regularaztion = regularizer(weights1) + regularizer(weights2)</span><br><span class="line">loss = cross_entropy_mean + regularaztion</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化损失函数</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 初始化会话，并开始训练过程。</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># tf.global_variables_initializer().run()</span></span><br><span class="line">    sess.run((tf.global_variables_initializer(),</span><br><span class="line">              tf.local_variables_initializer()))</span><br><span class="line">    coord = tf.train.Coordinator()</span><br><span class="line">    threads = tf.train.start_queue_runners(sess=sess, coord=coord)</span><br><span class="line">    <span class="comment"># 循环的训练神经网络。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %d training step(s), loss is %g "</span> % (i, sess.run(loss)))</span><br><span class="line">                  </span><br><span class="line">        sess.run(train_step) </span><br><span class="line">    coord.request_stop()</span><br><span class="line">    coord.join(threads)</span><br></pre></td></tr></table></figure><h3 id="7-4数据集"><a href="#7-4数据集" class="headerlink" title="7.4数据集"></a>7.4数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">input_data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>]</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices(input_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义迭代器用于遍历数据集</span></span><br><span class="line">iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get_next() 返回代表一个输入数据的张量。</span></span><br><span class="line">x = iterator.get_next()</span><br><span class="line">y = x * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input_data)):</span><br><span class="line">        print(sess.run(y))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取文本文件里的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文本文件作为本例的输入。</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./test1.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(<span class="string">"File1, line1.\n"</span>) </span><br><span class="line">    file.write(<span class="string">"File1, line2.\n"</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"./test2.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(<span class="string">"File2, line1.\n"</span>) </span><br><span class="line">    file.write(<span class="string">"File2, line2.\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从文本文件创建数据集。这里可以提供多个文件。</span></span><br><span class="line">input_files = [<span class="string">"./test1.txt"</span>, <span class="string">"./test2.txt"</span>]</span><br><span class="line">dataset = tf.data.TextLineDataset(input_files)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义迭代器。</span></span><br><span class="line">iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里get_next()返回一个字符串类型的张量，代表文件中的一行。</span></span><br><span class="line">x = iterator.get_next()  </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        print(sess.run(x))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解析TFRecord文件里的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析一个TFRecord的方法。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parser</span><span class="params">(record)</span>:</span></span><br><span class="line">    features = tf.parse_single_example(</span><br><span class="line">        record,</span><br><span class="line">        features=&#123;</span><br><span class="line">            <span class="string">'image_raw'</span>:tf.FixedLenFeature([],tf.string),</span><br><span class="line">            <span class="string">'pixels'</span>:tf.FixedLenFeature([],tf.int64),</span><br><span class="line">            <span class="string">'label'</span>:tf.FixedLenFeature([],tf.int64)</span><br><span class="line">        &#125;)</span><br><span class="line">    decoded_images = tf.decode_raw(features[<span class="string">'image_raw'</span>],tf.uint8)</span><br><span class="line">    retyped_images = tf.cast(decoded_images, tf.float32)</span><br><span class="line">    images = tf.reshape(retyped_images, [<span class="number">784</span>])</span><br><span class="line">    labels = tf.cast(features[<span class="string">'label'</span>],tf.int32)</span><br><span class="line">    <span class="comment">#pixels = tf.cast(features['pixels'],tf.int32)</span></span><br><span class="line">    <span class="keyword">return</span> images, labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从TFRecord文件创建数据集。这里可以提供多个文件。</span></span><br><span class="line">input_files = [<span class="string">"output.tfrecords"</span>]</span><br><span class="line">dataset = tf.data.TFRecordDataset(input_files)</span><br><span class="line"></span><br><span class="line"><span class="comment"># map()函数表示对数据集中的每一条数据进行调用解析方法。</span></span><br><span class="line">dataset = dataset.map(parser)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义遍历数据集的迭代器。</span></span><br><span class="line">iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据，可用于进一步计算</span></span><br><span class="line">image, label = iterator.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        x, y = sess.run([image, label]) </span><br><span class="line">        print(y)</span><br></pre></td></tr></table></figure><p>因为使用了$make_one_shot_iterator$，如果需要用$placeholder$来初始化数据集，则需要$initializable_iterator$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从TFRecord文件创建数据集，具体文件路径是一个placeholder，稍后再提供具体路径。</span></span><br><span class="line">input_files = tf.placeholder(tf.string)</span><br><span class="line">dataset = tf.data.TFRecordDataset(input_files)</span><br><span class="line">dataset = dataset.map(parser)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义遍历dataset的initializable_iterator。</span></span><br><span class="line">iterator = dataset.make_initializable_iterator()</span><br><span class="line">image, label = iterator.get_next()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 首先初始化iterator，并给出input_files的值。</span></span><br><span class="line">    sess.run(iterator.initializer,</span><br><span class="line">             feed_dict=&#123;input_files: [<span class="string">"output.tfrecords"</span>]&#125;)</span><br><span class="line">    <span class="comment"># 遍历所有数据一个epoch。当遍历结束时，程序会抛出OutOfRangeError。</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            x, y = sess.run([image, label])</span><br><span class="line">        <span class="keyword">except</span> tf.errors.OutOfRangeError:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h3 id="8-1-RNN"><a href="#8-1-RNN" class="headerlink" title="8.1 RNN"></a>8.1 RNN</h3><p><img src="/2020/01/24/实战Google深度学习框架-第6-9章/1581079106994.png" alt="1581079106994"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = [<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">state = [<span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line">w_cell_state = np.asarray([[<span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.3</span>, <span class="number">0.4</span>]])</span><br><span class="line">w_cell_input = np.asarray([<span class="number">0.5</span>, <span class="number">0.6</span>])</span><br><span class="line">b_cell = np.asarray([<span class="number">0.1</span>, <span class="number">-0.1</span>])</span><br><span class="line">w_output = np.asarray([[<span class="number">1.0</span>], [<span class="number">2.0</span>]])</span><br><span class="line">b_output = <span class="number">0.1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(X)):</span><br><span class="line">    before_activation = np.dot(state, w_cell_state) + X[i] * w_cell_input + b_cell</span><br><span class="line">    state = np.tanh(before_activation)</span><br><span class="line">    final_output = np.dot(state, w_output) + b_output</span><br><span class="line">    print(<span class="string">"before activation: "</span>, before_activation)</span><br><span class="line">    print(<span class="string">"state: "</span>, state)</span><br><span class="line">    print(<span class="string">"output: "</span>, final_output)</span><br></pre></td></tr></table></figure><p>全连接层神经网络的输入大小为n+x</p><h3 id="8-2-LSTM"><a href="#8-2-LSTM" class="headerlink" title="8.2 LSTM"></a>8.2 LSTM</h3><p>参考：<a href="https://www.cnblogs.com/zuotongbin/p/10698843.html" target="_blank" rel="noopener">https://www.cnblogs.com/zuotongbin/p/10698843.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">HIDDEN_SIZE = <span class="number">30</span>                            <span class="comment"># LSTM中隐藏节点的个数。</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span>                              <span class="comment"># LSTM的层数。</span></span><br><span class="line">TIMESTEPS = <span class="number">10</span>                              <span class="comment"># 循环神经网络的训练序列长度。</span></span><br><span class="line">TRAINING_STEPS = <span class="number">10000</span>                      <span class="comment"># 训练轮数。</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span>                             <span class="comment"># batch大小。</span></span><br><span class="line">TRAINING_EXAMPLES = <span class="number">10000</span>                   <span class="comment"># 训练数据个数。</span></span><br><span class="line">TESTING_EXAMPLES = <span class="number">1000</span>                     <span class="comment"># 测试数据个数。</span></span><br><span class="line">SAMPLE_GAP = <span class="number">0.01</span>                           <span class="comment"># 采样间隔。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 产生正弦数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_data</span><span class="params">(seq)</span>:</span></span><br><span class="line">    X = []</span><br><span class="line">    y = []</span><br><span class="line">    <span class="comment"># 序列的第i项和后面的TIMESTEPS-1项合在一起作为输入；第i + TIMESTEPS项作为输</span></span><br><span class="line">    <span class="comment"># 出。即用sin函数前面的TIMESTEPS个点的信息，预测第i + TIMESTEPS个点的函数值。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(seq) - TIMESTEPS):</span><br><span class="line">        X.append([seq[i: i + TIMESTEPS]])</span><br><span class="line">        y.append([seq[i + TIMESTEPS]])</span><br><span class="line">    <span class="keyword">return</span> np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 用正弦函数生成训练和测试数据集合。</span></span><br><span class="line">test_start = (TRAINING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP</span><br><span class="line">test_end = test_start + (TESTING_EXAMPLES + TIMESTEPS) * SAMPLE_GAP</span><br><span class="line">train_X, train_y = generate_data(np.sin(np.linspace(</span><br><span class="line">    <span class="number">0</span>, test_start, TRAINING_EXAMPLES + TIMESTEPS, dtype=np.float32)))</span><br><span class="line">test_X, test_y = generate_data(np.sin(np.linspace(</span><br><span class="line">    test_start, test_end, TESTING_EXAMPLES + TIMESTEPS, dtype=np.float32)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_model</span><span class="params">(X, y, is_training)</span>:</span></span><br><span class="line">    <span class="comment"># 使用多层的LSTM结构。</span></span><br><span class="line">    cell = tf.nn.rnn_cell.MultiRNNCell([</span><br><span class="line">        tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) </span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)])    </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用TensorFlow接口将多层的LSTM结构连接成RNN网络并计算其前向传播结果。</span></span><br><span class="line">    outputs, _ = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)</span><br><span class="line">    output = outputs[:, <span class="number">-1</span>, :]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对LSTM网络的输出再做加一层全链接层并计算损失。注意这里默认的损失为平均</span></span><br><span class="line">    <span class="comment"># 平方差损失函数。</span></span><br><span class="line">    predictions = tf.contrib.layers.fully_connected(</span><br><span class="line">        output, <span class="number">1</span>, activation_fn=<span class="literal">None</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 只在训练时计算损失函数和优化步骤。测试时直接返回预测结果。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">        <span class="keyword">return</span> predictions, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 计算损失函数。</span></span><br><span class="line">    loss = tf.losses.mean_squared_error(labels=y, predictions=predictions)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建模型优化器并得到优化步骤。</span></span><br><span class="line">    train_op = tf.contrib.layers.optimize_loss(</span><br><span class="line">        loss, tf.train.get_global_step(),</span><br><span class="line">        optimizer=<span class="string">"Adagrad"</span>, learning_rate=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions, loss, train_op</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_eval</span><span class="params">(sess, test_X, test_y)</span>:</span></span><br><span class="line">    <span class="comment"># 将测试数据以数据集的方式提供给计算图。</span></span><br><span class="line">    ds = tf.data.Dataset.from_tensor_slices((test_X, test_y))</span><br><span class="line">    ds = ds.batch(<span class="number">1</span>)</span><br><span class="line">    X, y = ds.make_one_shot_iterator().get_next()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调用模型得到计算结果。这里不需要输入真实的y值。</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">        prediction, _, _ = lstm_model(X, [<span class="number">0.0</span>], <span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将预测结果存入一个数组。</span></span><br><span class="line">    predictions = []</span><br><span class="line">    labels = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(TESTING_EXAMPLES):</span><br><span class="line">        p, l = sess.run([prediction, y])</span><br><span class="line">        predictions.append(p)</span><br><span class="line">        labels.append(l)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算rmse作为评价指标。</span></span><br><span class="line">    predictions = np.array(predictions).squeeze()</span><br><span class="line">    labels = np.array(labels).squeeze()</span><br><span class="line">    rmse = np.sqrt(((predictions - labels) ** <span class="number">2</span>).mean(axis=<span class="number">0</span>))</span><br><span class="line">    print(<span class="string">"Root Mean Square Error is: %f"</span> % rmse)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#对预测的sin函数曲线进行绘图。</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.plot(predictions, label=<span class="string">'predictions'</span>)</span><br><span class="line">    plt.plot(labels, label=<span class="string">'real_sin'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将训练数据以数据集的方式提供给计算图。</span></span><br><span class="line">ds = tf.data.Dataset.from_tensor_slices((train_X, train_y))</span><br><span class="line">ds = ds.repeat().shuffle(<span class="number">1000</span>).batch(BATCH_SIZE)</span><br><span class="line">X, y = ds.make_one_shot_iterator().get_next()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型，得到预测结果、损失函数，和训练操作。</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</span><br><span class="line">    _, loss, train_op = lstm_model(X, y, <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试在训练之前的模型效果。</span></span><br><span class="line">    print(<span class="string">"Evaluate model before training."</span>)</span><br><span class="line">    run_eval(sess, test_X, test_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练模型。</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(TRAINING_STEPS):</span><br><span class="line">        _, l = sess.run([train_op, loss])</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"train step: "</span> + str(i) + <span class="string">", loss: "</span> + str(l))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用训练好的模型对测试数据进行预测。</span></span><br><span class="line">    print(<span class="string">"Evaluate model after training."</span>)</span><br><span class="line">    run_eval(sess, test_X, test_y)</span><br></pre></td></tr></table></figure><h3 id="9-1-语言模型的背景知识"><a href="#9-1-语言模型的背景知识" class="headerlink" title="9.1 语言模型的背景知识"></a>9.1 语言模型的背景知识</h3><p>将句子看成单词序列，$S=(w_1,w_2,…,w_m)$,其中$m$为句子长度</p><p>则该句子的概率可以表示为：$p(S)=p(w_1)p(w_2|w_1)p(w_3|w_1,w_2)…$，所需$V^m$个参数，$V$为词汇量。</p><p>$n-gram$模型：$p(S)=\prod _i ^m p(w_i|w_{i-n+1},…,w_{i-1})$</p><h4 id="语言模型的评价方法"><a href="#语言模型的评价方法" class="headerlink" title="语言模型的评价方法"></a>语言模型的评价方法</h4><p>$perplexity(S)= \sqrt[m]{\frac{1}{p(w_1,w_2,w_3,…,w_m)}} $</p><p>对数形式：$log(perplexity(S))=-\frac{1}{m}\sum \limits _{i=1} ^m log(w_i|w_1,…,w_{i-1})$</p><p>log perplexity在数学上可以看作真实分布与预测分布之间的交叉熵$H(u,v)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设词汇表的大小为3， 语料包含两个单词"2 0"</span></span><br><span class="line">word_labels = tf.constant([<span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设模型对两个单词预测时，产生的logit分别是[2.0, -1.0, 3.0]和[1.0, 0.0, -0.5]</span></span><br><span class="line">predict_logits = tf.constant([[<span class="number">2.0</span>, <span class="number">-1.0</span>, <span class="number">3.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sparse_softmax_cross_entropy_with_logits计算交叉熵。</span></span><br><span class="line">loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    labels=word_labels, logits=predict_logits)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行程序，计算loss的结果是[0.32656264, 0.46436879], 这对应两个预测的</span></span><br><span class="line"><span class="comment"># perplexity损失。</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [0.32656264 0.4643688 ]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax_cross_entropy_with_logits与上面的函数相似，但是需要将预测目标以</span></span><br><span class="line"><span class="comment"># softmax_cross_entropy_with_logits_v2</span></span><br><span class="line"><span class="comment"># 概率分布的形式给出。</span></span><br><span class="line">word_prob_distribution = tf.constant([[<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]])</span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">    labels=word_prob_distribution, logits=predict_logits)</span><br><span class="line"><span class="comment"># 运行结果与上面相同：[ 0.32656264,  0.46436879]</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(loss))</span><br><span class="line"></span><br><span class="line"><span class="comment"># label smoothing：将正确数据的概率设为一个比1.0略小的值，将错误数据的概率</span></span><br><span class="line"><span class="comment"># 设为比0.0略大的值，这样可以避免模型与数据过拟合，在某些时候可以提高训练效果。</span></span><br><span class="line">word_prob_smooth = tf.constant([[<span class="number">0.01</span>, <span class="number">0.01</span>, <span class="number">0.98</span>], [<span class="number">0.98</span>, <span class="number">0.01</span>, <span class="number">0.01</span>]])</span><br><span class="line">loss = tf.nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    labels=word_prob_smooth, logits=predict_logits)</span><br><span class="line"><span class="comment"># 运行结果：[ 0.37656265,  0.48936883]</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(loss))</span><br></pre></td></tr></table></figure><h3 id="9-2-神经语言模型"><a href="#9-2-神经语言模型" class="headerlink" title="9.2 神经语言模型"></a>9.2 神经语言模型</h3><h4 id="PTB数据集"><a href="#PTB数据集" class="headerlink" title="PTB数据集"></a>PTB数据集</h4><h5 id="文本数据预处理—生产词汇表"><a href="#文本数据预处理—生产词汇表" class="headerlink" title="文本数据预处理—生产词汇表"></a>文本数据预处理—生产词汇表</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">MODE = <span class="string">"PTB"</span>    <span class="comment"># 将MODE设置为"PTB", "TRANSLATE_EN", "TRANSLATE_ZH"之一。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> MODE == <span class="string">"PTB"</span>:             <span class="comment"># PTB数据处理</span></span><br><span class="line">    RAW_DATA = <span class="string">"../../datasets/PTB_data/ptb.train.txt"</span>  <span class="comment"># 训练集数据文件</span></span><br><span class="line">    VOCAB_OUTPUT = <span class="string">"ptb.vocab"</span>                         <span class="comment"># 输出的词汇表文件</span></span><br><span class="line"><span class="keyword">elif</span> MODE == <span class="string">"TRANSLATE_ZH"</span>:  <span class="comment"># 翻译语料的中文部分</span></span><br><span class="line">    RAW_DATA = <span class="string">"../../datasets/TED_data/train.txt.zh"</span></span><br><span class="line">    VOCAB_OUTPUT = <span class="string">"zh.vocab"</span></span><br><span class="line">    VOCAB_SIZE = <span class="number">4000</span></span><br><span class="line"><span class="keyword">elif</span> MODE == <span class="string">"TRANSLATE_EN"</span>:  <span class="comment"># 翻译语料的英文部分</span></span><br><span class="line">    RAW_DATA = <span class="string">"../../datasets/TED_data/train.txt.en"</span></span><br><span class="line">    VOCAB_OUTPUT = <span class="string">"en.vocab"</span></span><br><span class="line">    VOCAB_SIZE = <span class="number">10000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对单词按词频排序</span></span><br><span class="line">counter = collections.Counter()</span><br><span class="line"><span class="keyword">with</span> codecs.open(RAW_DATA, <span class="string">"r"</span>, <span class="string">"utf-8"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> line.strip().split():</span><br><span class="line">            counter[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按词频顺序对单词进行排序。</span></span><br><span class="line">sorted_word_to_cnt = sorted(</span><br><span class="line">    counter.items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">sorted_words = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> sorted_word_to_cnt]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 插入特殊符号</span></span><br><span class="line"><span class="keyword">if</span> MODE == <span class="string">"PTB"</span>:</span><br><span class="line">    <span class="comment"># 稍后我们需要在文本换行处加入句子结束符"&lt;eos&gt;"，这里预先将其加入词汇表。</span></span><br><span class="line">    sorted_words = [<span class="string">"&lt;eos&gt;"</span>] + sorted_words</span><br><span class="line"><span class="keyword">elif</span> MODE <span class="keyword">in</span> [<span class="string">"TRANSLATE_EN"</span>, <span class="string">"TRANSLATE_ZH"</span>]:</span><br><span class="line">    <span class="comment"># 在9.3.2小节处理机器翻译数据时，除了"&lt;eos&gt;"以外，还需要将"&lt;unk&gt;"和句子起始符</span></span><br><span class="line">    <span class="comment"># "&lt;sos&gt;"加入词汇表，并从词汇表中删除低频词汇。</span></span><br><span class="line">    sorted_words = [<span class="string">"&lt;unk&gt;"</span>, <span class="string">"&lt;sos&gt;"</span>, <span class="string">"&lt;eos&gt;"</span>] + sorted_words</span><br><span class="line">    <span class="keyword">if</span> len(sorted_words) &gt; VOCAB_SIZE:</span><br><span class="line">        sorted_words = sorted_words[:VOCAB_SIZE]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存词汇表文件</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(VOCAB_OUTPUT, <span class="string">'w'</span>, <span class="string">'utf-8'</span>) <span class="keyword">as</span> file_output:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sorted_words:</span><br><span class="line">        file_output.write(word + <span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><h5 id="文本数据预处理—生产训练文件"><a href="#文本数据预处理—生产训练文件" class="headerlink" title="文本数据预处理—生产训练文件"></a>文本数据预处理—生产训练文件</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取词汇表，并建立词汇到单词编号的映射。</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(VOCAB, <span class="string">"r"</span>, <span class="string">"utf-8"</span>) <span class="keyword">as</span> f_vocab:</span><br><span class="line">    vocab = [w.strip() <span class="keyword">for</span> w <span class="keyword">in</span> f_vocab.readlines()]</span><br><span class="line">word_to_id = &#123;k: v <span class="keyword">for</span> (k, v) <span class="keyword">in</span> zip(vocab, range(len(vocab)))&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果出现了不在词汇表内的低频词，则替换为"unk"。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_id</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> word_to_id[word] <span class="keyword">if</span> word <span class="keyword">in</span> word_to_id <span class="keyword">else</span> word_to_id[<span class="string">"&lt;unk&gt;"</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据进行替换并保存结果。</span></span><br><span class="line">fin = codecs.open(RAW_DATA, <span class="string">"r"</span>, <span class="string">"utf-8"</span>)</span><br><span class="line">fout = codecs.open(OUTPUT_DATA, <span class="string">'w'</span>, <span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fin:</span><br><span class="line">    words = line.strip().split() + [<span class="string">"&lt;eos&gt;"</span>]  <span class="comment"># 读取单词并添加&lt;eos&gt;结束符</span></span><br><span class="line">    <span class="comment"># 将每个单词替换为词汇表中的编号</span></span><br><span class="line">    out_line = <span class="string">' '</span>.join([str(get_id(w)) <span class="keyword">for</span> w <span class="keyword">in</span> words]) + <span class="string">'\n'</span></span><br><span class="line">    fout.write(out_line)</span><br><span class="line">fin.close()</span><br><span class="line">fout.close()</span><br></pre></td></tr></table></figure><h5 id="PTB数据的batching方法"><a href="#PTB数据的batching方法" class="headerlink" title="PTB数据的batching方法"></a>PTB数据的batching方法</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从文件中读取数据，并返回包含单词编号的数组。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">"r"</span>) <span class="keyword">as</span> fin:</span><br><span class="line">        <span class="comment"># 将整个文档读进一个长字符串。</span></span><br><span class="line">        id_string = <span class="string">' '</span>.join([line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> fin.readlines()])</span><br><span class="line">    id_list = [int(w) <span class="keyword">for</span> w <span class="keyword">in</span> id_string.split()]  <span class="comment"># 将读取的单词编号转为整数</span></span><br><span class="line">    <span class="keyword">return</span> id_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_batches</span><span class="params">(id_list, batch_size, num_step)</span>:</span></span><br><span class="line">    <span class="comment"># 计算总的batch数量。每个batch包含的单词数量是batch_size * num_step。</span></span><br><span class="line">    num_batches = (len(id_list) - <span class="number">1</span>) // (batch_size * num_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如9-4图所示，将数据整理成一个维度为[batch_size, num_batches * num_step]</span></span><br><span class="line">    <span class="comment"># 的二维数组。</span></span><br><span class="line">    data = np.array(id_list[: num_batches * batch_size * num_step])</span><br><span class="line">    data = np.reshape(data, [batch_size, num_batches * num_step])</span><br><span class="line">    <span class="comment"># 沿着第二个维度将数据切分成num_batches个batch，存入一个数组。</span></span><br><span class="line">    data_batches = np.split(data, num_batches, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重复上述操作，但是每个位置向右移动一位。这里得到的是RNN每一步输出所需要预测的</span></span><br><span class="line">    <span class="comment"># 下一个单词。</span></span><br><span class="line">    label = np.array(id_list[<span class="number">1</span> : num_batches * batch_size * num_step + <span class="number">1</span>]) </span><br><span class="line">    label = np.reshape(label, [batch_size, num_batches * num_step])</span><br><span class="line">    label_batches = np.split(label, num_batches, axis=<span class="number">1</span>)  </span><br><span class="line">    <span class="comment"># 返回一个长度为num_batches的数组，其中每一项包括一个data矩阵和一个label矩阵。</span></span><br><span class="line">    <span class="keyword">return</span> list(zip(data_batches, label_batches))</span><br></pre></td></tr></table></figure><h5 id="基于RNN的神经语言模型"><a href="#基于RNN的神经语言模型" class="headerlink" title="基于RNN的神经语言模型"></a>基于RNN的神经语言模型</h5><p>双层LSTM</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">TRAIN_DATA = <span class="string">"ptb.train"</span>          <span class="comment"># 训练数据路径。</span></span><br><span class="line">EVAL_DATA = <span class="string">"ptb.valid"</span>           <span class="comment"># 验证数据路径。</span></span><br><span class="line">TEST_DATA = <span class="string">"ptb.test"</span>            <span class="comment"># 测试数据路径。</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">300</span>                 <span class="comment"># 隐藏层规模。</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span>                    <span class="comment"># 深层循环神经网络中LSTM结构的层数。</span></span><br><span class="line">VOCAB_SIZE = <span class="number">10000</span>                <span class="comment"># 词典规模。</span></span><br><span class="line">TRAIN_BATCH_SIZE = <span class="number">20</span>             <span class="comment"># 训练数据batch的大小。</span></span><br><span class="line">TRAIN_NUM_STEP = <span class="number">35</span>               <span class="comment"># 训练数据截断长度。</span></span><br><span class="line"></span><br><span class="line">EVAL_BATCH_SIZE = <span class="number">1</span>               <span class="comment"># 测试数据batch的大小。</span></span><br><span class="line">EVAL_NUM_STEP = <span class="number">1</span>                 <span class="comment"># 测试数据截断长度。</span></span><br><span class="line">NUM_EPOCH = <span class="number">5</span>                     <span class="comment"># 使用训练数据的轮数。</span></span><br><span class="line">LSTM_KEEP_PROB = <span class="number">0.9</span>              <span class="comment"># LSTM节点不被dropout的概率。</span></span><br><span class="line">EMBEDDING_KEEP_PROB = <span class="number">0.9</span>         <span class="comment"># 词向量不被dropout的概率。</span></span><br><span class="line">MAX_GRAD_NORM = <span class="number">5</span>                 <span class="comment"># 用于控制梯度膨胀的梯度大小上限。</span></span><br><span class="line">SHARE_EMB_AND_SOFTMAX = <span class="literal">True</span>      <span class="comment"># 在Softmax层和词向量层之间共享参数。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过一个PTBModel类来描述模型，这样方便维护循环神经网络中的状态。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PTBModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, is_training, batch_size, num_steps)</span>:</span></span><br><span class="line">        <span class="comment"># 记录使用的batch大小和截断长度。</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义每一步的输入和预期输出。两者的维度都是[batch_size, num_steps]。</span></span><br><span class="line">        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line">        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义使用LSTM结构为循环体结构且使用dropout的深层循环神经网络。</span></span><br><span class="line">        dropout_keep_prob = LSTM_KEEP_PROB <span class="keyword">if</span> is_training <span class="keyword">else</span> <span class="number">1.0</span></span><br><span class="line">        lstm_cells = [</span><br><span class="line">            tf.nn.rnn_cell.DropoutWrapper(</span><br><span class="line">                tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),</span><br><span class="line">                output_keep_prob=dropout_keep_prob)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)]     </span><br><span class="line">        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)            </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化最初的状态，即全零的向量。这个量只在每个epoch初始化第一个batch</span></span><br><span class="line">        <span class="comment"># 时使用。</span></span><br><span class="line">        self.initial_state = cell.zero_state(batch_size, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义单词的词向量矩阵。</span></span><br><span class="line">        embedding = tf.get_variable(<span class="string">"embedding"</span>, [VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将输入单词转化为词向量。</span></span><br><span class="line">        inputs = tf.nn.embedding_lookup(embedding, self.input_data)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 只在训练时使用dropout。</span></span><br><span class="line">        <span class="keyword">if</span> is_training:</span><br><span class="line">            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 定义输出列表。在这里先将不同时刻LSTM结构的输出收集起来，再一起提供给</span></span><br><span class="line">        <span class="comment"># softmax层。</span></span><br><span class="line">        outputs = []</span><br><span class="line">        state = self.initial_state</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"RNN"</span>):</span><br><span class="line">            <span class="keyword">for</span> time_step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">                <span class="keyword">if</span> time_step &gt; <span class="number">0</span>: tf.get_variable_scope().reuse_variables()</span><br><span class="line">                cell_output, state = cell(inputs[:, time_step, :], state)</span><br><span class="line">                outputs.append(cell_output) </span><br><span class="line">        <span class="comment"># 把输出队列展开成[batch, hidden_size*num_steps]的形状，然后再</span></span><br><span class="line">        <span class="comment"># reshape成[batch*numsteps, hidden_size]的形状。</span></span><br><span class="line">        output = tf.reshape(tf.concat(outputs, <span class="number">1</span>), [<span class="number">-1</span>, HIDDEN_SIZE])</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Softmax层：将RNN在每个位置上的输出转化为各个单词的logits。</span></span><br><span class="line">        <span class="keyword">if</span> SHARE_EMB_AND_SOFTMAX:</span><br><span class="line">            weight = tf.transpose(embedding)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weight = tf.get_variable(<span class="string">"weight"</span>, [HIDDEN_SIZE, VOCAB_SIZE])</span><br><span class="line">        bias = tf.get_variable(<span class="string">"bias"</span>, [VOCAB_SIZE])</span><br><span class="line">        logits = tf.matmul(output, weight) + bias</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义交叉熵损失函数和平均损失。</span></span><br><span class="line">        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">            labels=tf.reshape(self.targets, [<span class="number">-1</span>]),</span><br><span class="line">            logits=logits)</span><br><span class="line">        self.cost = tf.reduce_sum(loss) / batch_size</span><br><span class="line">        self.final_state = state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 只在训练模型时定义反向传播操作。</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_training: <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        trainable_variables = tf.trainable_variables()</span><br><span class="line">        <span class="comment"># 控制梯度大小，定义优化方法和训练步骤。</span></span><br><span class="line">        grads, _ = tf.clip_by_global_norm(</span><br><span class="line">            tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM)</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">1.0</span>)</span><br><span class="line">        self.train_op = optimizer.apply_gradients(</span><br><span class="line">            zip(grads, trainable_variables))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用给定的模型model在数据data上运行train_op并返回在全部数据上的perplexity值。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(session, model, batches, train_op, output_log, step)</span>:</span></span><br><span class="line">    <span class="comment"># 计算平均perplexity的辅助变量。</span></span><br><span class="line">    total_costs = <span class="number">0.0</span></span><br><span class="line">    iters = <span class="number">0</span></span><br><span class="line">    state = session.run(model.initial_state) </span><br><span class="line">    <span class="comment"># 训练一个epoch。</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> batches:</span><br><span class="line">        <span class="comment"># 在当前batch上运行train_op并计算损失值。交叉熵损失函数计算的就是下一个单</span></span><br><span class="line">        <span class="comment"># 词为给定单词的概率。</span></span><br><span class="line">        cost, state, _ = session.run(</span><br><span class="line">             [model.cost, model.final_state, train_op],</span><br><span class="line">             &#123;model.input_data: x, model.targets: y,</span><br><span class="line">              model.initial_state: state&#125;)</span><br><span class="line">        total_costs += cost</span><br><span class="line">        iters += model.num_steps</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只有在训练时输出日志。</span></span><br><span class="line">        <span class="keyword">if</span> output_log <span class="keyword">and</span> step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %d steps, perplexity is %.3f"</span> % (</span><br><span class="line">                  step, np.exp(total_costs / iters)))</span><br><span class="line">        step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回给定模型在给定数据上的perplexity值。</span></span><br><span class="line">    <span class="keyword">return</span> step, np.exp(total_costs / iters)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 定义初始化函数。</span></span><br><span class="line">    initializer = tf.random_uniform_initializer(<span class="number">-0.05</span>, <span class="number">0.05</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义训练用的循环神经网络模型。</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>, </span><br><span class="line">                           reuse=<span class="literal">None</span>, initializer=initializer):</span><br><span class="line">        train_model = PTBModel(<span class="literal">True</span>, TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义测试用的循环神经网络模型。它与train_model共用参数，但是没有dropout。</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"language_model"</span>,</span><br><span class="line">                           reuse=<span class="literal">True</span>, initializer=initializer):</span><br><span class="line">        eval_model = PTBModel(<span class="literal">False</span>, EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型。</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        train_batches = make_batches(</span><br><span class="line">            read_data(TRAIN_DATA), TRAIN_BATCH_SIZE, TRAIN_NUM_STEP)</span><br><span class="line">        eval_batches = make_batches(</span><br><span class="line">            read_data(EVAL_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line">        test_batches = make_batches(</span><br><span class="line">            read_data(TEST_DATA), EVAL_BATCH_SIZE, EVAL_NUM_STEP)</span><br><span class="line"></span><br><span class="line">        step = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_EPOCH):</span><br><span class="line">            print(<span class="string">"In iteration: %d"</span> % (i + <span class="number">1</span>))</span><br><span class="line">            step, train_pplx = run_epoch(session, train_model, train_batches,</span><br><span class="line">                                         train_model.train_op, <span class="literal">True</span>, step)</span><br><span class="line">            print(<span class="string">"Epoch: %d Train Perplexity: %.3f"</span> % (i + <span class="number">1</span>, train_pplx))</span><br><span class="line"></span><br><span class="line">            _, eval_pplx = run_epoch(session, eval_model, eval_batches,</span><br><span class="line">                                     tf.no_op(), <span class="literal">False</span>, <span class="number">0</span>)</span><br><span class="line">            print(<span class="string">"Epoch: %d Eval Perplexity: %.3f"</span> % (i + <span class="number">1</span>, eval_pplx))</span><br><span class="line"></span><br><span class="line">        _, test_pplx = run_epoch(session, eval_model, test_batches,</span><br><span class="line">                                 tf.no_op(), <span class="literal">False</span>, <span class="number">0</span>)</span><br><span class="line">        print(<span class="string">"Test Perplexity: %.3f"</span> % test_pplx)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="9-3-神经网络机器翻译"><a href="#9-3-神经网络机器翻译" class="headerlink" title="9.3 神经网络机器翻译"></a>9.3 神经网络机器翻译</h3><p>Seq2Seq模型也称为encoder-decoder模型</p><h4 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设输入数据已经用9.2.1小节中的方法转换成了单词编号的格式。</span></span><br><span class="line">SRC_TRAIN_DATA = <span class="string">"./train.en"</span>        <span class="comment"># 源语言输入文件。</span></span><br><span class="line">TRG_TRAIN_DATA = <span class="string">"./train.zh"</span>        <span class="comment"># 目标语言输入文件。</span></span><br><span class="line">CHECKPOINT_PATH = <span class="string">"./seq2seq_ckpt"</span>   <span class="comment"># checkpoint保存路径。  </span></span><br><span class="line"></span><br><span class="line">HIDDEN_SIZE = <span class="number">1024</span>                   <span class="comment"># LSTM的隐藏层规模。</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span>                       <span class="comment"># 深层循环神经网络中LSTM结构的层数。</span></span><br><span class="line">SRC_VOCAB_SIZE = <span class="number">10000</span>               <span class="comment"># 源语言词汇表大小。</span></span><br><span class="line">TRG_VOCAB_SIZE = <span class="number">4000</span>                <span class="comment"># 目标语言词汇表大小。</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span>                     <span class="comment"># 训练数据batch的大小。</span></span><br><span class="line">NUM_EPOCH = <span class="number">5</span>                        <span class="comment"># 使用训练数据的轮数。</span></span><br><span class="line">KEEP_PROB = <span class="number">0.8</span>                      <span class="comment"># 节点不被dropout的概率。</span></span><br><span class="line">MAX_GRAD_NORM = <span class="number">5</span>                    <span class="comment"># 用于控制梯度膨胀的梯度大小上限。</span></span><br><span class="line">SHARE_EMB_AND_SOFTMAX = <span class="literal">True</span>         <span class="comment"># 在Softmax层和词向量层之间共享参数。</span></span><br><span class="line"></span><br><span class="line">MAX_LEN = <span class="number">50</span>   <span class="comment"># 限定句子的最大单词数量。</span></span><br><span class="line">SOS_ID  = <span class="number">1</span>    <span class="comment"># 目标语言词汇表中&lt;sos&gt;的ID。</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用Dataset从一个文件中读取一个语言的数据。</span></span><br><span class="line"><span class="comment"># 数据的格式为每行一句话，单词已经转化为单词编号。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MakeDataset</span><span class="params">(file_path)</span>:</span></span><br><span class="line">    dataset = tf.data.TextLineDataset(file_path)</span><br><span class="line">    <span class="comment"># 根据空格将单词编号切分开并放入一个一维向量。</span></span><br><span class="line">    dataset = dataset.map(<span class="keyword">lambda</span> string: tf.string_split([string]).values)</span><br><span class="line">    <span class="comment"># 将字符串形式的单词编号转化为整数。</span></span><br><span class="line">    dataset = dataset.map(</span><br><span class="line">        <span class="keyword">lambda</span> string: tf.string_to_number(string, tf.int32))</span><br><span class="line">    <span class="comment"># 统计每个句子的单词数量，并与句子内容一起放入Dataset中。</span></span><br><span class="line">    dataset = dataset.map(<span class="keyword">lambda</span> x: (x, tf.size(x)))</span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从源语言文件src_path和目标语言文件trg_path中分别读取数据，并进行填充和</span></span><br><span class="line"><span class="comment"># batching操作。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MakeSrcTrgDataset</span><span class="params">(src_path, trg_path, batch_size)</span>:</span></span><br><span class="line">    <span class="comment"># 首先分别读取源语言数据和目标语言数据。</span></span><br><span class="line">    src_data = MakeDataset(src_path)</span><br><span class="line">    trg_data = MakeDataset(trg_path)</span><br><span class="line">    <span class="comment"># 通过zip操作将两个Dataset合并为一个Dataset。现在每个Dataset中每一项数据ds</span></span><br><span class="line">    <span class="comment"># 由4个张量组成：</span></span><br><span class="line">    <span class="comment">#   ds[0][0]是源句子</span></span><br><span class="line">    <span class="comment">#   ds[0][1]是源句子长度</span></span><br><span class="line">    <span class="comment">#   ds[1][0]是目标句子</span></span><br><span class="line">    <span class="comment">#   ds[1][1]是目标句子长度</span></span><br><span class="line">    dataset = tf.data.Dataset.zip((src_data, trg_data))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 删除内容为空（只包含&lt;EOS&gt;）的句子和长度过长的句子。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FilterLength</span><span class="params">(src_tuple, trg_tuple)</span>:</span></span><br><span class="line">        ((src_input, src_len), (trg_label, trg_len)) = (src_tuple, trg_tuple)</span><br><span class="line">        src_len_ok = tf.logical_and(</span><br><span class="line">            tf.greater(src_len, <span class="number">1</span>), tf.less_equal(src_len, MAX_LEN))</span><br><span class="line">        trg_len_ok = tf.logical_and(</span><br><span class="line">            tf.greater(trg_len, <span class="number">1</span>), tf.less_equal(trg_len, MAX_LEN))</span><br><span class="line">        <span class="keyword">return</span> tf.logical_and(src_len_ok, trg_len_ok)</span><br><span class="line">    dataset = dataset.filter(FilterLength)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从图9-5可知，解码器需要两种格式的目标句子：</span></span><br><span class="line">    <span class="comment">#   1.解码器的输入(trg_input)，形式如同"&lt;sos&gt; X Y Z"</span></span><br><span class="line">    <span class="comment">#   2.解码器的目标输出(trg_label)，形式如同"X Y Z &lt;eos&gt;"</span></span><br><span class="line">    <span class="comment"># 上面从文件中读到的目标句子是"X Y Z &lt;eos&gt;"的形式，我们需要从中生成"&lt;sos&gt; X Y Z"</span></span><br><span class="line">    <span class="comment"># 形式并加入到Dataset中。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">MakeTrgInput</span><span class="params">(src_tuple, trg_tuple)</span>:</span></span><br><span class="line">        ((src_input, src_len), (trg_label, trg_len)) = (src_tuple, trg_tuple)</span><br><span class="line">        trg_input = tf.concat([[SOS_ID], trg_label[:<span class="number">-1</span>]], axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> ((src_input, src_len), (trg_input, trg_label, trg_len))</span><br><span class="line">    dataset = dataset.map(MakeTrgInput)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 随机打乱训练数据。</span></span><br><span class="line">    dataset = dataset.shuffle(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 规定填充后输出的数据维度。</span></span><br><span class="line">    padded_shapes = (</span><br><span class="line">        (tf.TensorShape([<span class="literal">None</span>]),      <span class="comment"># 源句子是长度未知的向量</span></span><br><span class="line">         tf.TensorShape([])),         <span class="comment"># 源句子长度是单个数字</span></span><br><span class="line">        (tf.TensorShape([<span class="literal">None</span>]),      <span class="comment"># 目标句子（解码器输入）是长度未知的向量</span></span><br><span class="line">         tf.TensorShape([<span class="literal">None</span>]),      <span class="comment"># 目标句子（解码器目标输出）是长度未知的向量</span></span><br><span class="line">         tf.TensorShape([])))         <span class="comment"># 目标句子长度是单个数字</span></span><br><span class="line">    <span class="comment"># 调用padded_batch方法进行batching操作。</span></span><br><span class="line">    batched_dataset = dataset.padded_batch(batch_size, padded_shapes)</span><br><span class="line">    <span class="keyword">return</span> batched_dataset</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义NMTModel类来描述模型。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NMTModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 在模型的初始化函数中定义模型要用到的变量。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 定义编码器和解码器所使用的LSTM结构。</span></span><br><span class="line">        self.enc_cell = tf.nn.rnn_cell.MultiRNNCell(</span><br><span class="line">          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)</span><br><span class="line">           <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)])</span><br><span class="line">        self.dec_cell = tf.nn.rnn_cell.MultiRNNCell(</span><br><span class="line">          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) </span><br><span class="line">           <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为源语言和目标语言分别定义词向量。   </span></span><br><span class="line">        self.src_embedding = tf.get_variable(</span><br><span class="line">            <span class="string">"src_emb"</span>, [SRC_VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line">        self.trg_embedding = tf.get_variable(</span><br><span class="line">            <span class="string">"trg_emb"</span>, [TRG_VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义softmax层的变量</span></span><br><span class="line">        <span class="keyword">if</span> SHARE_EMB_AND_SOFTMAX:</span><br><span class="line">           self.softmax_weight = tf.transpose(self.trg_embedding)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">           self.softmax_weight = tf.get_variable(</span><br><span class="line">               <span class="string">"weight"</span>, [HIDDEN_SIZE, TRG_VOCAB_SIZE])</span><br><span class="line">        self.softmax_bias = tf.get_variable(</span><br><span class="line">            <span class="string">"softmax_bias"</span>, [TRG_VOCAB_SIZE])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在forward函数中定义模型的前向计算图。</span></span><br><span class="line">    <span class="comment"># src_input, src_size, trg_input, trg_label, trg_size分别是上面</span></span><br><span class="line">    <span class="comment"># MakeSrcTrgDataset函数产生的五种张量。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src_input, src_size, trg_input, trg_label, trg_size)</span>:</span></span><br><span class="line">        batch_size = tf.shape(src_input)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 将输入和输出单词编号转为词向量。</span></span><br><span class="line">        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)</span><br><span class="line">        trg_emb = tf.nn.embedding_lookup(self.trg_embedding, trg_input)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在词向量上进行dropout。</span></span><br><span class="line">        src_emb = tf.nn.dropout(src_emb, KEEP_PROB)</span><br><span class="line">        trg_emb = tf.nn.dropout(trg_emb, KEEP_PROB)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用dynamic_rnn构造编码器。</span></span><br><span class="line">        <span class="comment"># 编码器读取源句子每个位置的词向量，输出最后一步的隐藏状态enc_state。</span></span><br><span class="line">        <span class="comment"># 因为编码器是一个双层LSTM，因此enc_state是一个包含两个LSTMStateTuple类</span></span><br><span class="line">        <span class="comment"># 张量的tuple，每个LSTMStateTuple对应编码器中的一层。</span></span><br><span class="line">        <span class="comment"># enc_outputs是顶层LSTM在每一步的输出，它的维度是[batch_size, </span></span><br><span class="line">        <span class="comment"># max_time, HIDDEN_SIZE]。Seq2Seq模型中不需要用到enc_outputs，而</span></span><br><span class="line">        <span class="comment"># 后面介绍的attention模型会用到它。</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">            enc_outputs, enc_state = tf.nn.dynamic_rnn(</span><br><span class="line">                self.enc_cell, src_emb, src_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用dyanmic_rnn构造解码器。</span></span><br><span class="line">        <span class="comment"># 解码器读取目标句子每个位置的词向量，输出的dec_outputs为每一步</span></span><br><span class="line">        <span class="comment"># 顶层LSTM的输出。dec_outputs的维度是 [batch_size, max_time,</span></span><br><span class="line">        <span class="comment"># HIDDEN_SIZE]。</span></span><br><span class="line">        <span class="comment"># initial_state=enc_state表示用编码器的输出来初始化第一步的隐藏状态。</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder"</span>):</span><br><span class="line">            dec_outputs, _ = tf.nn.dynamic_rnn(</span><br><span class="line">                self.dec_cell, trg_emb, trg_size, initial_state=enc_state)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算解码器每一步的log perplexity。这一步与语言模型代码相同。</span></span><br><span class="line">        output = tf.reshape(dec_outputs, [<span class="number">-1</span>, HIDDEN_SIZE])</span><br><span class="line">        logits = tf.matmul(output, self.softmax_weight) + self.softmax_bias</span><br><span class="line">        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">            labels=tf.reshape(trg_label, [<span class="number">-1</span>]), logits=logits)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在计算平均损失时，需要将填充位置的权重设置为0，以避免无效位置的预测干扰</span></span><br><span class="line">        <span class="comment"># 模型的训练。</span></span><br><span class="line">        label_weights = tf.sequence_mask(</span><br><span class="line">            trg_size, maxlen=tf.shape(trg_label)[<span class="number">1</span>], dtype=tf.float32)</span><br><span class="line">        label_weights = tf.reshape(label_weights, [<span class="number">-1</span>])</span><br><span class="line">        cost = tf.reduce_sum(loss * label_weights)</span><br><span class="line">        cost_per_token = cost / tf.reduce_sum(label_weights)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义反向传播操作。反向操作的实现与语言模型代码相同。</span></span><br><span class="line">        trainable_variables = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 控制梯度大小，定义优化方法和训练步骤。</span></span><br><span class="line">        grads = tf.gradients(cost / tf.to_float(batch_size),</span><br><span class="line">                             trainable_variables)</span><br><span class="line">        grads, _ = tf.clip_by_global_norm(grads, MAX_GRAD_NORM)</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">1.0</span>)</span><br><span class="line">        train_op = optimizer.apply_gradients(</span><br><span class="line">            zip(grads, trainable_variables))</span><br><span class="line">        <span class="keyword">return</span> cost_per_token, train_op</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用给定的模型model上训练一个epoch，并返回全局步数。</span></span><br><span class="line"><span class="comment"># 每训练200步便保存一个checkpoint。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(session, cost_op, train_op, saver, step)</span>:</span></span><br><span class="line">    <span class="comment"># 训练一个epoch。</span></span><br><span class="line">    <span class="comment"># 重复训练步骤直至遍历完Dataset中所有数据。</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 运行train_op并计算损失值。训练数据在main()函数中以Dataset方式提供。</span></span><br><span class="line">            cost, _ = session.run([cost_op, train_op])</span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"After %d steps, per token cost is %.3f"</span> % (step, cost))</span><br><span class="line">            <span class="comment"># 每200步保存一个checkpoint。</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">                saver.save(session, CHECKPOINT_PATH, global_step=step)</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span> tf.errors.OutOfRangeError:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> step</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 定义初始化函数。</span></span><br><span class="line">    initializer = tf.random_uniform_initializer(<span class="number">-0.05</span>, <span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义训练用的循环神经网络模型。</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"nmt_model"</span>, reuse=<span class="literal">None</span>, </span><br><span class="line">                           initializer=initializer):</span><br><span class="line">        train_model = NMTModel()</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># 定义输入数据。</span></span><br><span class="line">    data = MakeSrcTrgDataset(SRC_TRAIN_DATA, TRG_TRAIN_DATA, BATCH_SIZE)</span><br><span class="line">    iterator = data.make_initializable_iterator()</span><br><span class="line">    (src, src_size), (trg_input, trg_label, trg_size) = iterator.get_next()</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 定义前向计算图。输入数据以张量形式提供给forward函数。</span></span><br><span class="line">    cost_op, train_op = train_model.forward(src, src_size, trg_input,</span><br><span class="line">                                            trg_label, trg_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型。</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(NUM_EPOCH):</span><br><span class="line">            print(<span class="string">"In iteration: %d"</span> % (i + <span class="number">1</span>))</span><br><span class="line">            sess.run(iterator.initializer)</span><br><span class="line">            step = run_epoch(sess, cost_op, train_op, saver, step)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> sys</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取checkpoint的路径。9000表示是训练程序在第9000步保存的checkpoint。</span></span><br><span class="line">CHECKPOINT_PATH = <span class="string">"./seq2seq_ckpt-9000"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型参数。必须与训练时的模型参数保持一致。</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">1024</span>                         	<span class="comment"># LSTM的隐藏层规模。</span></span><br><span class="line">NUM_LAYERS = <span class="number">2</span>                             	<span class="comment"># 深层循环神经网络中LSTM结构的层数。</span></span><br><span class="line">SRC_VOCAB_SIZE = <span class="number">10000</span>                   	<span class="comment"># 源语言词汇表大小。</span></span><br><span class="line">TRG_VOCAB_SIZE = <span class="number">4000</span>                    	<span class="comment"># 目标语言词汇表大小。</span></span><br><span class="line">SHARE_EMB_AND_SOFTMAX = <span class="literal">True</span>            	<span class="comment"># 在Softmax层和词向量层之间共享参数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词汇表文件</span></span><br><span class="line">SRC_VOCAB = <span class="string">"./en.vocab"</span></span><br><span class="line">TRG_VOCAB = <span class="string">"./zh.vocab"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 词汇表中&lt;sos&gt;和&lt;eos&gt;的ID。在解码过程中需要用&lt;sos&gt;作为第一步的输入，并将检查</span></span><br><span class="line"><span class="comment"># 是否是&lt;eos&gt;，因此需要知道这两个符号的ID。</span></span><br><span class="line">SOS_ID = <span class="number">1</span></span><br><span class="line">EOS_ID = <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义NMTModel类来描述模型。</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NMTModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 在模型的初始化函数中定义模型要用到的变量。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 定义编码器和解码器所使用的LSTM结构。</span></span><br><span class="line">        self.enc_cell = tf.nn.rnn_cell.MultiRNNCell(</span><br><span class="line">          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)</span><br><span class="line">           <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)])</span><br><span class="line">        self.dec_cell = tf.nn.rnn_cell.MultiRNNCell(</span><br><span class="line">          [tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) </span><br><span class="line">           <span class="keyword">for</span> _ <span class="keyword">in</span> range(NUM_LAYERS)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为源语言和目标语言分别定义词向量。   </span></span><br><span class="line">        self.src_embedding = tf.get_variable(</span><br><span class="line">            <span class="string">"src_emb"</span>, [SRC_VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line">        self.trg_embedding = tf.get_variable(</span><br><span class="line">            <span class="string">"trg_emb"</span>, [TRG_VOCAB_SIZE, HIDDEN_SIZE])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义softmax层的变量</span></span><br><span class="line">        <span class="keyword">if</span> SHARE_EMB_AND_SOFTMAX:</span><br><span class="line">            self.softmax_weight = tf.transpose(self.trg_embedding)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.softmax_weight = tf.get_variable(</span><br><span class="line">               <span class="string">"weight"</span>, [HIDDEN_SIZE, TRG_VOCAB_SIZE])</span><br><span class="line">        self.softmax_bias = tf.get_variable(</span><br><span class="line">            <span class="string">"softmax_bias"</span>, [TRG_VOCAB_SIZE])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, src_input)</span>:</span></span><br><span class="line">        <span class="comment"># 虽然输入只有一个句子，但因为dynamic_rnn要求输入是batch的形式，因此这里</span></span><br><span class="line">        <span class="comment"># 将输入句子整理为大小为1的batch。</span></span><br><span class="line">        src_size = tf.convert_to_tensor([len(src_input)], dtype=tf.int32)</span><br><span class="line">        src_input = tf.convert_to_tensor([src_input], dtype=tf.int32)</span><br><span class="line">        src_emb = tf.nn.embedding_lookup(self.src_embedding, src_input)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 使用dynamic_rnn构造编码器。这一步与训练时相同。</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">            enc_outputs, enc_state = tf.nn.dynamic_rnn(</span><br><span class="line">                self.enc_cell, src_emb, src_size, dtype=tf.float32)</span><br><span class="line">   </span><br><span class="line">        <span class="comment"># 设置解码的最大步数。这是为了避免在极端情况出现无限循环的问题。</span></span><br><span class="line">        MAX_DEC_LEN=<span class="number">100</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"decoder/rnn/multi_rnn_cell"</span>):</span><br><span class="line">            <span class="comment"># 使用一个变长的TensorArray来存储生成的句子。</span></span><br><span class="line">            init_array = tf.TensorArray(dtype=tf.int32, size=<span class="number">0</span>,</span><br><span class="line">                dynamic_size=<span class="literal">True</span>, clear_after_read=<span class="literal">False</span>)</span><br><span class="line">            <span class="comment"># 填入第一个单词&lt;sos&gt;作为解码器的输入。</span></span><br><span class="line">            init_array = init_array.write(<span class="number">0</span>, SOS_ID)</span><br><span class="line">            <span class="comment"># 构建初始的循环状态。循环状态包含循环神经网络的隐藏状态，保存生成句子的</span></span><br><span class="line">            <span class="comment"># TensorArray，以及记录解码步数的一个整数step。</span></span><br><span class="line">            init_loop_var = (enc_state, init_array, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># tf.while_loop的循环条件：</span></span><br><span class="line">            <span class="comment"># 循环直到解码器输出&lt;eos&gt;，或者达到最大步数为止。</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">continue_loop_condition</span><span class="params">(state, trg_ids, step)</span>:</span></span><br><span class="line">                <span class="keyword">return</span> tf.reduce_all(tf.logical_and(</span><br><span class="line">                    tf.not_equal(trg_ids.read(step), EOS_ID),</span><br><span class="line">                    tf.less(step, MAX_DEC_LEN<span class="number">-1</span>)))</span><br><span class="line"></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">loop_body</span><span class="params">(state, trg_ids, step)</span>:</span></span><br><span class="line">                <span class="comment"># 读取最后一步输出的单词，并读取其词向量。</span></span><br><span class="line">                trg_input = [trg_ids.read(step)]</span><br><span class="line">                trg_emb = tf.nn.embedding_lookup(self.trg_embedding,</span><br><span class="line">                                                 trg_input)</span><br><span class="line">                <span class="comment"># 这里不使用dynamic_rnn，而是直接调用dec_cell向前计算一步。</span></span><br><span class="line">                dec_outputs, next_state = self.dec_cell.call(</span><br><span class="line">                    state=state, inputs=trg_emb)</span><br><span class="line">                <span class="comment"># 计算每个可能的输出单词对应的logit，并选取logit值最大的单词作为</span></span><br><span class="line">                <span class="comment"># 这一步的而输出。</span></span><br><span class="line">                output = tf.reshape(dec_outputs, [<span class="number">-1</span>, HIDDEN_SIZE])</span><br><span class="line">                logits = (tf.matmul(output, self.softmax_weight)</span><br><span class="line">                          + self.softmax_bias)</span><br><span class="line">                next_id = tf.argmax(logits, axis=<span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">                <span class="comment"># 将这一步输出的单词写入循环状态的trg_ids中。</span></span><br><span class="line">                trg_ids = trg_ids.write(step+<span class="number">1</span>, next_id[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">return</span> next_state, trg_ids, step+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 执行tf.while_loop，返回最终状态。</span></span><br><span class="line">            state, trg_ids, step = tf.while_loop(</span><br><span class="line">                continue_loop_condition, loop_body, init_loop_var)</span><br><span class="line">            <span class="keyword">return</span> trg_ids.stack()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 翻译一个测试句子</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 定义训练用的循环神经网络模型。</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"nmt_model"</span>, reuse=<span class="literal">None</span>):</span><br><span class="line">        model = NMTModel()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义个测试句子。</span></span><br><span class="line">    test_en_text = <span class="string">"This is a test . &lt;eos&gt;"</span></span><br><span class="line">    print(test_en_text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据英文词汇表，将测试句子转为单词ID。</span></span><br><span class="line">    <span class="keyword">with</span> codecs.open(SRC_VOCAB, <span class="string">"r"</span>, <span class="string">"utf-8"</span>) <span class="keyword">as</span> f_vocab:</span><br><span class="line">        src_vocab = [w.strip() <span class="keyword">for</span> w <span class="keyword">in</span> f_vocab.readlines()]</span><br><span class="line">        src_id_dict = dict((src_vocab[x], x) <span class="keyword">for</span> x <span class="keyword">in</span> range(len(src_vocab)))</span><br><span class="line">    test_en_ids = [(src_id_dict[token] <span class="keyword">if</span> token <span class="keyword">in</span> src_id_dict <span class="keyword">else</span> src_id_dict[<span class="string">'&lt;unk&gt;'</span>])</span><br><span class="line">                   <span class="keyword">for</span> token <span class="keyword">in</span> test_en_text.split()]</span><br><span class="line">    print(test_en_ids)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 建立解码所需的计算图。</span></span><br><span class="line">    output_op = model.inference(test_en_ids)</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(sess, CHECKPOINT_PATH)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取翻译结果。</span></span><br><span class="line">    output_ids = sess.run(output_op)</span><br><span class="line">    print(output_ids)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据中文词汇表，将翻译结果转换为中文文字。</span></span><br><span class="line">    <span class="keyword">with</span> codecs.open(TRG_VOCAB, <span class="string">"r"</span>, <span class="string">"utf-8"</span>) <span class="keyword">as</span> f_vocab:</span><br><span class="line">        trg_vocab = [w.strip() <span class="keyword">for</span> w <span class="keyword">in</span> f_vocab.readlines()]</span><br><span class="line">    output_text = <span class="string">''</span>.join([trg_vocab[x] <span class="keyword">for</span> x <span class="keyword">in</span> output_ids])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出翻译结果。</span></span><br><span class="line">    print(output_text.encode(<span class="string">'utf8'</span>).decode(sys.stdout.encoding))</span><br><span class="line">    sess.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h4></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/框架/" rel="tag"><i class="fa fa-tag"></i> 框架</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/01/08/实战Google深度学习框架-第3-5章/" rel="next" title="实战Google深度学习框架-第3~5章"><i class="fa fa-chevron-left"></i> 实战Google深度学习框架-第3~5章</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/01/24/实战Google深度学习框架-第10-12章/" rel="prev" title="实战Google深度学习框架-第10~12章">实战Google深度学习框架-第10~12章 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/kikyo.jpg" alt="Kikyō"><p class="site-author-name" itemprop="name">Kikyō</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">33</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-CNN"><span class="nav-text">6.2 CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积层和池化层"><span class="nav-text">卷积层和池化层</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-经典卷积网络模型"><span class="nav-text">6.4 经典卷积网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-2-LeNet-5"><span class="nav-text">6.4.2 LeNet-5</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#训练"><span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-2-Inception-v3"><span class="nav-text">6.4.2 Inception-v3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-迁移学习"><span class="nav-text">6.5 迁移学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-TFRecord"><span class="nav-text">7.1 TFRecord</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-图像数据处理"><span class="nav-text">7.2 图像数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-TensorFlow图像处理函数"><span class="nav-text">7.2.1 TensorFlow图像处理函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-多线程输入数据处理框架"><span class="nav-text">7.3 多线程输入数据处理框架</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-1-队列与多线程"><span class="nav-text">7.3.1 队列与多线程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-2-输入文件队列"><span class="nav-text">7.3.2 输入文件队列</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-3-组合训练数据"><span class="nav-text">7.3.3 组合训练数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-4-输入数据处理框架"><span class="nav-text">7.3.4 输入数据处理框架</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4数据集"><span class="nav-text">7.4数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-RNN"><span class="nav-text">8.1 RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-LSTM"><span class="nav-text">8.2 LSTM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-语言模型的背景知识"><span class="nav-text">9.1 语言模型的背景知识</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#语言模型的评价方法"><span class="nav-text">语言模型的评价方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-神经语言模型"><span class="nav-text">9.2 神经语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PTB数据集"><span class="nav-text">PTB数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#文本数据预处理—生产词汇表"><span class="nav-text">文本数据预处理—生产词汇表</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#文本数据预处理—生产训练文件"><span class="nav-text">文本数据预处理—生产训练文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PTB数据的batching方法"><span class="nav-text">PTB数据的batching方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基于RNN的神经语言模型"><span class="nav-text">基于RNN的神经语言模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-神经网络机器翻译"><span class="nav-text">9.3 神经网络机器翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#训练-1"><span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试"><span class="nav-text">测试</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#注意力机制"><span class="nav-text">注意力机制</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Kikyō</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">160k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script></body></html><!-- rebuild by neat -->