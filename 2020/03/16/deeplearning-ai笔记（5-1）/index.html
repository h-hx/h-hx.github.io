<!-- build time:Tue May 25 2021 16:59:44 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|DejaVu Sans Mono for Powerline:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-flower.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-flower.png?v=5.1.4"><meta name="keywords" content="深度学习,"><meta name="description" content="循环序列模型（Recurrent Neural Networks）1.1 为什么选择序列模型？（Why Sequence Models?）1.2 数学符号（Notation）比如要建立一个序列模型，用于识别句中人名位置的序列模型，即命名实体识别问题。用$x^{\langle t \rangle}$来索引这个序列的中间位置。$t$意味着它们是时序序列，但不论是否是时序序列，我们都将用$t$来索引序列"><meta name="keywords" content="深度学习"><meta property="og:type" content="article"><meta property="og:title" content="deeplearning-ai笔记（5-1）"><meta property="og:url" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/index.html"><meta property="og:site_name"><meta property="og:description" content="循环序列模型（Recurrent Neural Networks）1.1 为什么选择序列模型？（Why Sequence Models?）1.2 数学符号（Notation）比如要建立一个序列模型，用于识别句中人名位置的序列模型，即命名实体识别问题。用$x^{\langle t \rangle}$来索引这个序列的中间位置。$t$意味着它们是时序序列，但不论是否是时序序列，我们都将用$t$来索引序列"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584339778623.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584339942835.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584340265481.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584340666375.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584341068697.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584341492801.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584341767595.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584341969844.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584342233365.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584342526558.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584342574253.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584342818803.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584343026474.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584343508188.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584343878680.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584344146614.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584344436479.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584345040620.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584345615940.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584345758746.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584346082206.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584346210101.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584346364708.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584346509959.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584346977943.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584347026009.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584347128143.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584427092952.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584427787391.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584429763258.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584429883421.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584430666170.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584432593143.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584434881145.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584435049643.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584435414319.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584447272362.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584447433355.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584449910545.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584450014791.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584450856819.png"><meta property="og:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584450956611.png"><meta property="og:updated_time" content="2020-03-22T03:11:22.357Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="deeplearning-ai笔记（5-1）"><meta name="twitter:description" content="循环序列模型（Recurrent Neural Networks）1.1 为什么选择序列模型？（Why Sequence Models?）1.2 数学符号（Notation）比如要建立一个序列模型，用于识别句中人名位置的序列模型，即命名实体识别问题。用$x^{\langle t \rangle}$来索引这个序列的中间位置。$t$意味着它们是时序序列，但不论是否是时序序列，我们都将用$t$来索引序列"><meta name="twitter:image" content="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/1584339778623.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"right",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/"><title>deeplearning-ai笔记（5-1） |</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title"></span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/16/deeplearning-ai笔记（5-1）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Kikyō"><meta itemprop="description" content><meta itemprop="image" content="/images/kikyo.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content></span><header class="post-header"><h1 class="post-title" itemprop="name headline">deeplearning-ai笔记（5-1）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-16T14:16:09+08:00">2020-03-16 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-03-22T11:11:22+08:00">2020-03-22 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deeplearning-ai笔记/" itemprop="url" rel="index"><span itemprop="name">deeplearning.ai笔记</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">13.7k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">78</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="循环序列模型（Recurrent-Neural-Networks）"><a href="#循环序列模型（Recurrent-Neural-Networks）" class="headerlink" title="循环序列模型（Recurrent Neural Networks）"></a>循环序列模型（Recurrent Neural Networks）</h2><h3 id="1-1-为什么选择序列模型？（Why-Sequence-Models-）"><a href="#1-1-为什么选择序列模型？（Why-Sequence-Models-）" class="headerlink" title="1.1 为什么选择序列模型？（Why Sequence Models?）"></a>1.1 为什么选择序列模型？（Why Sequence Models?）</h3><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584339778623.png" alt="1584339778623"></p><h3 id="1-2-数学符号（Notation）"><a href="#1-2-数学符号（Notation）" class="headerlink" title="1.2 数学符号（Notation）"></a>1.2 数学符号（Notation）</h3><p>比如要建立一个序列模型，用于识别句中人名位置的序列模型，即命名实体识别问题。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584339942835.png" alt="1584339942835"></p><p>用$x^{\langle t \rangle}$来索引这个序列的中间位置。$t$意味着它们是时序序列，但不论是否是时序序列，我们都将用$t$来索引序列中的位置。</p><p>$T_{x}$和$T_{y}$来表示输入和输出序列的长度。</p><p>在这个例子中，如果有一张词表，并且使用<strong>one-hot</strong>表示法来表示词典里的每个单词。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584340265481.png" alt="1584340265481"></p><p>用这样的表示方式表示$X$，用序列模型在$X$和目标输出$Y$之间学习建立一个映射，把它当作监督学习的问题。</p><p>对于不在词表的单词，用&lt;<strong>UNK</strong>&gt;作为标记。</p><h3 id="1-3-循环神经网络模型（Recurrent-Neural-Network-Model）"><a href="#1-3-循环神经网络模型（Recurrent-Neural-Network-Model）" class="headerlink" title="1.3 循环神经网络模型（Recurrent Neural Network Model）"></a>1.3 循环神经网络模型（Recurrent Neural Network Model）</h3><p>怎样才能建立一个模型，建立一个神经网络来学习$X$到$Y$的映射。</p><p>可以尝试的方法之一是使用标准神经网络，将它们输入到一个标准神经网络中，经过一些隐藏层，最终会输出9个值为0或1的项，它表明每个输入单词是否是人名的一部分。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584340666375.png" alt="1584340666375"></p><p>但是这样会有个主要问题：</p><ul><li>输入和输出数据在不同例子中可能有不同的长度。</li></ul><p>即使每个句子都有最大长度，使用填充（<strong>pad</strong>）或零填充（<strong>zero pad</strong>）使每个输入语句都达到最大长度，但仍然看起来不是一个好的表达方式。</p><ul><li><p>这种朴素的神经网络结果并不能共享从文本不同位置所学习到的特征 。</p><p>卷积神经网络中学到的特征的快速地推广到图片其他位置 。</p></li></ul><p>并且使用<strong>one-hot</strong>向量，这会是一个十分庞大的输入层，第一层的权重矩阵也会有着巨量的参数。</p><h4 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h4><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584341068697.png" alt="1584341068697"></p><p>要开始整个流程，在零时刻需要构造一个激活值$a^{\langle 0 \rangle}$，这通常是零向量。有些研究人员会随机用其他方法初始化$a^{\langle 0 \rangle}$。</p><p>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的。</p><p>$W_{\text{ax}}$来表示管理着从$x^{\langle t \rangle}$到隐藏层的连接，每个时间步使用的都是相同的参数$W_{\text{ax}}$。</p><p>$W_{aa}$来决定激活值$a^{\langle t \rangle}$ 到隐藏层的连接 。</p><p>$W_{ya}$ 管理隐藏层到激活值$y^{\langle t \rangle}$的连接。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584341492801.png" alt="1584341492801"></p><p>在这个循环神经网络中，它的意思是在预测${\hat{y}}^{\langle 3 \rangle}$时，不仅要使用$x^{\langle 3 \rangle}$的信息，还要使用来自$x^{\langle 1 \rangle}$和$x^{\langle 2 \rangle}$的信息，因为来自$x^{\langle 1 \rangle}$的信息可以通过这样的路径（上图编号1所示的路径）来帮助预测${\hat{y}}^{\langle 3 \rangle}$。这个循环神经网络的一个缺点就是它只使用了这个序列中之前的信息来做出预测，尤其当预测${\hat{y}}^{\langle 3 \rangle}$时，它没有用到$x^{\langle 4 \rangle}$，$x^{\langle 5 \rangle}$，$x^{\langle 6 \rangle}$等等的信息。</p><h4 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h4><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584341767595.png" alt="1584341767595"></p><p>$a^{\langle 1 \rangle} = g_{1}(W_{aa}a^{\langle 0 \rangle} + W_{ax}x^{\langle 1 \rangle} + b_{a})$</p><p>$\hat y^{\langle 1 \rangle} = g_{2}(W_{ya}a^{\langle 1 \rangle} + b_{y})$</p><p>循环神经网络用的激活函数经常是<strong>tanh</strong>，不过有时候也会用<strong>ReLU</strong>，但是<strong>tanh</strong>是更通常的选择，有其他方法来避免梯度消失问题。</p><p>如果它是一个二分问题，使用<strong>sigmoid</strong>函数作为激活函数，如果是$k$类别分类问题的话，那么可以选用<strong>softmax</strong>作为激活函数。</p><p>更一般的情况下，在$t$时刻，</p><p>$a^{\langle t \rangle} = g_{1}(W_{aa}a^{\langle t - 1 \rangle} + W_{ax}x^{\langle t \rangle} + b_{a})$</p><p>$\hat y^{\langle t \rangle} = g_{2}(W_{ya}a^{\langle t \rangle} + b_{y})$</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584341969844.png" alt="1584341969844"></p><p>简化这些符号：</p><p>$a^{\langle t \rangle} =g(W_{a}\left\lbrack a^{\langle t-1 \rangle},x^{\langle t \rangle} \right\rbrack +b_{a})$</p><p>$\hat y^{\langle t \rangle} = g(W_{y}a^{\langle t \rangle} +b_{y})$</p><p>其中：</p><p>$[ {W_{aa}}\vdots {W_{ax}}]=W_{a}$，如果$a$是100维的，$x$是10,000维的，那么$W_{aa}$就是个$（100，100）$维的矩阵，$W_{ax}$就是个$（100，10000）$维的矩阵，因此如果将这两个矩阵堆起来，$W_{a}$就会是个$（100，10100）$维的矩阵。</p><p>$\begin{bmatrix}a^{\langle t-1 \rangle} \\ x^{\langle t \rangle} \\\end{bmatrix}$，就是个10,100维的向量。</p><p><strong>RNN</strong>前向传播示意图：</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584342233365.png" alt="1584342233365"></p><h3 id="1-4-通过时间的反向传播（Backpropagation-through-time）"><a href="#1-4-通过时间的反向传播（Backpropagation-through-time）" class="headerlink" title="1.4 通过时间的反向传播（Backpropagation through time）"></a>1.4 通过时间的反向传播（Backpropagation through time）</h3><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584342526558.png" alt="1584342526558"></p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584342574253.png" alt="1584342574253"></p><p>为了计算反向传播，先定义一个元素损失函数（上图编号1所示）</p><p>$L^{\langle t \rangle}( \hat y^{\langle t \rangle},y^{\langle t \rangle}) = - y^{\langle t \rangle}\log\hat y^{\langle t \rangle}-( 1-\hat y^{\langle t \rangle})log(1-\hat y^{\langle t \rangle})$</p><p>定义整个序列的损失函数，将$L$定义为（上图编号2所示）</p><p>$L(\hat y,y) = \ \sum \limits_{t = 1}^{T_{x}}{L^{\langle t \rangle}(\hat y^{\langle t \rangle},y^{\langle t \rangle})}$</p><p><strong>RNN</strong>反向传播示意图：</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584342818803.png" alt="1584342818803"></p><h3 id="1-5-不同类型的循环神经网络（Different-types-of-RNNs）"><a href="#1-5-不同类型的循环神经网络（Different-types-of-RNNs）" class="headerlink" title="1.5 不同类型的循环神经网络（Different types of RNNs）"></a>1.5 不同类型的循环神经网络（Different types of <strong>RNN</strong>s）</h3><p>对于其他一些应用，$T_{x}$和$T_{y}$并不一定相等。</p><ul><li><strong>many-to-many</strong></li><li><strong>many-to-one</strong></li><li><strong>one-to-one</strong></li><li><strong>one-to-many</strong></li></ul><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584343026474.png" alt="1584343026474"></p><h3 id="1-6-语言模型和序列生成（Language-model-and-sequence-generation）"><a href="#1-6-语言模型和序列生成（Language-model-and-sequence-generation）" class="headerlink" title="1.6 语言模型和序列生成（Language model and sequence generation）"></a>1.6 语言模型和序列生成（Language model and sequence generation）</h3><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584343508188.png" alt="1584343508188"></p><p>一个语音识别模型可能算出第一句话的概率是$P( \text{The apple and pair salad}) = 3.2 \times 10^{-13}$，而第二句话的概率是$P\left(\text{The apple and pear salad} \right) = 5.7 \times 10^{-10}$，比较这两个概率值，语音识别系统将会选择第二句话。</p><p><strong>使用RNN构建语言模型：</strong></p><ul><li>训练集：一个很大的语言文本语料库；</li><li>Tokenize：将句子使用字典库标记化；</li><li>其中，未出现在字典库中的词使用<strong>UNK</strong>来表示；</li><li>第一步：使用零向量对输出进行预测，即预测第一个单词是某个单词的可能性；</li><li>第二步：通过前面的输入，逐步预测后面一个单词出现的概率；</li><li>训练网络：使用<strong>softmax</strong>损失函数计算损失，对网络进行参数更新，提升语言模型的准确率。</li></ul><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584343878680.png" alt="1584343878680"></p><p><strong>softmax</strong>损失函数，$L\left( \hat y^{\langle t \rangle},y^{\langle t \rangle}\right) = - \sum_{i}^{}{y_{i}^{\langle t \rangle}\log\hat y_{i}^{\langle t \rangle}}$</p><p>总体损失函数（上图编号9所示）$L = \sum_{t}^{}{L^{\langle t \rangle}\left( \hat y^{\langle t \rangle},y^{\langle t \rangle} \right)}$</p><h3 id="1-7-对新序列采样（Sampling-novel-sequences）"><a href="#1-7-对新序列采样（Sampling-novel-sequences）" class="headerlink" title="1.7 对新序列采样（Sampling novel sequences）"></a>1.7 对新序列采样（Sampling novel sequences）</h3><p>记住一个序列模型模拟了任意特定单词序列的概率，我们要做的就是对这些概率分布进行采样来生成一个新的单词序列。下图编号1所示的网络已经被上方所展示的结构训练训练过了，而为了进行采样（下图编号2所示的网络），你要做一些截然不同的事情。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584344146614.png" alt="1584344146614"></p><ul><li>首先输入$x^{\langle 1 \rangle}=0$，$a^{\langle 0 \rangle}=0$, 在这第一个时间步，我们得到所有可能的输出经过softmax层后可能的概率，根据这个softmax的分布，进行随机采样，获取第一个随机采样单词 $y^{\langle 1 \rangle}$</li><li>然后继续下一个时间步，我们以刚刚采样得到的$y^{\langle 1 \rangle}$作为下一个时间步的输入，进而softmax层会预测下一个输出$y^{\langle 2 \rangle}$</li><li><p>如果字典中有结束的标志如：<strong>EOS</strong>，那么输出是该符号时则表示结束；若没有这种标志，则我们可以自行设置结束的时间步。</p><p>上面的模型是基于词汇的语言模型，我们还可以构建基于字符的语言模型，其中每个单词和符号则表示一个相应的输入或者输出：</p></li></ul><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584344436479.png" alt="1584344436479"></p><p>在这种情况下，你的字典仅包含从<strong>a</strong>到<strong>z</strong>的字母，可能还会有空格符，如果你需要的话，还可以有数字0到9，如果你想区分字母大小写，你可以再加上大写的字母。</p><p>基于字符的语言模型一个主要缺点就是你最后会得到太多太长的序列。 其对于捕捉句子前后依赖关系，也就是句子前部分如何影响后面部分，不如基于词汇的语言模型那样效果好；同时基于字符的语言模型训练代价比较高。所以目前的趋势和常见的均是基于词汇的语言模型。但随着计算机运算能力的增强，在一些特定的情况下，也会开始使用基于字符的语言模型。</p><h3 id="1-8-循环神经网络的梯度消失（Vanishing-gradients-with-RNNs）"><a href="#1-8-循环神经网络的梯度消失（Vanishing-gradients-with-RNNs）" class="headerlink" title="1.8 循环神经网络的梯度消失（Vanishing gradients with RNNs）"></a>1.8 循环神经网络的梯度消失（Vanishing gradients with <strong>RNN</strong>s）</h3><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584345040620.png" alt="1584345040620"></p><p>假如看到这个句子（上图编号1所示），“<strong>The cat, which already ate ……, was full.</strong>”，前后应该保持一致，因为<strong>cat</strong>是单数，所以应该用<strong>was</strong>。“<strong>The cats, which ate ……, were full.</strong>”（上图编号2所示），<strong>cats</strong>是复数，所以用<strong>were</strong>。这个例子中的句子有长期的依赖，最前面的单词对句子后面的单词有影响。但是我们目前见到的基本的<strong>RNN</strong>模型（上图编号3所示的网络模型），不擅长捕获这种长期依赖效应。</p><p>如果这是个很深的神经网络，从输出$\hat y$得到的梯度很难传播回去，很难影响靠前层的权重，很难影响前面层（编号5所示的层）的计算。</p><p>对于有同样问题的<strong>RNN</strong>，首先从左到右前向传播，然后反向传播。但是反向传播会很困难，因为同样的梯度消失的问题，后面层的输出误差（上图编号6所示）很难影响前面层（上图编号7所示的层）的计算。</p><p>对于梯度消失问题，在RNN的结构中是我们首要关心的问题，也更难解决；虽然梯度爆炸在RNN中也会出现，但对于梯度爆炸问题，因为参数会指数级的梯度，会让我们的网络参数变得很大，得到很多的Nan或者数值溢出，所以梯度爆炸是很容易发现的，我们的解决方法就是用梯度修剪，也就是观察梯度向量，如果其大于某个阈值，则对其进行缩放，保证它不会太大。</p><h3 id="1-9-GRU单元（Gated-Recurrent-Unit（GRU））"><a href="#1-9-GRU单元（Gated-Recurrent-Unit（GRU））" class="headerlink" title="1.9 GRU单元（Gated Recurrent Unit（GRU））"></a>1.9 <strong>GRU</strong>单元（Gated Recurrent Unit（<strong>GRU</strong>））</h3><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584345615940.png" alt="1584345615940"></p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584345758746.png" alt="1584345758746"></p><h4 id="简化过的GRU单元"><a href="#简化过的GRU单元" class="headerlink" title="简化过的GRU单元"></a>简化过的<strong>GRU</strong>单元</h4><p><strong>GRU</strong>单元将会有个新的变量称为$c$，代表细胞（<strong>cell</strong>），即记忆细胞（下图编号1所示）。</p><p><strong>GRU</strong>实际上输出了激活值$a^{\langle t \rangle}$，$c^{\langle t \rangle} = a^{\langle t \rangle}$（下图编号2所示）。</p><p>在每个时间步，我们将用一个候选值重写记忆细胞，即${c}^{\langle t \rangle}$的值，${c}^{\langle t \rangle} =tanh(W_{c}\left\lbrack c^{\langle t-1 \rangle},x^{\langle t \rangle} \right\rbrack +b_{c})$，所以${c}^{\langle t \rangle }$的值就是个替代值，代替表示$c^{\langle t \rangle}$的值（下图编号3所示）。</p><p>更新门$\Gamma_{u}$，这个一直在0到1之间的门值，$\Gamma_{u}= \sigma(W_{u}\left\lbrack c^{\langle t-1 \rangle},x^{\langle t \rangle} \right\rbrack +b_{u})$。</p><p>$c^{\langle t \rangle} = \Gamma_{u}<em>{c}^{\langle t \rangle} +\left( 1- \Gamma_{u} \right)</em>c^{\langle t-1 \rangle}$ ，记忆细胞的更新规则，门控值处于0-1之间，根据跟新公式能够有效地缓解梯度消失的问题。</p><p>其中， $c^{}、\widetilde c^{}、{\Gamma _u}$ 均具有相同的维度。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584346082206.png" alt="1584346082206"></p><h4 id="完整的GRU单元"><a href="#完整的GRU单元" class="headerlink" title="完整的GRU单元"></a>完整的<strong>GRU</strong>单元</h4><p>完整的GRU单元还存在另外一个门$\Gamma_{r}$，以决定每个时间步的候选值，$\Gamma_{r}$门告诉你计算出的下一个$c^{\langle t \rangle}$的候选值${c}^{\langle t \rangle}$跟$c^{\langle t-1 \rangle}$有多大的相关性，公式如下：</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584346210101.png" alt="1584346210101"></p><h3 id="1-10-长短期记忆（LSTM（long-short-term-memory）unit）"><a href="#1-10-长短期记忆（LSTM（long-short-term-memory）unit）" class="headerlink" title="1.10 长短期记忆（LSTM（long short term memory）unit）"></a>1.10 长短期记忆（<strong>LSTM</strong>（long short term memory）unit）</h3><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584346364708.png" alt="1584346364708"></p><p>LSTM中，使用了单独的更新门$Γ_u$和遗忘门$Γ_f$，以及一个输出门$Γ_o$，其主要的公式如下：</p><p>$\widetilde c^{} = \tanh (W_{c}[a^{}, x^{}] + b_{c}) \\ {\Gamma _u}=\sigma (W_{u}[a^{}, x^{}] + b_{u}) \\ {\Gamma _f}=\sigma (W_{f}[a^{}, x^{}] + b_{f}) \\{\Gamma _o}=\sigma (W_{o}[a^{}, x^{}] + b_{o}) \\ c^{} = {\Gamma _u}<em>\widetilde c^{} + {\Gamma _f}</em>c^{} \\ a^{} = {\Gamma _o}*\tanh c^{}$</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584346509959.png" alt="1584346509959"></p><p>“<strong>偷窥孔连接</strong>”就是门值不仅取决于$a^{\langle t-1 \rangle}$和$x^{\langle t \rangle}$，也取决于上一个记忆细胞的值（$c^{\langle t-1 \rangle}$）。</p><p><strong>LSTM</strong>反向传播计算：</p><p><strong>门求偏导：</strong></p><p>$d \Gamma_o^{\langle t \rangle} = da_{next}<em>\tanh(c_{next}) </em>\Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle})\tag{1}$</p><p>$d c^{\langle t \rangle} = (dc_{next}<em>\Gamma_u^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} </em>(1-\tanh(c_{next})^2) <em>\Gamma_u^{\langle t \rangle} </em>da_{next}) * (1-( c^{\langle t \rangle})^2) \tag{2}$</p><p>$d\Gamma_u^{\langle t \rangle} = (dc_{next}<em> c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) </em>c^{\langle t \rangle} <em>da_{next})</em>\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle})\tag{3}$</p><p>$d\Gamma_f^{\langle t \rangle} = (dc_{next}<em> c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) </em>c_{prev} <em>da_{next})</em>\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})\tag{4}$</p><p><strong>参数求偏导 ：</strong></p><p>$ dW_f = d\Gamma_f^{\langle t \rangle} <em>\begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{5} $<br>$ dW_u = d\Gamma_u^{\langle t \rangle} </em>\begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{6} $<br>$ dW_c = d c^{\langle t \rangle} <em>\begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{7} $<br>$ dW_o = d\Gamma_o^{\langle t \rangle} </em>\begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \tag{8}$</p><p>为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。</p><p>最后，计算隐藏状态、记忆状态和输入的偏导数：</p><p>$ da_{prev} = W_f^T<em>d\Gamma_f^{\langle t \rangle} + W_u^T </em>d\Gamma_u^{\langle t \rangle}+ W_c^T <em>d c^{\langle t \rangle} + W_o^T </em>d\Gamma_o^{\langle t \rangle} \tag{9}$</p><p>$ dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} <em>(1- \tanh(c_{next})^2)</em>\Gamma_f^{\langle t \rangle}<em>da_{next} \tag{10}$<br>$ dx^{\langle t \rangle} = W_f^T</em>d\Gamma_f^{\langle t \rangle} + W_u^T <em>d\Gamma_u^{\langle t \rangle}+ W_c^T </em>d c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{11} $</p><h3 id="1-11-双向循环神经网络（Bidirectional-RNN）"><a href="#1-11-双向循环神经网络（Bidirectional-RNN）" class="headerlink" title="1.11 双向循环神经网络（Bidirectional RNN）"></a>1.11 双向循环神经网络（Bidirectional <strong>RNN</strong>）</h3><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584346977943.png" alt="1584346977943"></p><p>双向RNN则可以解决单向RNN存在的弊端。在BRNN中，不仅有从左向右的前向连接层，还存在一个从右向左的反向连接层。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584347026009.png" alt="1584347026009"></p><p>$\hat y^{\langle t \rangle} =g(W_{g}\left\lbrack {\overrightarrow{a}}^{\langle t \rangle},{\overleftarrow{a}}^{\langle t \rangle} \right\rbrack +b_{y})$</p><h3 id="1-12-深层循环神经网络（Deep-RNNs）"><a href="#1-12-深层循环神经网络（Deep-RNNs）" class="headerlink" title="1.12 深层循环神经网络（Deep RNNs）"></a>1.12 深层循环神经网络（Deep <strong>RNN</strong>s）</h3><p>一个标准的神经网络，首先是输入$x$，然后堆叠上隐含层，所以这里应该有激活值，比如说第一层是$a^{\left\lbrack 1 \right\rbrack}$，接着堆叠上下一层，激活值$a^{\left\lbrack 2 \right\rbrack}$，可以再加一层$a^{\left\lbrack 3 \right\rbrack}$，然后得到预测值$\hat{y}$。深层的<strong>RNN</strong>网络跟这个有点像，用手画的这个网络（下图编号1所示），然后把它按时间展开就是了。</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584347128143.png" alt="1584347128143"></p><h2 id="Building-your-Recurrent-Neural-Network-Step-by-Step"><a href="#Building-your-Recurrent-Neural-Network-Step-by-Step" class="headerlink" title="Building your Recurrent Neural Network - Step by Step"></a>Building your Recurrent Neural Network - Step by Step</h2><p>Welcome to Course 5’s first assignment! In this assignment, you will implement key components of a Recurrent Neural Network in numpy.</p><p>Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have “memory”. They can read inputs $x^{\langle t \rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a unidirectional RNN to take information from the past to process later inputs. A bidirectional RNN can take context from both the past and the future.</p><p><strong>Notation</strong>:</p><ul><li><p>Superscript $[l]$ denotes an object associated with the $l^{th}$ layer.</p></li><li><p>Superscript $(i)$ denotes an object associated with the $i^{th}$ example.</p></li><li><p>Superscript $\langle t \rangle$ denotes an object at the $t^{th}$ time-step.</p></li><li><p>Subscript $i$ denotes the $i^{th}$ entry of a vector.</p></li></ul><p>Example:</p><ul><li>$a^{(2)[3]\langle4 \rangle}_5$ denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step, and 5th entry in the vector.</li></ul><h4 id="Pre-requisites"><a href="#Pre-requisites" class="headerlink" title="Pre-requisites"></a>Pre-requisites</h4><ul><li>We assume that you are already familiar with <code>numpy</code>.</li><li>To refresh your knowledge of numpy, you can review course 1 of this specialization “Neural Networks and Deep Learning”.<ul><li>Specifically, review the week 2 assignment <a href="https://www.coursera.org/learn/neural-networks-deep-learning/item/Zh0CU" target="_blank" rel="noopener">“Python Basics with numpy (optional)”</a>.</li></ul></li></ul><h4 id="Be-careful-when-modifying-the-starter-code"><a href="#Be-careful-when-modifying-the-starter-code" class="headerlink" title="Be careful when modifying the starter code"></a>Be careful when modifying the starter code</h4><ul><li>When working on graded functions, please remember to only modify the code that is between the<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### START CODE HERE</span></span><br></pre></td></tr></table></figure></li></ul><p>and<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### END CODE HERE</span></span><br></pre></td></tr></table></figure><p></p><ul><li><p>In particular, Be careful to not modify the first line of graded routines. These start with:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: routine_name</span></span><br></pre></td></tr></table></figure></li><li><p>The automatic grader (autograder) needs these to locate the function.</p></li><li>Even a change in spacing will cause issues with the autograder.</li><li>It will return ‘failed’ if these are modified or missing.”</li></ul><h3 id="Updates-for-3a"><a href="#Updates-for-3a" class="headerlink" title="Updates for 3a"></a>Updates for 3a</h3><h4 id="If-you-were-working-on-the-notebook-before-this-update…"><a href="#If-you-were-working-on-the-notebook-before-this-update…" class="headerlink" title="If you were working on the notebook before this update…"></a>If you were working on the notebook before this update…</h4><ul><li>The current notebook is version “3a”.</li><li>You can find your original work saved in the notebook with the previous version name (“v3”)</li><li>To view the file directory, go to the menu “File-&gt;Open”, and this will open a new tab that shows the file directory.</li></ul><h4 id="List-of-updates"><a href="#List-of-updates" class="headerlink" title="List of updates"></a>List of updates</h4><ul><li>“Forward propagation for the basic RNN”, added sections to clarify variable names and shapes:<ul><li>“Dimensions of $x^{\langle t \rangle}$”</li><li>“Hidden State $a$”,</li><li>“Dimensions of hidden state $a^{\langle t \rangle}$”</li><li>“Dimensions of prediction $y^{\langle t \rangle}$”</li></ul></li><li><code>rnn_cell_forward</code>:<ul><li>Added additional hints.</li><li>Updated figure 2.</li></ul></li><li><code>rnn_forward</code><ul><li>Set <code>xt</code> in a separate line of code to clarify what code is expected; added additional hints.</li><li>Clarifies instructions to specify dimensions (2D or 3D), and clarifies variable names.</li><li>Additional Hints</li><li>Clarifies when the basic RNN works well.</li><li>Updated figure 3.</li></ul></li><li>“About the gates” replaced with “overview of gates and states”:<ul><li>Updated to include conceptual description of each gate’s purpose, and an explanation of each equation.</li><li>Added sections about the cell state, hidden state, and prediction.</li><li>Lists variable names that are used in the code, and notes when they differ from the variables used in the equations.</li><li>Lists shapes of the variables.</li><li>Updated figure 4.</li></ul></li><li><code>lstm_forward</code><ul><li>Added instructions, noting the shapes of the variables.</li><li>Added hints about <code>c</code> and <code>c_next</code> to help students avoid copy-by-reference mistakes.</li><li>Set <code>xt</code> in a separate line to make this step explicit.</li></ul></li><li><code>dimension</code><ul><li>clarified use of $x^{(i)\langle t \rangle}$ in dimension description</li></ul></li><li><code>Backward description</code><ul><li>Updated figure 6 and 7</li></ul></li><li><code>rnn_cell_backward</code><ul><li>changed ‘dtanh’ to ‘dz’ to avoid confusion with the output of the tanh</li><li>fixed naming of ‘b’ in test section, changed to ‘ba’, updated results</li></ul></li><li><code>rnn_backward</code><ul><li>modified instructions in comments to sum da and gradients from previous timesteps</li></ul></li><li><code>lstm_cell_backward</code><ul><li>updated equations and description to fix errors.</li><li>Added Figure 8</li><li>modified equations to use the term dlower_case_gamma vs dGamma - the previous naming confused the location of the gate derivative.</li><li>removed redundant lines for equations 7-10, changed equation numbers to match new equation numbers</li></ul></li><li><code>lstm_backward</code><ul><li>removed comment listing dc as argument.</li><li>added da_prevt and dc_prevt to for loop and recalculated results</li><li>in test, added “Wy” and “by” zero fill as it is required for lstm_forward.</li></ul></li><li>Renamed global variables so that they do not conflict with local variables within the function.</li><li>Spelling, grammar and wording corrections.</li><li>For unit tests, updated print statements and “expected output” for easier comparisons.</li><li>Many thanks to mentors and students for suggested improvements and fixes in the assignments for course 5!</li></ul><p>Let’s first import all the packages that you will need during this assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> rnn_utils <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h3 id="1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="1 - Forward propagation for the basic Recurrent Neural Network"></a>1 - Forward propagation for the basic Recurrent Neural Network</h3><p>Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$.</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584427092952.png" alt="1584427092952"></p><h4 id="Dimensions-of-input-x"><a href="#Dimensions-of-input-x" class="headerlink" title="Dimensions of input $x$"></a>Dimensions of input $x$</h4><h5 id="Input-with-n-x-number-of-units"><a href="#Input-with-n-x-number-of-units" class="headerlink" title="Input with $n_x$ number of units"></a>Input with $n_x$ number of units</h5><ul><li>For a single timestep of a single input example, $x^{(i) \langle t \rangle }$ is a one-dimensional input vector.</li><li>Using language as an example, a language with a 5000 word vocabulary could be one-hot encoded into a vector that has 5000 units. So $x^{(i)\langle t \rangle}$ would have the shape (5000,).</li><li>We’ll use the notation $n_x$ to denote the number of units in a single timestep of a single training example.</li></ul><h5 id="Time-steps-of-size-T-x"><a href="#Time-steps-of-size-T-x" class="headerlink" title="Time steps of size $T_{x}$"></a>Time steps of size $T_{x}$</h5><ul><li>A recurrent neural network has multiple time steps, which we’ll index with $t$.</li><li>In the lessons, we saw a single training example $x^{(i)}$ consist of multiple time steps $T_x$. For example, if there are 10 time steps, $T_{x} = 10$</li></ul><h5 id="Batches-of-size-m"><a href="#Batches-of-size-m" class="headerlink" title="Batches of size $m$"></a>Batches of size $m$</h5><ul><li>Let’s say we have mini-batches, each with 20 training examples.</li><li>To benefit from vectorization, we’ll stack 20 columns of $x^{(i)}$ examples.</li><li>For example, this tensor has the shape (5000,20,10).</li><li>We’ll use $m$ to denote the number of training examples.</li><li>So the shape of a mini-batch is $(n_x,m,T_x)$</li></ul><h5 id="3D-Tensor-of-shape-n-x-m-T-x"><a href="#3D-Tensor-of-shape-n-x-m-T-x" class="headerlink" title="3D Tensor of shape $(n_{x},m,T_{x})$"></a>3D Tensor of shape $(n_{x},m,T_{x})$</h5><ul><li>The 3-dimensional tensor $x$ of shape $(n_x,m,T_x)$ represents the input $x$ that is fed into the RNN.</li></ul><h5 id="Taking-a-2D-slice-for-each-time-step-x-langle-t-rangle"><a href="#Taking-a-2D-slice-for-each-time-step-x-langle-t-rangle" class="headerlink" title="Taking a 2D slice for each time step: $x^{\langle t \rangle}$"></a>Taking a 2D slice for each time step: $x^{\langle t \rangle}$</h5><ul><li>At each time step, we’ll use a mini-batches of training examples (not just a single example).</li><li>So, for each time step $t$, we’ll use a 2D slice of shape $(n_x,m)$.</li><li>We’re referring to this 2D slice as $x^{\langle t \rangle}$. The variable name in the code is <code>xt</code>.</li></ul><h4 id="Definition-of-hidden-state-a"><a href="#Definition-of-hidden-state-a" class="headerlink" title="Definition of hidden state $a$"></a>Definition of hidden state $a$</h4><ul><li>The activation $a^{\langle t \rangle}$ that is passed to the RNN from one time step to another is called a “hidden state.”</li></ul><h4 id="Dimensions-of-hidden-state-a"><a href="#Dimensions-of-hidden-state-a" class="headerlink" title="Dimensions of hidden state $a$"></a>Dimensions of hidden state $a$</h4><ul><li>Similar to the input tensor $x$, the hidden state for a single training example is a vector of length $n_{a}$.</li><li>If we include a mini-batch of $m$ training examples, the shape of a mini-batch is $(n_{a},m)$.</li><li>When we include the time step dimension, the shape of the hidden state is $(n_{a}, m, T_x)$</li><li>We will loop through the time steps with index $t$, and work with a 2D slice of the 3D tensor.</li><li>We’ll refer to this 2D slice as $a^{\langle t \rangle}$.</li><li>In the code, the variable names we use are either <code>a_prev</code> or <code>a_next</code>, depending on the function that’s being implemented.</li><li>The shape of this 2D slice is $(n_{a}, m)$</li></ul><h4 id="Dimensions-of-prediction-hat-y"><a href="#Dimensions-of-prediction-hat-y" class="headerlink" title="Dimensions of prediction $\hat{y}$"></a>Dimensions of prediction $\hat{y}$</h4><ul><li>Similar to the inputs and hidden states, $\hat{y}$ is a 3D tensor of shape $(n_{y}, m, T_{y})$.<ul><li>$n_{y}$: number of units in the vector representing the prediction.</li><li>$m$: number of examples in a mini-batch.</li><li>$T_{y}$: number of time steps in the prediction.</li></ul></li><li>For a single time step $t$, a 2D slice $\hat{y}^{\langle t \rangle}$ has shape $(n_{y}, m)$.</li><li>In the code, the variable names are:<ul><li><code>y_pred</code>: $\hat{y}$</li><li><code>yt_pred</code>: $\hat{y}^{\langle t \rangle}$</li></ul></li></ul><p>Here’s how you can implement an RNN:</p><p><strong>Steps</strong>:</p><ol><li>Implement the calculations needed for one time-step of the RNN.</li><li>Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time.</li></ol><h4 id="1-1-RNN-cell"><a href="#1-1-RNN-cell" class="headerlink" title="1.1 - RNN cell"></a>1.1 - RNN cell</h4><p>A recurrent neural network can be seen as the repeated use of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584427787391.png" alt="1584427787391"></p><h5 id="rnn-cell-versus-rnn-cell-forward"><a href="#rnn-cell-versus-rnn-cell-forward" class="headerlink" title="rnn cell versus rnn_cell_forward"></a>rnn cell versus rnn_cell_forward</h5><ul><li>Note that an RNN cell outputs the hidden state $a^{\langle t \rangle}$.<ul><li>The rnn cell is shown in the figure as the inner box which has solid lines.</li></ul></li><li>The function that we will implement, <code>rnn_cell_forward</code>, also calculates the prediction $\hat{y}^{\langle t \rangle}$<ul><li>The rnn_cell_forward is shown in the figure as the outer box that has dashed lines.</li></ul></li></ul><p><strong>Exercise</strong>: Implement the RNN-cell described in Figure (2).</p><p><strong>Instructions</strong>:</p><ol><li>Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$.</li><li>Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$. We provided the function <code>softmax</code>.</li><li>Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in a <code>cache</code>.</li><li>Return $a^{\langle t \rangle}$ , $\hat{y}^{\langle t \rangle}$ and <code>cache</code></li></ol><h5 id="Additional-Hints"><a href="#Additional-Hints" class="headerlink" title="Additional Hints"></a>Additional Hints</h5><ul><li><a href="https://www.google.com/search?q=numpy+tanh&amp;rlz=1C5CHFA_enUS854US855&amp;oq=numpy+tanh&amp;aqs=chrome..69i57j0l5.1340j0j7&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">numpy.tanh</a></li><li>We’ve created a <code>softmax</code> function that you can use. It is located in the file ‘rnn_utils.py’ and has been imported.</li><li>For matrix multiplication, use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" target="_blank" rel="noopener">numpy.dot</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya, a_next) + by)    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Waa'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wax'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wya'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'ba'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)</span><br><span class="line">print(<span class="string">"a_next[4] = \n"</span>, a_next_tmp[<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"a_next.shape = \n"</span>, a_next_tmp.shape)</span><br><span class="line">print(<span class="string">"yt_pred[1] =\n"</span>, yt_pred_tmp[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"yt_pred.shape = \n"</span>, yt_pred_tmp.shape)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584429763258.png" alt="1584429763258"></p><h4 id="1-2-RNN-forward-pass"><a href="#1-2-RNN-forward-pass" class="headerlink" title="1.2 - RNN forward pass"></a>1.2 - RNN forward pass</h4><ul><li>A recurrent neural network (RNN) is a repetition of the RNN cell that you’ve just built.<ul><li>If your input sequence of data is 10 time steps long, then you will re-use the RNN cell 10 times.</li></ul></li><li>Each cell takes two inputs at each time step:<ul><li>$a^{\langle t-1 \rangle}$: The hidden state from the previous cell.</li><li>$x^{\langle t \rangle}$: The current time-step’s input data.</li></ul></li><li>It has two outputs at each time step:<ul><li>A hidden state ($a^{\langle t \rangle}$)</li><li>A prediction ($y^{\langle t \rangle}$)</li></ul></li><li>The weights and biases $(W_{aa}, b_{a}, W_{ax}, b_{x})$ are re-used each time step.<ul><li>They are maintained between calls to rnn_cell_forward in the ‘parameters’ dictionary.</li></ul></li></ul><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584429883421.png" alt="1584429883421"></p><p><strong>Exercise</strong>: Code the forward propagation of the RNN described in Figure (3).</p><p><strong>Instructions</strong>:</p><ul><li>Create a 3D array of zeros, $a$ of shape $(n_{a}, m, T_{x})$ that will store all the hidden states computed by the RNN.</li><li>Create a 3D array of zeros, $\hat{y}$, of shape $(n_{y}, m, T_{x})$ that will store the predictions.<ul><li>Note that in this case, $T_{y} = T_{x}$ (the prediction and input have the same number of time steps).</li></ul></li><li>Initialize the 2D hidden state <code>a_next</code> by setting it equal to the initial hidden state, $a_{0}$.</li><li>At each time step $t$:<ul><li>Get $x^{\langle t \rangle}$, which is a 2D slice of $x$ for a single time step $t$.<ul><li>$x^{\langle t \rangle}$ has shape $(n_{x}, m)$</li><li>$x$ has shape $(n_{x}, m, T_{x})$</li></ul></li><li>Update the 2D hidden state $a^{\langle t \rangle}$ (variable name <code>a_next</code>), the prediction $\hat{y}^{\langle t \rangle}$ and the cache by running <code>rnn_cell_forward</code>.<ul><li>$a^{\langle t \rangle}$ has shape $(n_{a}, m)$</li></ul></li><li>Store the 2D hidden state in the 3D tensor $a$, at the $t^{th}$ position.<ul><li>$a$ has shape $(n_{a}, m, T_{x})$</li></ul></li><li>Store the 2D $\hat{y}^{\langle t \rangle}$ prediction (variable name <code>yt_pred</code>) in the 3D tensor $\hat{y}_{pred}$ at the $t^{th}$ position.<ul><li>$\hat{y}^{\langle t \rangle}$ has shape $(n_{y}, m)$</li><li>$\hat{y}$ has shape $(n_{y}, m, T_x)$</li></ul></li><li>Append the cache to the list of caches.</li></ul></li><li>Return the 3D tensor $a$ and $\hat{y}$, as well as the list of caches.</li></ul><h5 id="Additional-Hints-1"><a href="#Additional-Hints-1" class="headerlink" title="Additional Hints"></a>Additional Hints</h5><ul><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html" target="_blank" rel="noopener">np.zeros</a></li><li>If you have a 3 dimensional numpy array and are indexing by its third dimension, you can use array slicing like this: <code>var_name[:,:,i]</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters["Wya"]</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a" and "y_pred" with zeros (≈2 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next (≈1 line)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps of the input 'x' (1 line)</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, compute the prediction, get the cache (≈2 lines)</span></span><br><span class="line">        xt = x[:, :, t]</span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(xt, a_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Append "cache" to "caches" (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line">a0_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Waa'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wax'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wya'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'ba'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)</span><br><span class="line">print(<span class="string">"a[4][1] = \n"</span>, a_tmp[<span class="number">4</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"a.shape = \n"</span>, a_tmp.shape)</span><br><span class="line">print(<span class="string">"y_pred[1][3] =\n"</span>, y_pred_tmp[<span class="number">1</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"y_pred.shape = \n"</span>, y_pred_tmp.shape)</span><br><span class="line">print(<span class="string">"caches[1][1][3] =\n"</span>, caches_tmp[<span class="number">1</span>][<span class="number">1</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"len(caches) = \n"</span>, len(caches_tmp))</span><br></pre></td></tr></table></figure><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584430666170.png" alt="1584430666170"></p><p>Congratulations! You’ve successfully built the forward propagation of a recurrent neural network from scratch.</p><h5 id="Situations-when-this-RNN-will-perform-better"><a href="#Situations-when-this-RNN-will-perform-better" class="headerlink" title="Situations when this RNN will perform better:"></a>Situations when this RNN will perform better:</h5><ul><li>This will work well enough for some applications, but it suffers from the vanishing gradient problems.</li><li>The RNN works best when each output $\hat{y}^{\langle t \rangle}$ can be estimated using “local” context.</li><li>“Local” context refers to information that is close to the prediction’s time step $t$.</li><li>More formally, local context refers to inputs $x^{\langle t’ \rangle}$ and predictions $\hat{y}^{\langle t \rangle}$ where $t’$ is close to $t$.</li></ul><p>In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps.</p><h3 id="2-Long-Short-Term-Memory-LSTM-network"><a href="#2-Long-Short-Term-Memory-LSTM-network" class="headerlink" title="2 - Long Short-Term Memory (LSTM) network"></a>2 - Long Short-Term Memory (LSTM) network</h3><p>The following figure shows the operations of an LSTM-cell.</p><p>Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a “for-loop” to have it process an input with $T_x$ time-steps.</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584432593143.png" alt="1584432593143"></p><h4 id="Overview-of-gates-and-states"><a href="#Overview-of-gates-and-states" class="headerlink" title="Overview of gates and states"></a>Overview of gates and states</h4><h5 id="Forget-gate-mathbf-Gamma-f"><a href="#Forget-gate-mathbf-Gamma-f" class="headerlink" title="- Forget gate $\mathbf{\Gamma}_{f}$"></a>- Forget gate $\mathbf{\Gamma}_{f}$</h5><ul><li>Let’s assume we are reading words in a piece of text, and plan to use an LSTM to keep track of grammatical structures, such as whether the subject is singular (“puppy”) or plural (“puppies”).</li><li>If the subject changes its state (from a singular word to a plural word), the memory of the previous state becomes outdated, so we “forget” that outdated state.</li><li>The “forget gate” is a tensor containing values that are between 0 and 1.<ul><li>If a unit in the forget gate has a value close to 0, the LSTM will “forget” the stored state in the corresponding unit of the previous cell state.</li><li>If a unit in the forget gate has a value close to 1, the LSTM will mostly remember the corresponding value in the stored state.</li></ul></li></ul><h6 id="Equation"><a href="#Equation" class="headerlink" title="Equation"></a>Equation</h6><script type="math/tex;mode=display">\mathbf{\Gamma}_f^{\langle t \rangle} = \sigma(\mathbf{W}_f[\mathbf{a}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_f)\tag{1}</script><h6 id="Explanation-of-the-equation"><a href="#Explanation-of-the-equation" class="headerlink" title="Explanation of the equation:"></a>Explanation of the equation:</h6><ul><li>$\mathbf{W_{f}}$ contains weights that govern the forget gate’s behavior.</li><li>The previous time step’s hidden state $[a^{\langle t-1 \rangle}$ and current time step’s input $x^{\langle t \rangle}]$ are concatenated together and multiplied by $\mathbf{W_{f}}$.</li><li>A sigmoid function is used to make each of the gate tensor’s values $\mathbf{\Gamma}_f^{\langle t \rangle}$ range from 0 to 1.</li><li>The forget gate $\mathbf{\Gamma}_f^{\langle t \rangle}$ has the same dimensions as the previous cell state $c^{\langle t-1 \rangle}$.</li><li>This means that the two can be multiplied together, element-wise.</li><li>Multiplying the tensors $\mathbf{\Gamma}_f^{\langle t \rangle} * \mathbf{c}^{\langle t-1 \rangle}$ is like applying a mask over the previous cell state.</li><li>If a single value in $\mathbf{\Gamma}_f^{\langle t \rangle}$ is 0 or close to 0, then the product is close to 0.<ul><li>This keeps the information stored in the corresponding unit in $\mathbf{c}^{\langle t-1 \rangle}$ from being remembered for the next time step.</li></ul></li><li>Similarly, if one value is close to 1, the product is close to the original value in the previous cell state.<ul><li>The LSTM will keep the information from the corresponding unit of $\mathbf{c}^{\langle t-1 \rangle}$, to be used in the next time step.</li></ul></li></ul><h6 id="Variable-names-in-the-code"><a href="#Variable-names-in-the-code" class="headerlink" title="Variable names in the code"></a>Variable names in the code</h6><p>The variable names in the code are similar to the equations, with slight differences.</p><ul><li><code>Wf</code>: forget gate weight $\mathbf{W}_{f}$</li><li><code>Wb</code>: forget gate bias $\mathbf{W}_{b}$</li><li><code>ft</code>: forget gate $\Gamma_f^{\langle t \rangle}$</li></ul><h5 id="Candidate-value-mathbf-c-langle-t-rangle"><a href="#Candidate-value-mathbf-c-langle-t-rangle" class="headerlink" title="Candidate value ${\mathbf{c}}^{\langle t \rangle}$"></a>Candidate value ${\mathbf{c}}^{\langle t \rangle}$</h5><ul><li>The candidate value is a tensor containing information from the current time step that <strong>may</strong> be stored in the current cell state $\mathbf{c}^{\langle t \rangle}$.</li><li>Which parts of the candidate value get passed on depends on the update gate.</li><li>The candidate value is a tensor containing values that range from -1 to 1.</li><li>The tilde “~” is used to differentiate the candidate ${\mathbf{c}}^{\langle t \rangle}$ from the cell state $\mathbf{c}^{\langle t \rangle}$.</li></ul><h6 id="Equation-1"><a href="#Equation-1" class="headerlink" title="Equation"></a>Equation</h6><script type="math/tex;mode=display">\mathbf{c}^{\langle t \rangle} = \tanh\left( \mathbf{W}_{c} [\mathbf{a}^{\langle t - 1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_{c} \right) \tag{3}</script><h6 id="Explanation-of-the-equation-1"><a href="#Explanation-of-the-equation-1" class="headerlink" title="Explanation of the equation"></a>Explanation of the equation</h6><ul><li>The ‘tanh’ function produces values between -1 and +1.</li></ul><h6 id="Variable-names-in-the-code-1"><a href="#Variable-names-in-the-code-1" class="headerlink" title="Variable names in the code"></a>Variable names in the code</h6><ul><li><code>cct</code>: candidate value $\mathbf{c}^{\langle t \rangle}$</li></ul><h5 id="Update-gate-mathbf-Gamma-i"><a href="#Update-gate-mathbf-Gamma-i" class="headerlink" title="- Update gate $\mathbf{\Gamma}_{i}$"></a>- Update gate $\mathbf{\Gamma}_{i}$</h5><ul><li>We use the update gate to decide what aspects of the candidate ${\mathbf{c}}^{\langle t \rangle}$ to add to the cell state $c^{\langle t \rangle}$.</li><li>The update gate decides what parts of a “candidate” tensor ${\mathbf{c}}^{\langle t \rangle}$ are passed onto the cell state $\mathbf{c}^{\langle t \rangle}$.</li><li>The update gate is a tensor containing values between 0 and 1.<ul><li>When a unit in the update gate is close to 1, it allows the value of the candidate ${\mathbf{c}}^{\langle t \rangle}$ to be passed onto the hidden state $\mathbf{c}^{\langle t \rangle}$</li><li>When a unit in the update gate is close to 0, it prevents the corresponding value in the candidate from being passed onto the hidden state.</li></ul></li><li>Notice that we use the subscript “i” and not “u”, to follow the convention used in the literature.</li></ul><h6 id="Equation-2"><a href="#Equation-2" class="headerlink" title="Equation"></a>Equation</h6><script type="math/tex;mode=display">\mathbf{\Gamma}_i^{\langle t \rangle} = \sigma(\mathbf{W}_i[a^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_i)\tag{2}</script><h6 id="Explanation-of-the-equation-2"><a href="#Explanation-of-the-equation-2" class="headerlink" title="Explanation of the equation"></a>Explanation of the equation</h6><ul><li>Similar to the forget gate, here $\mathbf{\Gamma}_i^{\langle t \rangle}$, the sigmoid produces values between 0 and 1.</li><li>The update gate is multiplied element-wise with the candidate, and this product ($\mathbf{\Gamma}_{i}^{\langle t \rangle} * {c}^{\langle t \rangle}$) is used in determining the cell state $\mathbf{c}^{\langle t \rangle}$.</li></ul><h6 id="Variable-names-in-code-Please-note-that-they’re-different-than-the-equations"><a href="#Variable-names-in-code-Please-note-that-they’re-different-than-the-equations" class="headerlink" title="Variable names in code (Please note that they’re different than the equations)"></a>Variable names in code (Please note that they’re different than the equations)</h6><p>In the code, we’ll use the variable names found in the academic literature. These variables don’t use “u” to denote “update”.</p><ul><li><code>Wi</code> is the update gate weight $\mathbf{W}_i$ (not “Wu”)</li><li><code>bi</code> is the update gate bias $\mathbf{b}_i$ (not “bu”)</li><li><code>it</code> is the forget gate $\mathbf{\Gamma}_i^{\langle t \rangle}$ (not “ut”)</li></ul><h5 id="Cell-state-mathbf-c-langle-t-rangle"><a href="#Cell-state-mathbf-c-langle-t-rangle" class="headerlink" title="- Cell state $\mathbf{c}^{\langle t \rangle}$"></a>- Cell state $\mathbf{c}^{\langle t \rangle}$</h5><ul><li>The cell state is the “memory” that gets passed onto future time steps.</li><li>The new cell state $\mathbf{c}^{\langle t \rangle}$ is a combination of the previous cell state and the candidate value.</li></ul><h6 id="Equation-3"><a href="#Equation-3" class="headerlink" title="Equation"></a>Equation</h6><script type="math/tex;mode=display">\mathbf{c}^{\langle t \rangle} = \mathbf{\Gamma}_f^{\langle t \rangle}* \mathbf{c}^{\langle t-1 \rangle} + \mathbf{\Gamma}_{i}^{\langle t \rangle} *\mathbf{c}^{\langle t \rangle} \tag{4}</script><h6 id="Explanation-of-equation"><a href="#Explanation-of-equation" class="headerlink" title="Explanation of equation"></a>Explanation of equation</h6><ul><li>The previous cell state $\mathbf{c}^{\langle t-1 \rangle}$ is adjusted (weighted) by the forget gate $\mathbf{\Gamma}_{f}^{\langle t \rangle}$</li><li>and the candidate value ${\mathbf{c}}^{\langle t \rangle}$, adjusted (weighted) by the update gate $\mathbf{\Gamma}_{i}^{\langle t \rangle}$</li></ul><h6 id="Variable-names-and-shapes-in-the-code"><a href="#Variable-names-and-shapes-in-the-code" class="headerlink" title="Variable names and shapes in the code"></a>Variable names and shapes in the code</h6><ul><li><code>c</code>: cell state, including all time steps, $\mathbf{c}$ shape $(n_{a}, m, T)$</li><li><code>c_next</code>: new (next) cell state, $\mathbf{c}^{\langle t \rangle}$ shape $(n_{a}, m)$</li><li><code>c_prev</code>: previous cell state, $\mathbf{c}^{\langle t-1 \rangle}$, shape $(n_{a}, m)$</li></ul><h5 id="Output-gate-mathbf-Gamma-o"><a href="#Output-gate-mathbf-Gamma-o" class="headerlink" title="- Output gate $\mathbf{\Gamma}_{o}$"></a>- Output gate $\mathbf{\Gamma}_{o}$</h5><ul><li>The output gate decides what gets sent as the prediction (output) of the time step.</li><li>The output gate is like the other gates. It contains values that range from 0 to 1.</li></ul><h6 id="Equation-4"><a href="#Equation-4" class="headerlink" title="Equation"></a>Equation</h6><script type="math/tex;mode=display">\mathbf{\Gamma}_o^{\langle t \rangle}=  \sigma(\mathbf{W}_o[\mathbf{a}^{\langle t-1 \rangle}, \mathbf{x}^{\langle t \rangle}] + \mathbf{b}_{o})\tag{5}</script><h6 id="Explanation-of-the-equation-3"><a href="#Explanation-of-the-equation-3" class="headerlink" title="Explanation of the equation"></a>Explanation of the equation</h6><ul><li>The output gate is determined by the previous hidden state $\mathbf{a}^{\langle t-1 \rangle}$ and the current input $\mathbf{x}^{\langle t \rangle}$</li><li>The sigmoid makes the gate range from 0 to 1.</li></ul><h6 id="Variable-names-in-the-code-2"><a href="#Variable-names-in-the-code-2" class="headerlink" title="Variable names in the code"></a>Variable names in the code</h6><ul><li><code>Wo</code>: output gate weight, $\mathbf{W_o}$</li><li><code>bo</code>: output gate bias, $\mathbf{b_o}$</li><li><code>ot</code>: output gate, $\mathbf{\Gamma}_{o}^{\langle t \rangle}$</li></ul><h5 id="Hidden-state-mathbf-a-langle-t-rangle"><a href="#Hidden-state-mathbf-a-langle-t-rangle" class="headerlink" title="- Hidden state $\mathbf{a}^{\langle t \rangle}$"></a>- Hidden state $\mathbf{a}^{\langle t \rangle}$</h5><ul><li>The hidden state gets passed to the LSTM cell’s next time step.</li><li>It is used to determine the three gates ($\mathbf{\Gamma}_{f}, \mathbf{\Gamma}_{u}, \mathbf{\Gamma}_{o}$) of the next time step.</li><li>The hidden state is also used for the prediction $y^{\langle t \rangle}$.</li></ul><h6 id="Equation-5"><a href="#Equation-5" class="headerlink" title="Equation"></a>Equation</h6><script type="math/tex;mode=display">\mathbf{a}^{\langle t \rangle} = \mathbf{\Gamma}_o^{\langle t \rangle} * \tanh(\mathbf{c}^{\langle t \rangle})\tag{6}</script><h6 id="Explanation-of-equation-1"><a href="#Explanation-of-equation-1" class="headerlink" title="Explanation of equation"></a>Explanation of equation</h6><ul><li>The hidden state $\mathbf{a}^{\langle t \rangle}$ is determined by the cell state $\mathbf{c}^{\langle t \rangle}$ in combination with the output gate $\mathbf{\Gamma}_{o}$.</li><li>The cell state state is passed through the “tanh” function to rescale values between -1 and +1.</li><li>The output gate acts like a “mask” that either preserves the values of $\tanh(\mathbf{c}^{\langle t \rangle})$ or keeps those values from being included in the hidden state $\mathbf{a}^{\langle t \rangle}$</li></ul><h6 id="Variable-names-and-shapes-in-the-code-1"><a href="#Variable-names-and-shapes-in-the-code-1" class="headerlink" title="Variable names  and shapes in the code"></a>Variable names and shapes in the code</h6><ul><li><code>a</code>: hidden state, including time steps. $\mathbf{a}$ has shape $(n_{a}, m, T_{x})$</li><li><code>a_prev</code>: hidden state from previous time step. $\mathbf{a}^{\langle t-1 \rangle}$ has shape $(n_{a}, m)$</li><li><code>a_next</code>: hidden state for next time step. $\mathbf{a}^{\langle t \rangle}$ has shape $(n_{a}, m)$</li></ul><h5 id="Prediction-mathbf-y-langle-t-rangle-pred"><a href="#Prediction-mathbf-y-langle-t-rangle-pred" class="headerlink" title="- Prediction $\mathbf{y}^{\langle t \rangle}_{pred}$"></a>- Prediction $\mathbf{y}^{\langle t \rangle}_{pred}$</h5><ul><li>The prediction in this use case is a classification, so we’ll use a softmax.</li></ul><p>The equation is:</p><script type="math/tex;mode=display">\mathbf{y}^{\langle t \rangle}_{pred} = \textrm{softmax}(\mathbf{W}_{y} \mathbf{a}^{\langle t \rangle} + \mathbf{b}_{y})</script><h6 id="Variable-names-and-shapes-in-the-code-2"><a href="#Variable-names-and-shapes-in-the-code-2" class="headerlink" title="Variable names and shapes in the code"></a>Variable names and shapes in the code</h6><ul><li><code>y_pred</code>: prediction, including all time steps. $\mathbf{y}_{pred}$ has shape $(n_{y}, m, T_{x})$. Note that $(T_{y} = T_{x})$ for this example.</li><li><code>yt_pred</code>: prediction for the current time step $t$. $\mathbf{y}^{\langle t \rangle}_{pred}$ has shape $(n_{y}, m)$</li></ul><h4 id="2-1-LSTM-cell"><a href="#2-1-LSTM-cell" class="headerlink" title="2.1 - LSTM cell"></a>2.1 - LSTM cell</h4><p><strong>Exercise</strong>: Implement the LSTM cell described in the Figure (4).</p><p><strong>Instructions</strong>:</p><ol><li>Concatenate the hidden state $a^{\langle t-1 \rangle}$ and input $x^{\langle t \rangle}$ into a single matrix:</li></ol><script type="math/tex;mode=display">concat = \begin{bmatrix} a^{\langle t-1 \rangle} \\ x^{\langle t \rangle} \end{bmatrix}</script><ol><li>Compute all the formulas 1 through 6 for the gates, hidden state, and cell state.</li><li>Compute the prediction $y^{\langle t \rangle}$.</li></ol><h4 id="Additional-Hints-2"><a href="#Additional-Hints-2" class="headerlink" title="Additional Hints"></a>Additional Hints</h4><ul><li>You can use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html" target="_blank" rel="noopener">numpy.concatenate</a>. Check which value to use for the <code>axis</code> parameter.</li><li>The functions <code>sigmoid()</code> and <code>softmax</code> are imported from <code>rnn_utils.py</code>.</li><li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html" target="_blank" rel="noopener">numpy.tanh</a></li><li>Use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" target="_blank" rel="noopener">np.dot</a> for matrix multiplication.</li><li>Notice that the variable names <code>Wi</code>, <code>bi</code> refer to the weights and biases of the <strong>update</strong> gate. There are no variables named “Wu” or “bu” in this function.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span></span><br><span class="line"><span class="string">          c stands for the cell state (memory)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>] <span class="comment"># forget gate weight</span></span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>] <span class="comment"># update gate weight (notice the variable name)</span></span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>] <span class="comment"># (notice the variable name)</span></span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>] <span class="comment"># candidate value weight</span></span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>] <span class="comment"># output gate weight</span></span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>] <span class="comment"># prediction weight</span></span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt (≈1 line)</span></span><br><span class="line">    concat = np.concatenate((a_prev, xt), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute values for ft (forget gate), it (update gate),</span></span><br><span class="line">    <span class="comment"># cct (candidate value), c_next (cell state), </span></span><br><span class="line">    <span class="comment"># ot (output gate), a_next (hidden state) (≈6 lines)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)        <span class="comment"># forget gate</span></span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)        <span class="comment"># update gate</span></span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)       <span class="comment"># candidate value</span></span><br><span class="line">    c_next = ft*c_prev + it*cct    <span class="comment"># cell state</span></span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)        <span class="comment"># output gate</span></span><br><span class="line">    a_next = ot*np.tanh(c_next)    <span class="comment"># hidden state</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell (≈1 line)</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wy, a_next) + by)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">c_prev_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Wf'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bf'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wi'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bi'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wo'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bo'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wc'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bc'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wy'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)</span><br><span class="line">print(<span class="string">"a_next[4] = \n"</span>, a_next_tmp[<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"a_next.shape = "</span>, c_next_tmp.shape)</span><br><span class="line">print(<span class="string">"c_next[2] = \n"</span>, c_next_tmp[<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"c_next.shape = "</span>, c_next_tmp.shape)</span><br><span class="line">print(<span class="string">"yt[1] ="</span>, yt_tmp[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"yt.shape = "</span>, yt_tmp.shape)</span><br><span class="line">print(<span class="string">"cache[1][3] =\n"</span>, cache_tmp[<span class="number">1</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"len(cache) = "</span>, len(cache_tmp))</span><br></pre></td></tr></table></figure><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584434881145.png" alt="1584434881145"></p><h4 id="2-2-Forward-pass-for-LSTM"><a href="#2-2-Forward-pass-for-LSTM" class="headerlink" title="2.2 - Forward pass for LSTM"></a>2.2 - Forward pass for LSTM</h4><p>Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs.</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584435049643.png" alt="1584435049643"></p><p><strong>Exercise:</strong> Implement <code>lstm_forward()</code> to run an LSTM over $T_x$ time-steps.</p><p><strong>Instructions</strong></p><ul><li>Get the dimensions $n_x, n_a, n_y, m, T_x$ from the shape of the variables: <code>x</code> and <code>parameters</code>.</li><li>Initialize the 3D tensors $a$, $c$ and $y$.<ul><li>$a$: hidden state, shape $(n_{a}, m, T_{x})$</li><li>$c$: cell state, shape $(n_{a}, m, T_{x})$</li><li>$y$: prediction, shape $(n_{y}, m, T_{x})$ (Note that $T_{y} = T_{x}$ in this example).</li><li><strong>Note</strong> Setting one variable equal to the other is a “copy by reference”. In other words, don’t do <code>c = a</code>, otherwise both these variables point to the same underlying variable.</li></ul></li><li>Initialize the 2D tensor $a^{\langle t \rangle}$<ul><li>$a^{\langle t \rangle}$ stores the hidden state for time step $t$. The variable name is <code>a_next</code>.</li><li>$a^{\langle 0 \rangle}$, the initial hidden state at time step 0, is passed in when calling the function. The variable name is <code>a0</code>.</li><li>$a^{\langle t \rangle}$ and $a^{\langle 0 \rangle}$ represent a single time step, so they both have the shape $(n_{a}, m)$</li><li>Initialize $a^{\langle t \rangle}$ by setting it to the initial hidden state ($a^{\langle 0 \rangle}$) that is passed into the function.</li></ul></li><li>Initialize $c^{\langle t \rangle}$ with zeros.<ul><li>The variable name is <code>c_next</code>.</li><li>$c^{\langle t \rangle}$ represents a single time step, so its shape is $(n_{a}, m)$</li><li><strong>Note</strong>: create <code>c_next</code> as its own variable with its own location in memory. Do not initialize it as a slice of the 3D tensor $c$. In other words, <strong>don’t</strong> do <code>c_next = c[:,:,0]</code>.</li></ul></li><li>For each time step, do the following:<ul><li>From the 3D tensor $x$, get a 2D slice $x^{\langle t \rangle}$ at time step $t$.</li><li>Call the <code>lstm_cell_forward</code> function that you defined previously, to get the hidden state, cell state, prediction, and cache.</li><li>Store the hidden state, cell state and prediction (the 2D tensors) inside the 3D tensors.</li><li>Also append the cache to the list of caches.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    Wy = parameters[<span class="string">'Wy'</span>] <span class="comment"># saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">'Wy'</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros (≈3 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = np.zeros((n_a, m, T_x))</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next (≈2 lines)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Get the 2D slice 'xt' from the 3D input 'x' at time step 't'</span></span><br><span class="line">        xt = x[:, :, t]</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the next cell state (≈1 line)</span></span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        <span class="comment"># Append the cache into caches (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584435414319.png" alt="1584435414319"></p><p>Congratulations! You have now implemented the forward passes for the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance.</p><p>The rest of this notebook is optional, and will not be graded.</p><h3 id="3-Backpropagation-in-recurrent-neural-networks-OPTIONAL-UNGRADED"><a href="#3-Backpropagation-in-recurrent-neural-networks-OPTIONAL-UNGRADED" class="headerlink" title="3 - Backpropagation in recurrent neural networks (OPTIONAL / UNGRADED)"></a>3 - Backpropagation in recurrent neural networks (OPTIONAL / UNGRADED)</h3><p>In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. If however you are an expert in calculus and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook.</p><p>When in an earlier <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph" target="_blank" rel="noopener">course</a> you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks you can calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are quite complicated and we did not derive them in lecture. However, we will briefly present them below.</p><p>Note that this notebook does not implement the backward path from the Loss ‘J’ backwards to ‘a’. This would have included the dense layer and softmax which are a part of the forward path. This is assumed to be calculated elsewhere and the result passed to rnn_backward in ‘da’. It is further assumed that loss has been adjusted for batch size (m) and division by the number of examples is not required here.</p><p>This section is optional and ungraded. It is more difficult and has fewer details regarding its implementation. This section only implements key elements of the full path.</p><h4 id="3-1-Basic-RNN-backward-pass"><a href="#3-1-Basic-RNN-backward-pass" class="headerlink" title="3.1 - Basic RNN  backward pass"></a>3.1 - Basic RNN backward pass</h4><p>We will start by computing the backward pass for the basic RNN-cell and then in the following sections, iterate through the cells.</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584447272362.png" alt="1584447272362"></p><p>Recall from lecture, the shorthand for the partial derivative of cost relative to a variable is dVariable. For example, $\frac{\partial J}{\partial W_{ax}}$ is $dW_{ax}$. This will be used throughout the remaining sections.</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584447433355.png" alt="1584447433355"></p><h5 id="Equations"><a href="#Equations" class="headerlink" title="Equations"></a>Equations</h5><p>To compute the rnn_cell_backward you can utilize the following equations. It is a good exercise to derive them by hand. Here, $*$ denotes element-wise multiplication while the absence of a symbol indicates matrix multiplication.</p><p>$a^{\langle t \rangle} = \tanh(W_{ax} x^{\langle t \rangle} + W_{aa} a^{\langle t-1 \rangle} + b_{a})\tag{-}$</p><p>$\displaystyle \frac{\partial \tanh(x)} {\partial x} = 1 - \tanh^2(x) \tag{-}$</p><p>$\displaystyle {dW_{ax}} = da_{next} * ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) ) x^{\langle t \rangle T}\tag{1}$</p><p>$\displaystyle dW_{aa} = da_{next} * (( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) ) a^{\langle t-1 \rangle T}\tag{2}$</p><p>$\displaystyle db_a = da_{next} * \sum_{batch}( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{3}$</p><p>$\displaystyle dx^{\langle t \rangle} = da_{next} * { W_{ax}}^T ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) )\tag{4}$</p><p>$\displaystyle da_{prev} = da_{next} * { W_{aa}}^T ( 1-\tanh^2(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a}) ) \tag{5}$</p><h4 id="Implementing-rnn-cell-backward"><a href="#Implementing-rnn-cell-backward" class="headerlink" title="Implementing rnn_cell_backward"></a>Implementing rnn_cell_backward</h4><p>The results can be computed directly by implementing the equations above. However, the above can optionally be simplified by computing ‘dz’ and utlilizing the chain rule.<br>This can be further simplified by noting that $\tanh(W_{ax}x^{\langle t \rangle}+W_{aa} a^{\langle t-1 \rangle} + b_{a})$ was computed and saved in the forward pass.</p><p>To calculate dba, the ‘batch’ above is a sum across the horizontal (axis= 1) axis. Note that you should use the keepdims = True option.</p><p>It may be worthwhile to review Course 1 <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph" target="_blank" rel="noopener">Derivatives with a computational graph</a> through <a href="https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional" target="_blank" rel="noopener">Backpropagation Intuition</a>, which decompose the calculation into steps using the chain rule.<br>Matrix vector derivatives are described <a href="http://cs231n.stanford.edu/vecDerivs.pdf" target="_blank" rel="noopener">here</a>, though the equations above incorporate the required transformations.</p><p>Note rnn_cell_backward does <strong>not</strong> include the calculation of loss from $y \langle t \rangle$, this is incorporated into the incoming da_next. This is a slight mismatch with rnn_cell_forward which includes a dense layer and softmax.</p><p>Note: in the code:<br>$\displaystyle dx^{\langle t \rangle}$ is represented by dxt,<br>$\displaystyle d W_{ax}$ is represented by dWax,<br>$\displaystyle da_{prev}$ is represented by da_prev,<br>$\displaystyle dW_{aa}$ is represented by dWaa,<br>$\displaystyle db_{a}$ is represented by dba,<br>dz is not derived above but can optionally be derived by students to simplify the repeated calculations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Wax'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Waa'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wya'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'ba'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a_next_tmp, yt_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)</span><br><span class="line"></span><br><span class="line">da_next_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">gradients_tmp = rnn_cell_backward(da_next_tmp, cache_tmp)</span><br><span class="line">print(<span class="string">"gradients[\"dxt\"][1][2] ="</span>, gradients_tmp[<span class="string">"dxt"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dxt\"].shape ="</span>, gradients_tmp[<span class="string">"dxt"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"da_prev\"][2][3] ="</span>, gradients_tmp[<span class="string">"da_prev"</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"gradients[\"da_prev\"].shape ="</span>, gradients_tmp[<span class="string">"da_prev"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWax\"][3][1] ="</span>, gradients_tmp[<span class="string">"dWax"</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWax\"].shape ="</span>, gradients_tmp[<span class="string">"dWax"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWaa\"][1][2] ="</span>, gradients_tmp[<span class="string">"dWaa"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWaa\"].shape ="</span>, gradients_tmp[<span class="string">"dWaa"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dba\"][4] ="</span>, gradients_tmp[<span class="string">"dba"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dba\"].shape ="</span>, gradients_tmp[<span class="string">"dba"</span>].shape)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Wax'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Waa'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wya'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'ba'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a_next_tmp, yt_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)</span><br><span class="line"></span><br><span class="line">da_next_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">gradients_tmp = rnn_cell_backward(da_next_tmp, cache_tmp)</span><br><span class="line">print(<span class="string">"gradients[\"dxt\"][1][2] ="</span>, gradients_tmp[<span class="string">"dxt"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dxt\"].shape ="</span>, gradients_tmp[<span class="string">"dxt"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"da_prev\"][2][3] ="</span>, gradients_tmp[<span class="string">"da_prev"</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"gradients[\"da_prev\"].shape ="</span>, gradients_tmp[<span class="string">"da_prev"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWax\"][3][1] ="</span>, gradients_tmp[<span class="string">"dWax"</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWax\"].shape ="</span>, gradients_tmp[<span class="string">"dWax"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWaa\"][1][2] ="</span>, gradients_tmp[<span class="string">"dWaa"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWaa\"].shape ="</span>, gradients_tmp[<span class="string">"dWaa"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dba\"][4] ="</span>, gradients_tmp[<span class="string">"dba"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dba\"].shape ="</span>, gradients_tmp[<span class="string">"dba"</span>].shape)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584449910545.png" alt="1584449910545"></p><h4 id="Backward-pass-through-the-RNN"><a href="#Backward-pass-through-the-RNN" class="headerlink" title="Backward pass through the RNN"></a>Backward pass through the RNN</h4><p>Computing the gradients of the cost with respect to $a^{\langle t \rangle}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_a$, $dW_{aa}$, $dW_{ax}$ and you store $dx$.</p><p><strong>Instructions</strong>:</p><p>Implement the <code>rnn_backward</code> function. Initialize the return variables with zeros first and then loop through all the time steps while calling the <code>rnn_cell_backward</code> at each time timestep, update the other variables accordingly.</p><ul><li>Note that this notebook does not implement the backward path from the Loss ‘J’ backwards to ‘a’.<ul><li>This would have included the dense layer and softmax which are a part of the forward path.</li><li>This is assumed to be calculated elsewhere and the result passed to rnn_backward in ‘da’.</li><li>You must combine this with the loss from the previous stages when calling rnn_cell_backward (see figure 7 above).</li></ul></li><li>It is further assumed that loss has been adjusted for batch size (m).<ul><li>Therefore, division by the number of examples is not required here.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches (≈2 lines)</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, a0, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈6 lines)</span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    dWax = np.zeros((n_a, n_x))</span><br><span class="line">    dWaa = np.zeros((n_a, n_a))</span><br><span class="line">    dba = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    da0 = np.zeros((n_a, m))</span><br><span class="line">    da_prevt = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop through all the time steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line)</span></span><br><span class="line">        gradients = rnn_cell_backward(da[:,:,t]+da_prevt, caches[t]) <span class="comment"># !!!!</span></span><br><span class="line">        <span class="comment"># Retrieve derivatives from gradients (≈ 1 line)</span></span><br><span class="line">        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[<span class="string">"dxt"</span>], gradients[<span class="string">"da_prev"</span>], gradients[<span class="string">"dWax"</span>], gradients[<span class="string">"dWaa"</span>], gradients[<span class="string">"dba"</span>]</span><br><span class="line">        <span class="comment"># Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)</span></span><br><span class="line">        dx[:, :, t] = dxt</span><br><span class="line">        dWax += dWaxt</span><br><span class="line">        dWaa += dWaat</span><br><span class="line">        dba += dbat</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) </span></span><br><span class="line">    da0 = da_prevt</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa,<span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line">a0_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Wax'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Waa'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wya'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'ba'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a_tmp, y_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)</span><br><span class="line">da_tmp = np.random.randn(<span class="number">5</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">gradients_tmp = rnn_backward(da_tmp, caches_tmp)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"gradients[\"dx\"][1][2] ="</span>, gradients_tmp[<span class="string">"dx"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dx\"].shape ="</span>, gradients_tmp[<span class="string">"dx"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"da0\"][2][3] ="</span>, gradients_tmp[<span class="string">"da0"</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"gradients[\"da0\"].shape ="</span>, gradients_tmp[<span class="string">"da0"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWax\"][3][1] ="</span>, gradients_tmp[<span class="string">"dWax"</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWax\"].shape ="</span>, gradients_tmp[<span class="string">"dWax"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWaa\"][1][2] ="</span>, gradients_tmp[<span class="string">"dWaa"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWaa\"].shape ="</span>, gradients_tmp[<span class="string">"dWaa"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dba\"][4] ="</span>, gradients_tmp[<span class="string">"dba"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dba\"].shape ="</span>, gradients_tmp[<span class="string">"dba"</span>].shape)</span><br></pre></td></tr></table></figure><h4 id="3-2-LSTM-backward-pass"><a href="#3-2-LSTM-backward-pass" class="headerlink" title="3.2 - LSTM backward pass"></a>3.2 - LSTM backward pass</h4><h5 id="3-2-1-One-Step-backward"><a href="#3-2-1-One-Step-backward" class="headerlink" title="3.2.1 One Step backward"></a>3.2.1 One Step backward</h5><p>The LSTM backward pass is slighltly more complicated than the forward one.</p><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584450014791.png" alt="1584450014791"></p><h5 id="3-2-2-gate-derivatives"><a href="#3-2-2-gate-derivatives" class="headerlink" title="3.2.2 gate derivatives"></a>3.2.2 gate derivatives</h5><p>Note the location of the gate derivatives ($\gamma$..) between the dense layer and the activation function (see graphic above). This is convenient for computing parameter derivatives in the next step.</p><p>$d\gamma_o^{\langle t \rangle} = da_{next}<em>\tanh(c_{next}) </em>\Gamma_o^{\langle t \rangle}*\left(1-\Gamma_o^{\langle t \rangle}\right)\tag{7}$</p><p>$dp\widetilde{c}^{\langle t \rangle} = \left(dc_{next}<em>\Gamma_u^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle}</em> (1-\tanh^2(c_{next})) <em>\Gamma_u^{\langle t \rangle} </em>da_{next} \right) * \left(1-\left(\widetilde c^{\langle t \rangle}\right)^2\right) \tag{8}$</p><p>$d\gamma_u^{\langle t \rangle} = \left(dc_{next}<em>\widetilde{c}^{\langle t \rangle} + \Gamma_o^{\langle t \rangle}</em> (1-\tanh^2(c_{next})) <em>\widetilde{c}^{\langle t \rangle} </em>da_{next}\right)<em>\Gamma_u^{\langle t \rangle}</em>\left(1-\Gamma_u^{\langle t \rangle}\right)\tag{9}$</p><p>$d\gamma_f^{\langle t \rangle} = \left(dc_{next}<em> c_{prev} + \Gamma_o^{\langle t \rangle} </em>(1-\tanh^2(c_{next})) <em>c_{prev} </em>da_{next}\right)<em>\Gamma_f^{\langle t \rangle}</em>\left(1-\Gamma_f^{\langle t \rangle}\right)\tag{10}$</p><h5 id="3-2-3-parameter-derivatives"><a href="#3-2-3-parameter-derivatives" class="headerlink" title="3.2.3 parameter derivatives"></a>3.2.3 parameter derivatives</h5><p>$ dW_f = d\gamma_f^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{11} $<br>$ dW_u = d\gamma_u^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{12} $<br>$ dW_c = dp\widetilde c^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{13} $<br>$ dW_o = d\gamma_o^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{14}$</p><p>To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\gamma_f^{\langle t \rangle}, d\gamma_u^{\langle t \rangle}, dp\widetilde c^{\langle t \rangle}, d\gamma_o^{\langle t \rangle}$ respectively. Note that you should have the <code>keepdims = True</code> option.</p><p>$\displaystyle db_f = \sum_{batch}d\gamma_f^{\langle t \rangle}\tag{15}$<br>$\displaystyle db_u = \sum_{batch}d\gamma_u^{\langle t \rangle}\tag{16}$<br>$\displaystyle db_c = \sum_{batch}d\gamma_c^{\langle t \rangle}\tag{17}$<br>$\displaystyle db_o = \sum_{batch}d\gamma_o^{\langle t \rangle}\tag{18}$</p><p>Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input.</p><p>$ da_{prev} = W_f^T d\gamma_f^{\langle t \rangle} + W_u^T d\gamma_u^{\langle t \rangle}+ W_c^T dp\widetilde c^{\langle t \rangle} + W_o^T d\gamma_o^{\langle t \rangle} \tag{19}$</p><p>Here, to account for concatenation, the weights for equations 19 are the first n_a, (i.e. $W_f = W_f[:,:n_a]$ etc…)</p><p>$ dc_{prev} = dc_{next}<em>\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} </em>(1- \tanh^2(c_{next}))<em>\Gamma_f^{\langle t \rangle}</em>da_{next} \tag{20}$</p><p>$ dx^{\langle t \rangle} = W_f^T d\gamma_f^{\langle t \rangle} + W_u^T d\gamma_u^{\langle t \rangle}+ W_c^T dp\widetilde c^{\langle t \rangle} + W_o^T d\gamma_o^{\langle t \rangle}\tag{21} $</p><p>where the weights for equation 21 are from n_a to the end, (i.e. $W_f = W_f[:,n_a:]$ etc…)</p><p><strong>Exercise:</strong> Implement <code>lstm_cell_backward</code> by implementing equations $7-21$ below.</p><p>Note: In the code:</p><p>$d\gamma_o^{\langle t \rangle}$ is represented by <code>dot</code>,<br>$dp\widetilde{c}^{\langle t \rangle}$ is represented by <code>dcct</code>,<br>$d\gamma_u^{\langle t \rangle}$ is represented by <code>dit</code>,<br>$d\gamma_f^{\langle t \rangle}$ is represented by <code>dft</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next, dc_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the LSTM-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradients of next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    dc_next -- Gradients of next cell state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    cache -- cache storing information from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from xt's and a_next's shape (≈2 lines)</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_a, m = a_next.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)</span></span><br><span class="line">    dot = da_next*np.tanh(c_next)*ot*(<span class="number">1</span> - ot)</span><br><span class="line">    dcct = (dc_next+da_next*ot*( <span class="number">1</span>- np.square(np.tanh(c_next))))*it*( <span class="number">1</span>-np.square(cct))</span><br><span class="line">    dit = (dc_next+da_next*ot*( <span class="number">1</span>- np.square(np.tanh(c_next))))*cct*it*(<span class="number">1</span>-it)</span><br><span class="line">    dft = (dc_next+da_next*ot*( <span class="number">1</span>- np.square(np.tanh(c_next))))*c_prev*ft*(<span class="number">1</span>-ft)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute parameters related derivatives. Use equations (11)-(18) (≈8 lines)</span></span><br><span class="line">    concat = np.concatenate((a_prev, xt), axis=<span class="number">0</span>)</span><br><span class="line">    dWf = np.dot( dft, concat.T)</span><br><span class="line">    dWi = np.dot( dit, concat.T)</span><br><span class="line">    dWc = np.dot( dcct, concat.T)</span><br><span class="line">    dWo = np.dot( dot, concat.T)</span><br><span class="line">    dbf = np.sum( dft, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dbi = np.sum( dit, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dbc = np.sum( dcct, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dbo = np.sum( dot, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (19)-(21). (≈3 lines)</span></span><br><span class="line">    da_prev = np.dot(parameters[<span class="string">'Wf'</span>][:,:n_a].T, dft) + np.dot(parameters[<span class="string">'Wi'</span>][:,:n_a].T, dit) + \</span><br><span class="line">        np.dot(parameters[<span class="string">'Wc'</span>][:,:n_a].T, dcct) + np.dot(parameters[<span class="string">'Wo'</span>][:,:n_a].T, dot) </span><br><span class="line">    dc_prev = dc_next*ft + ot*(<span class="number">1</span>-np.square(np.tanh(c_next)))*ft*da_next </span><br><span class="line">    dxt = np.dot(parameters[<span class="string">'Wf'</span>][:,n_a:].T,dft)+np.dot(parameters[<span class="string">'Wi'</span>][:,n_a:].T,dit) + \</span><br><span class="line">        np.dot(parameters[<span class="string">'Wc'</span>][:,n_a:].T,dcct)+np.dot(parameters[<span class="string">'Wo'</span>][:,n_a:].T,dot) </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save gradients in dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dc_prev"</span>: dc_prev, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">c_prev_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Wf'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bf'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wi'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bi'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wo'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bo'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wc'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bc'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wy'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)</span><br><span class="line"></span><br><span class="line">da_next_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">dc_next_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">gradients_tmp = lstm_cell_backward(da_next_tmp, dc_next_tmp, cache_tmp)</span><br><span class="line">print(<span class="string">"gradients[\"dxt\"][1][2] ="</span>, gradients_tmp[<span class="string">"dxt"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dxt\"].shape ="</span>, gradients_tmp[<span class="string">"dxt"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"da_prev\"][2][3] ="</span>, gradients_tmp[<span class="string">"da_prev"</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"gradients[\"da_prev\"].shape ="</span>, gradients_tmp[<span class="string">"da_prev"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dc_prev\"][2][3] ="</span>, gradients_tmp[<span class="string">"dc_prev"</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dc_prev\"].shape ="</span>, gradients_tmp[<span class="string">"dc_prev"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWf\"][3][1] ="</span>, gradients_tmp[<span class="string">"dWf"</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWf\"].shape ="</span>, gradients_tmp[<span class="string">"dWf"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWi\"][1][2] ="</span>, gradients_tmp[<span class="string">"dWi"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWi\"].shape ="</span>, gradients_tmp[<span class="string">"dWi"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWc\"][3][1] ="</span>, gradients_tmp[<span class="string">"dWc"</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWc\"].shape ="</span>, gradients_tmp[<span class="string">"dWc"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWo\"][1][2] ="</span>, gradients_tmp[<span class="string">"dWo"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWo\"].shape ="</span>, gradients_tmp[<span class="string">"dWo"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbf\"][4] ="</span>, gradients_tmp[<span class="string">"dbf"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbf\"].shape ="</span>, gradients_tmp[<span class="string">"dbf"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbi\"][4] ="</span>, gradients_tmp[<span class="string">"dbi"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbi\"].shape ="</span>, gradients_tmp[<span class="string">"dbi"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbc\"][4] ="</span>, gradients_tmp[<span class="string">"dbc"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbc\"].shape ="</span>, gradients_tmp[<span class="string">"dbc"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbo\"][4] ="</span>, gradients_tmp[<span class="string">"dbo"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbo\"].shape ="</span>, gradients_tmp[<span class="string">"dbo"</span>].shape)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584450856819.png" alt="1584450856819"></p><h4 id="3-3-Backward-pass-through-the-LSTM-RNN"><a href="#3-3-Backward-pass-through-the-LSTM-RNN" class="headerlink" title="3.3 Backward pass through the LSTM RNN"></a>3.3 Backward pass through the LSTM RNN</h4><p>This part is very similar to the <code>rnn_backward</code> function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients.</p><p><strong>Instructions</strong>: Implement the <code>lstm_backward</code> function. Create a for loop starting from $T_x$ and going backward. For each step call <code>lstm_cell_backward</code> and update the your old gradients by adding the new gradients to them. Note that <code>dxt</code> is not updated but is stored.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- cache storing information from the forward pass (lstm_forward)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient of inputs, of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches.</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈12 lines)</span></span><br><span class="line">    dx = np.zeros(( n_x, m, T_x))</span><br><span class="line">    da0 = np.zeros(( n_a, m))</span><br><span class="line">    da_prevt = np.zeros(( n_a, m))</span><br><span class="line">    dc_prevt = np.zeros(( n_a, m))</span><br><span class="line">    dWf = np.zeros(( n_a, n_a+n_x))</span><br><span class="line">    dWi = np.zeros(( n_a, n_a+n_x))</span><br><span class="line">    dWc = np.zeros(( n_a, n_a+n_x))</span><br><span class="line">    dWo = np.zeros(( n_a, n_a+n_x))</span><br><span class="line">    dbf = np.zeros(( n_a, <span class="number">1</span>))</span><br><span class="line">    dbi = np.zeros(( n_a, <span class="number">1</span>))</span><br><span class="line">    dbc = np.zeros(( n_a, <span class="number">1</span>))</span><br><span class="line">    dbo = np.zeros(( n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop back over the whole sequence</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute all gradients using lstm_cell_backward</span></span><br><span class="line">        gradients = lstm_cell_backward(da[:,:,t]+da_prevt, dc_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Store or add the gradient to the parameters' previous step's gradient</span></span><br><span class="line">        dxt, da_prevt, dc_prevt = gradients[<span class="string">'dxt'</span>], gradients[<span class="string">'da_prev'</span>], gradients[<span class="string">'dc_prev'</span>]</span><br><span class="line">        dx[:,:,t] = gradients[<span class="string">'dxt'</span>]</span><br><span class="line">        dWf += gradients[<span class="string">'dWf'</span>]</span><br><span class="line">        dWi += gradients[<span class="string">'dWi'</span>]</span><br><span class="line">        dWc += gradients[<span class="string">'dWc'</span>]</span><br><span class="line">        dWo += gradients[<span class="string">'dWo'</span>]</span><br><span class="line">        dbf += gradients[<span class="string">'dbf'</span>]</span><br><span class="line">        dbi += gradients[<span class="string">'dbi'</span>]</span><br><span class="line">        dbc += gradients[<span class="string">'dbc'</span>]</span><br><span class="line">        dbo += gradients[<span class="string">'dbo'</span>]</span><br><span class="line">    <span class="comment"># Set the first activation's gradient to the backpropagated gradient da_prev.</span></span><br><span class="line">    da0 = da_prevt</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x_tmp = np.random.randn(<span class="number">3</span>,<span class="number">10</span>,<span class="number">7</span>)</span><br><span class="line">a0_tmp = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">parameters_tmp = &#123;&#125;</span><br><span class="line">parameters_tmp[<span class="string">'Wf'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bf'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wi'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bi'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wo'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bo'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wc'</span>] = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">parameters_tmp[<span class="string">'bc'</span>] = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">parameters_tmp[<span class="string">'Wy'</span>] = np.zeros((<span class="number">2</span>,<span class="number">5</span>))       <span class="comment"># unused, but needed for lstm_forward</span></span><br><span class="line">parameters_tmp[<span class="string">'by'</span>] = np.zeros((<span class="number">2</span>,<span class="number">1</span>))       <span class="comment"># unused, but needed for lstm_forward</span></span><br><span class="line"></span><br><span class="line">a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)</span><br><span class="line"></span><br><span class="line">da_tmp = np.random.randn(<span class="number">5</span>, <span class="number">10</span>, <span class="number">4</span>)</span><br><span class="line">gradients_tmp = lstm_backward(da_tmp, caches_tmp)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"gradients[\"dx\"][1][2] ="</span>, gradients_tmp[<span class="string">"dx"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dx\"].shape ="</span>, gradients_tmp[<span class="string">"dx"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"da0\"][2][3] ="</span>, gradients_tmp[<span class="string">"da0"</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"gradients[\"da0\"].shape ="</span>, gradients_tmp[<span class="string">"da0"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWf\"][3][1] ="</span>, gradients_tmp[<span class="string">"dWf"</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWf\"].shape ="</span>, gradients_tmp[<span class="string">"dWf"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWi\"][1][2] ="</span>, gradients_tmp[<span class="string">"dWi"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWi\"].shape ="</span>, gradients_tmp[<span class="string">"dWi"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWc\"][3][1] ="</span>, gradients_tmp[<span class="string">"dWc"</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWc\"].shape ="</span>, gradients_tmp[<span class="string">"dWc"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dWo\"][1][2] ="</span>, gradients_tmp[<span class="string">"dWo"</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dWo\"].shape ="</span>, gradients_tmp[<span class="string">"dWo"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbf\"][4] ="</span>, gradients_tmp[<span class="string">"dbf"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbf\"].shape ="</span>, gradients_tmp[<span class="string">"dbf"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbi\"][4] ="</span>, gradients_tmp[<span class="string">"dbi"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbi\"].shape ="</span>, gradients_tmp[<span class="string">"dbi"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbc\"][4] ="</span>, gradients_tmp[<span class="string">"dbc"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbc\"].shape ="</span>, gradients_tmp[<span class="string">"dbc"</span>].shape)</span><br><span class="line">print(<span class="string">"gradients[\"dbo\"][4] ="</span>, gradients_tmp[<span class="string">"dbo"</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">"gradients[\"dbo\"].shape ="</span>, gradients_tmp[<span class="string">"dbo"</span>].shape)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/16/deeplearning-ai笔记（5-1）/1584450956611.png" alt="1584450956611"></p><h3 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations !"></a>Congratulations !</h3><p>Congratulations on completing this assignment. You now understand how recurrent neural networks work!</p><p>Let’s go on to the next exercise, where you’ll use an RNN to build a character-level language model.</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p><p><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">https://www.coursera.org/specializations/deep-learning</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p><p><a href="https://blog.csdn.net/Koala_Tree/article/details/79913655" target="_blank" rel="noopener">https://blog.csdn.net/Koala_Tree/article/details/79913655</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/03/10/deeplearning-ai笔记（4-4）/" rel="next" title="deeplearning-ai笔记（4-4）"><i class="fa fa-chevron-left"></i> deeplearning-ai笔记（4-4）</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/03/22/Hadoop安装教程/" rel="prev" title="Hadoop安装教程">Hadoop安装教程 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/kikyo.jpg" alt="Kikyō"><p class="site-author-name" itemprop="name">Kikyō</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">33</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#循环序列模型（Recurrent-Neural-Networks）"><span class="nav-text">循环序列模型（Recurrent Neural Networks）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-为什么选择序列模型？（Why-Sequence-Models-）"><span class="nav-text">1.1 为什么选择序列模型？（Why Sequence Models?）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-数学符号（Notation）"><span class="nav-text">1.2 数学符号（Notation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-循环神经网络模型（Recurrent-Neural-Network-Model）"><span class="nav-text">1.3 循环神经网络模型（Recurrent Neural Network Model）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Recurrent-Neural-Networks"><span class="nav-text">Recurrent Neural Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward-Propagation"><span class="nav-text">Forward Propagation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-通过时间的反向传播（Backpropagation-through-time）"><span class="nav-text">1.4 通过时间的反向传播（Backpropagation through time）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-不同类型的循环神经网络（Different-types-of-RNNs）"><span class="nav-text">1.5 不同类型的循环神经网络（Different types of RNNs）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-语言模型和序列生成（Language-model-and-sequence-generation）"><span class="nav-text">1.6 语言模型和序列生成（Language model and sequence generation）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-对新序列采样（Sampling-novel-sequences）"><span class="nav-text">1.7 对新序列采样（Sampling novel sequences）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-循环神经网络的梯度消失（Vanishing-gradients-with-RNNs）"><span class="nav-text">1.8 循环神经网络的梯度消失（Vanishing gradients with RNNs）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-GRU单元（Gated-Recurrent-Unit（GRU））"><span class="nav-text">1.9 GRU单元（Gated Recurrent Unit（GRU））</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简化过的GRU单元"><span class="nav-text">简化过的GRU单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#完整的GRU单元"><span class="nav-text">完整的GRU单元</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-长短期记忆（LSTM（long-short-term-memory）unit）"><span class="nav-text">1.10 长短期记忆（LSTM（long short term memory）unit）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-11-双向循环神经网络（Bidirectional-RNN）"><span class="nav-text">1.11 双向循环神经网络（Bidirectional RNN）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-12-深层循环神经网络（Deep-RNNs）"><span class="nav-text">1.12 深层循环神经网络（Deep RNNs）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Building-your-Recurrent-Neural-Network-Step-by-Step"><span class="nav-text">Building your Recurrent Neural Network - Step by Step</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Pre-requisites"><span class="nav-text">Pre-requisites</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Be-careful-when-modifying-the-starter-code"><span class="nav-text">Be careful when modifying the starter code</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Updates-for-3a"><span class="nav-text">Updates for 3a</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#If-you-were-working-on-the-notebook-before-this-update…"><span class="nav-text">If you were working on the notebook before this update…</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#List-of-updates"><span class="nav-text">List of updates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><span class="nav-text">1 - Forward propagation for the basic Recurrent Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Dimensions-of-input-x"><span class="nav-text">Dimensions of input $x$</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Input-with-n-x-number-of-units"><span class="nav-text">Input with $n_x$ number of units</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Time-steps-of-size-T-x"><span class="nav-text">Time steps of size $T_{x}$</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Batches-of-size-m"><span class="nav-text">Batches of size $m$</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3D-Tensor-of-shape-n-x-m-T-x"><span class="nav-text">3D Tensor of shape $(n_{x},m,T_{x})$</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Taking-a-2D-slice-for-each-time-step-x-langle-t-rangle"><span class="nav-text">Taking a 2D slice for each time step: $x^{\langle t \rangle}$</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Definition-of-hidden-state-a"><span class="nav-text">Definition of hidden state $a$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dimensions-of-hidden-state-a"><span class="nav-text">Dimensions of hidden state $a$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Dimensions-of-prediction-hat-y"><span class="nav-text">Dimensions of prediction $\hat{y}$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-RNN-cell"><span class="nav-text">1.1 - RNN cell</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#rnn-cell-versus-rnn-cell-forward"><span class="nav-text">rnn cell versus rnn_cell_forward</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Additional-Hints"><span class="nav-text">Additional Hints</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-RNN-forward-pass"><span class="nav-text">1.2 - RNN forward pass</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Additional-Hints-1"><span class="nav-text">Additional Hints</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Situations-when-this-RNN-will-perform-better"><span class="nav-text">Situations when this RNN will perform better:</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Long-Short-Term-Memory-LSTM-network"><span class="nav-text">2 - Long Short-Term Memory (LSTM) network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overview-of-gates-and-states"><span class="nav-text">Overview of gates and states</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Forget-gate-mathbf-Gamma-f"><span class="nav-text">- Forget gate $\mathbf{\Gamma}_{f}$</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Equation"><span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Explanation-of-the-equation"><span class="nav-text">Explanation of the equation:</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Variable-names-in-the-code"><span class="nav-text">Variable names in the code</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Candidate-value-mathbf-c-langle-t-rangle"><span class="nav-text">Candidate value ${\mathbf{c}}^{\langle t \rangle}$</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Equation-1"><span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Explanation-of-the-equation-1"><span class="nav-text">Explanation of the equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Variable-names-in-the-code-1"><span class="nav-text">Variable names in the code</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Update-gate-mathbf-Gamma-i"><span class="nav-text">- Update gate $\mathbf{\Gamma}_{i}$</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Equation-2"><span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Explanation-of-the-equation-2"><span class="nav-text">Explanation of the equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Variable-names-in-code-Please-note-that-they’re-different-than-the-equations"><span class="nav-text">Variable names in code (Please note that they’re different than the equations)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Cell-state-mathbf-c-langle-t-rangle"><span class="nav-text">- Cell state $\mathbf{c}^{\langle t \rangle}$</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Equation-3"><span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Explanation-of-equation"><span class="nav-text">Explanation of equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Variable-names-and-shapes-in-the-code"><span class="nav-text">Variable names and shapes in the code</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Output-gate-mathbf-Gamma-o"><span class="nav-text">- Output gate $\mathbf{\Gamma}_{o}$</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Equation-4"><span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Explanation-of-the-equation-3"><span class="nav-text">Explanation of the equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Variable-names-in-the-code-2"><span class="nav-text">Variable names in the code</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hidden-state-mathbf-a-langle-t-rangle"><span class="nav-text">- Hidden state $\mathbf{a}^{\langle t \rangle}$</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Equation-5"><span class="nav-text">Equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Explanation-of-equation-1"><span class="nav-text">Explanation of equation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Variable-names-and-shapes-in-the-code-1"><span class="nav-text">Variable names and shapes in the code</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Prediction-mathbf-y-langle-t-rangle-pred"><span class="nav-text">- Prediction $\mathbf{y}^{\langle t \rangle}_{pred}$</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Variable-names-and-shapes-in-the-code-2"><span class="nav-text">Variable names and shapes in the code</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-LSTM-cell"><span class="nav-text">2.1 - LSTM cell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Additional-Hints-2"><span class="nav-text">Additional Hints</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Forward-pass-for-LSTM"><span class="nav-text">2.2 - Forward pass for LSTM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Backpropagation-in-recurrent-neural-networks-OPTIONAL-UNGRADED"><span class="nav-text">3 - Backpropagation in recurrent neural networks (OPTIONAL / UNGRADED)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Basic-RNN-backward-pass"><span class="nav-text">3.1 - Basic RNN backward pass</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Equations"><span class="nav-text">Equations</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implementing-rnn-cell-backward"><span class="nav-text">Implementing rnn_cell_backward</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Backward-pass-through-the-RNN"><span class="nav-text">Backward pass through the RNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-LSTM-backward-pass"><span class="nav-text">3.2 - LSTM backward pass</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-1-One-Step-backward"><span class="nav-text">3.2.1 One Step backward</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-2-gate-derivatives"><span class="nav-text">3.2.2 gate derivatives</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-3-parameter-derivatives"><span class="nav-text">3.2.3 parameter derivatives</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Backward-pass-through-the-LSTM-RNN"><span class="nav-text">3.3 Backward pass through the LSTM RNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Congratulations"><span class="nav-text">Congratulations !</span></a></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料</span></a></li></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Kikyō</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">159.6k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script></body></html><!-- rebuild by neat -->