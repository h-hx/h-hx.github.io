<!-- build time:Fri Jun 04 2021 16:00:12 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-big-counter.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|DejaVu Sans Mono for Powerline:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-flower.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-flower.png?v=5.1.4"><meta name="keywords" content="深度学习,"><meta name="description" content="特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;amp;Neural style transfer）4.1 什么是人脸识别？（What is face recognition?）人脸验证（face verification）和人脸识别（face recognition）4.2 One-Shot学习（One-shot learning"><meta name="keywords" content="深度学习"><meta property="og:type" content="article"><meta property="og:title" content="deeplearning-ai笔记（4-4）"><meta property="og:url" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/index.html"><meta property="og:site_name"><meta property="og:description" content="特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;amp;Neural style transfer）4.1 什么是人脸识别？（What is face recognition?）人脸验证（face verification）和人脸识别（face recognition）4.2 One-Shot学习（One-shot learning"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583824432362.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583824456015.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583824758099.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583824898492.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583825314572.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583826893953.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827165886.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827217905.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827251760.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827336381.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827381866.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827478199.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827590182.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827712083.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583827783288.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584103235884.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584104899569.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584105010151.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584105106919.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584105966190.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584106392783.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584106340980.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584106376027.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584106418445.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584106593459.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584081693904.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584083143450.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584083273358.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584084274530.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584084356557.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584085275933.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584087836172.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584088516041.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584088539696.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584088682024.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584088725213.png"><meta property="og:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1584088734228.png"><meta property="og:updated_time" content="2020-03-17T12:22:04.282Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="deeplearning-ai笔记（4-4）"><meta name="twitter:description" content="特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;amp;Neural style transfer）4.1 什么是人脸识别？（What is face recognition?）人脸验证（face verification）和人脸识别（face recognition）4.2 One-Shot学习（One-shot learning"><meta name="twitter:image" content="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/1583824432362.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",version:"5.1.4",sidebar:{position:"right",display:"post",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/"><title>deeplearning-ai笔记（4-4） |</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title"></span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/10/deeplearning-ai笔记（4-4）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Kikyō"><meta itemprop="description" content><meta itemprop="image" content="/images/kikyo.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content></span><header class="post-header"><h1 class="post-title" itemprop="name headline">deeplearning-ai笔记（4-4）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-10T15:02:08+08:00">2020-03-10 </time><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i> </span><span class="post-meta-item-text">更新于&#58;</span> <time title="更新于" itemprop="dateModified" datetime="2020-03-17T20:22:04+08:00">2020-03-17 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/deeplearning-ai笔记/" itemprop="url" rel="index"><span itemprop="name">deeplearning.ai笔记</span></a></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计&#58;</span> <span title="字数统计">9.4k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span title="阅读时长">56</span></div></div></header><div class="post-body" itemprop="articleBody"><h2 id="特殊应用：人脸识别和神经风格转换（Special-applications-Face-recognition-amp-Neural-style-transfer）"><a href="#特殊应用：人脸识别和神经风格转换（Special-applications-Face-recognition-amp-Neural-style-transfer）" class="headerlink" title="特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;Neural style transfer）"></a>特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;Neural style transfer）</h2><h3 id="4-1-什么是人脸识别？（What-is-face-recognition-）"><a href="#4-1-什么是人脸识别？（What-is-face-recognition-）" class="headerlink" title="4.1 什么是人脸识别？（What is face recognition?）"></a>4.1 什么是人脸识别？（What is face recognition?）</h3><p>人脸验证（<strong>face verification</strong>）和人脸识别（<strong>face recognition</strong>）</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583824432362.png" alt="1583824432362"></p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583824456015.png" alt="1583824456015"></p><h3 id="4-2-One-Shot学习（One-shot-learning）"><a href="#4-2-One-Shot学习（One-shot-learning）" class="headerlink" title="4.2 One-Shot学习（One-shot learning）"></a>4.2 One-Shot学习（One-shot learning）</h3><p>对于一个人脸识别系统，我们需要仅仅通过先前的一张人脸的图片或者说一个人脸的样例，就能够实现该人的识别，那么这样的问题就是one shot 问题。</p><h4 id="Similarity函数"><a href="#Similarity函数" class="headerlink" title="Similarity函数"></a>Similarity函数</h4><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583824758099.png" alt="1583824758099"></p><h3 id="4-3-Siamese-网络（Siamese-network）"><a href="#4-3-Siamese-网络（Siamese-network）" class="headerlink" title="4.3 Siamese 网络（Siamese network）"></a>4.3 Siamese 网络（Siamese network）</h3><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583824898492.png" alt="1583824898492"></p><p>对于一个卷积神经网络结构，去掉最后的softmax层，将图片样本1输入网络，最后由网络输出一个N维的向量，这N维向量则代表输入图片样本1的编码。将不同人的图片样本输入相同参数的网络结构，得到各自相应的图片编码。</p><p>将$x^{(1)}$和$x^{(2)}$的距离定义为这两幅图片的编码之差的范数，$d( x^{( 1)},x^{( 2)}) =|| f( x^{( 1)}) - f( x^{( 2)})||_{2}^{2}$。</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583825314572.png" alt="1583825314572"></p><h3 id="4-4-Triplet-损失（Triplet-损失）"><a href="#4-4-Triplet-损失（Triplet-损失）" class="headerlink" title="4.4 Triplet 损失（Triplet 损失）"></a>4.4 Triplet 损失（Triplet 损失）</h3><p>通常是让<strong>Anchor</strong>图片和<strong>Positive</strong>图片（<strong>Positive</strong>意味着是同一个人）的距离很接近。与<strong>Negative</strong>图片（<strong>Negative</strong>意味着是非同一个人）对比时，你让他们的距离离得更远一点。</p><p>即：$|| f(A) - f(P)||^{2} \leq ||f(A) - f(N)||^{2}$）</p><p>（$|| f(A) - f(P) ||^{2}$）就是$d(A,P)$，（$|| f(A) - f(N) ||^{2}$）是$d(A,N)$。</p><p>为了避免所有图像的$f$都是一个零向量，总能满足这个方程。通常使用一个超参数$\alpha$使</p><p>$|| f(A) - f(P)||^{2} -||f(A) - f(N)||^{2} +a\leq0$</p><p>损失函数为：$L(A,P,N) = \max (||f(A) - f(P)||^{2} - ||f(A) - f(N)||^{2} + \alpha, \ 0)$</p><h3 id="4-5-人脸验证与二分类（Face-verification-and-binary-classification）"><a href="#4-5-人脸验证与二分类（Face-verification-and-binary-classification）" class="headerlink" title="4.5 人脸验证与二分类（Face verification and binary classification）"></a>4.5 人脸验证与二分类（Face verification and binary classification）</h3><p><strong>Triplet loss</strong>是一个学习人脸识别卷积网络参数的好方法，还有其他学习参数的方法，让我们看看如何将人脸识别当成一个二分类问题。</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583826893953.png" alt="1583826893953"></p><p><strong>sigmoid</strong>函数应用到某些特征上</p><p>$\hat y = \sigma(\sum_{k = 1}^{128}{w_{i}| f( x^{( i)})_{k} - f( x^{( j)})_{k}| + b})$</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827165886.png" alt="1583827165886"></p><p>还有其他不同的形式来计算绿色标记的这部分公式（$| f( x^{( i)})_{k} - f( x^{( j)})_{k}|$），比如说，公式可以是$\frac{(f( x^{( i)})_{k} - f(x^{( j)})_{k})^{2}}{f(x^{( i)})_{k} + f( x^{( j)})_{k}}$，这个公式也被叫做$\chi^{2}$公式，是一个希腊字母$\chi$，也被称为$\chi$平方相似度。</p><h3 id="4-6-什么是神经风格迁移？（What-is-neural-style-transfer-）"><a href="#4-6-什么是神经风格迁移？（What-is-neural-style-transfer-）" class="headerlink" title="4.6 什么是神经风格迁移？（What is neural style transfer?）"></a>4.6 什么是神经风格迁移？（What is neural style transfer?）</h3><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827217905.png" alt="1583827217905"></p><h3 id="4-7-CNN特征可视化（What-are-deep-ConvNets-learning-）"><a href="#4-7-CNN特征可视化（What-are-deep-ConvNets-learning-）" class="headerlink" title="4.7 CNN特征可视化（What are deep ConvNets learning?）"></a>4.7 CNN特征可视化（What are deep ConvNets learning?）</h3><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827251760.png" alt="1583827251760"></p><p>我们希望看到不同层的隐藏单元的计算结果。依次对各个层进行如下操作：</p><ul><li>在当前层挑选一个隐藏单元；</li><li>遍历训练集，找到最大化地激活了该运算单元的图片或者图片块；</li><li>对该层的其他运算单元执行操作。</li></ul><h3 id="4-8-代价函数（Cost-function）"><a href="#4-8-代价函数（Cost-function）" class="headerlink" title="4.8 代价函数（Cost function）"></a>4.8 代价函数（Cost function）</h3><p>怎么判断生成图像的好坏呢？我们把这个代价函数定义为两个部分。</p><p>$J_{\text{content}}(C,G)$</p><p>第一部分被称作内容代价，这是一个关于内容图片和生成图片的函数，它是用来度量生成图片$G$的内容与内容图片$C$的内容有多相似。</p><p>$J_{\text{style}}(S,G)$</p><p>然后我们会把结果加上一个风格代价函数，也就是关于$S$和$G$的函数，用来度量图片$G$的风格和图片$S$的风格的相似度。</p><p>$J( G) = a J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)$</p><p>最后我们用两个超参数$a$和$\beta$来来确定内容代价和风格代价，两者之间的权重用两个超参数来确定。</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827336381.png" alt="1583827336381"></p><h3 id="4-9-内容代价函数（Content-cost-function）"><a href="#4-9-内容代价函数（Content-cost-function）" class="headerlink" title="4.9 内容代价函数（Content cost function）"></a>4.9 内容代价函数（Content cost function）</h3><p>风格迁移网络的代价函数有一个内容代价部分，还有一个风格代价部分。</p><p>$J( G) = \alpha J_{\text{content}}( C,G) + \beta J_{\text{style}}(S,G)$</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827381866.png" alt="1583827381866"></p><p>定义内容代价函数如下：</p><p>$J_{\text{content}}( C,G) = \frac{1}{2}|| a^{[l][C]} - a^{[l][G]}||^{2}$</p><h3 id="4-10-风格代价函数（Style-cost-function）"><a href="#4-10-风格代价函数（Style-cost-function）" class="headerlink" title="4.10 风格代价函数（Style cost function）"></a>4.10 风格代价函数（Style cost function）</h3><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827478199.png" alt="1583827478199"></p><p>$G$是一个矩阵，这个矩阵的高度和宽度都是$l$层的通道数。在这个矩阵中$k$和$k’$元素被用来描述$k$通道和$k’$通道之间的相关系数。具体地：</p><p>$G_{kk^{‘}}^{<a href="S">l</a>} = \sum_{i = 1}^{n_{H}^{[l]}}{\sum_{j = 1}^{n_{W}^{[l]}}{a_{i,\ j,\ k}^{<a href="S">l</a>}a_{i,\ j,\ k^{‘}}^{<a href="S">l</a>}}}$</p><p>$G_{kk^{‘}}^{<a href="G">l</a>} = \sum_{i = 1}^{n_{H}^{[l]}}{\sum_{j = 1}^{n_{W}^{[l]}}{a_{i,\ j,\ k}^{<a href="G">l</a>}a_{i,\ j,\ k^{‘}}^{<a href="G">l</a>}}}$</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827590182.png" alt="1583827590182"></p><h3 id="4-11-一维到三维推广（1D-and-3D-generalizations-of-models）"><a href="#4-11-一维到三维推广（1D-and-3D-generalizations-of-models）" class="headerlink" title="4.11 一维到三维推广（1D and 3D generalizations of models）"></a>4.11 一维到三维推广（1D and 3D generalizations of models）</h3><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827712083.png" alt="1583827712083"></p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1583827783288.png" alt="1583827783288"></p><h2 id="Face-Recognition"><a href="#Face-Recognition" class="headerlink" title="Face Recognition"></a>Face Recognition</h2><p>In this assignment, you will build a face recognition system. Many of the ideas presented here are from <a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="noopener">FaceNet</a>. In lecture, we also talked about <a href="https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf" target="_blank" rel="noopener">DeepFace</a>.</p><p>Face recognition problems commonly fall into two categories:</p><ul><li><strong>Face Verification</strong> - “is this the claimed person?”. For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem.</li><li><strong>Face Recognition</strong> - “who is this person?”. For example, the video lecture showed a <a href="https://www.youtube.com/watch?v=wr4rx0Spihs" target="_blank" rel="noopener">face recognition video</a> of Baidu employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem.</li></ul><p>FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, you can then determine if two pictures are of the same person.</p><p><strong>In this assignment, you will:</strong></p><ul><li>Implement the triplet loss function</li><li>Use a pretrained model to map face images into 128-dimensional encodings</li><li>Use these encodings to perform face verification and face recognition</li></ul><h4 id="Channels-first-notation"><a href="#Channels-first-notation" class="headerlink" title="Channels-first notation"></a>Channels-first notation</h4><ul><li>In this exercise, we will be using a pre-trained model which represents ConvNet activations using a <strong>“channels first”</strong> convention, as opposed to the “channels last” convention used in lecture and previous programming assignments.</li><li>In other words, a batch of images will be of shape $(m, n_C, n_H, n_W)$ instead of $(m, n_H, n_W, n_C)$.</li><li>Both of these conventions have a reasonable amount of traction among open-source implementations; there isn’t a uniform standard yet within the deep learning community.</li></ul><h3 id="Updates"><a href="#Updates" class="headerlink" title="Updates"></a>Updates</h3><h4 id="If-you-were-working-on-the-notebook-before-this-update…"><a href="#If-you-were-working-on-the-notebook-before-this-update…" class="headerlink" title="If you were working on the notebook before this update…"></a>If you were working on the notebook before this update…</h4><ul><li>The current notebook is version “3a”.</li><li>You can find your original work saved in the notebook with the previous version name (“v3”)</li><li>To view the file directory, go to the menu “File-&gt;Open”, and this will open a new tab that shows the file directory.</li></ul><h4 id="List-of-updates"><a href="#List-of-updates" class="headerlink" title="List of updates"></a>List of updates</h4><ul><li><code>triplet_loss</code>: Additional Hints added.</li><li><code>verify</code>: Hints added.</li><li><code>who_is_it</code>: corrected hints given in the comments.</li><li>Spelling and formatting updates for easier reading.</li></ul><h3 id="Load-packages"><a href="#Load-packages" class="headerlink" title="Load packages"></a>Load packages</h3><p>Let’s load the required packages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, ZeroPadding2D, Activation, Input, concatenate</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers.normalization <span class="keyword">import</span> BatchNormalization</span><br><span class="line"><span class="keyword">from</span> keras.layers.pooling <span class="keyword">import</span> MaxPooling2D, AveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.layers.merge <span class="keyword">import</span> Concatenate</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Lambda, Flatten, Dense</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">from</span> keras.engine.topology <span class="keyword">import</span> Layer</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_first'</span>)</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> fr_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> inception_blocks_v2 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.nan)</span><br></pre></td></tr></table></figure><h3 id="0-Naive-Face-Verification"><a href="#0-Naive-Face-Verification" class="headerlink" title="0 - Naive Face Verification"></a>0 - Naive Face Verification</h3><p>In Face Verification, you’re given two images and you have to determine if they are of the same person. The simplest way to do this is to compare the two images pixel-by-pixel. If the distance between the raw images are less than a chosen threshold, it may be the same person!</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584103235884.png" alt="1584103235884"></p><ul><li>Of course, this algorithm performs really poorly, since the pixel values change dramatically due to variations in lighting, orientation of the person’s face, even minor changes in head position, and so on.</li><li>You’ll see that rather than using the raw image, you can learn an encoding, $f(img)$.</li><li>By using an encoding for each image, an element-wise comparison produces a more accurate judgement as to whether two pictures are of the same person.</li></ul><h3 id="1-Encoding-face-images-into-a-128-dimensional-vector"><a href="#1-Encoding-face-images-into-a-128-dimensional-vector" class="headerlink" title="1 - Encoding face images into a 128-dimensional vector"></a>1 - Encoding face images into a 128-dimensional vector</h3><h4 id="1-1-Using-a-ConvNet-to-compute-encodings"><a href="#1-1-Using-a-ConvNet-to-compute-encodings" class="headerlink" title="1.1 - Using a ConvNet  to compute encodings"></a>1.1 - Using a ConvNet to compute encodings</h4><p>The FaceNet model takes a lot of data and a long time to train. So following common practice in applied deep learning, let’s load weights that someone else has already trained. The network architecture follows the Inception model from <a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Szegedy <em>et al.</em></a>. We have provided an inception network implementation. You can look in the file <code>inception_blocks_v2.py</code> to see how it is implemented (do so by going to “File-&gt;Open…” at the top of the Jupyter notebook. This opens the file directory that contains the ‘.py’ file).</p><p>The key things you need to know are:</p><ul><li>This network uses 96x96 dimensional RGB images as its input. Specifically, inputs a face image (or batch of $m$ face images) as a tensor of shape $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$</li><li>It outputs a matrix of shape $(m, 128)$ that encodes each input face image into a 128-dimensional vector</li></ul><p>Run the cell below to create the model for face images.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FRmodel = faceRecoModel(input_shape=(<span class="number">3</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Total Params:"</span>, FRmodel.count_params())</span><br><span class="line"><span class="comment"># Total Params: 3743280</span></span><br></pre></td></tr></table></figure><p>By using a 128-neuron fully connected layer as its last layer, the model ensures that the output is an encoding vector of size 128. You then use the encodings to compare two face images as follows:</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584104899569.png" alt="1584104899569"></p><p>So, an encoding is a good one if:</p><ul><li>The encodings of two images of the same person are quite similar to each other.</li><li>The encodings of two images of different persons are very different.</li></ul><p>The triplet loss function formalizes this, and tries to “push” the encodings of two images of the same person (Anchor and Positive) closer together, while “pulling” the encodings of two images of different persons (Anchor, Negative) further apart.</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584105010151.png" alt="1584105010151"></p><h4 id="1-2-The-Triplet-Loss"><a href="#1-2-The-Triplet-Loss" class="headerlink" title="1.2 - The Triplet Loss"></a>1.2 - The Triplet Loss</h4><p>For an image $x$, we denote its encoding $f(x)$, where $f$ is the function computed by the neural network.</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584105106919.png" alt="1584105106919"></p><p>Training will use triplets of images $(A, P, N)$:</p><ul><li>A is an “Anchor” image—a picture of a person.</li><li>P is a “Positive” image—a picture of the same person as the Anchor image.</li><li>N is a “Negative” image—a picture of a different person than the Anchor image.</li></ul><p>These triplets are picked from our training dataset. We will write $(A^{(i)}, P^{(i)}, N^{(i)})$ to denote the $i$-th training example.</p><p>You’d like to make sure that an image $A^{(i)}$ of an individual is closer to the Positive $P^{(i)}$ than to the Negative image $N^{(i)}$) by at least a margin $\alpha$:</p><script type="math/tex;mode=display">\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2 + \alpha < \mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2</script><p>You would thus like to minimize the following “triplet cost”:</p><script type="math/tex;mode=display">\mathcal{J} = \sum^{m}_{i=1} \large[ \small \underbrace{\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2}_\text{(1)} - \underbrace{\mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2}_\text{(2)} + \alpha \large ] \small_+ \tag{3}</script><p>Here, we are using the notation “$[z]_+$” to denote $max(z,0)$.</p><p>Notes:</p><ul><li>The term (1) is the squared distance between the anchor “A” and the positive “P” for a given triplet; you want this to be small.</li><li>The term (2) is the squared distance between the anchor “A” and the negative “N” for a given triplet, you want this to be relatively large. It has a minus sign preceding it because minimizing the negative of the term is the same as maximizing that term.</li><li>$\alpha$ is called the margin. It is a hyperparameter that you pick manually. We will use $\alpha = 0.2$.</li></ul><p>Most implementations also rescale the encoding vectors to haven L2 norm equal to one (i.e., $\mid \mid f(img)\mid \mid_2$=1); you won’t have to worry about that in this assignment.</p><p><strong>Exercise</strong>: Implement the triplet loss as defined by formula (3). Here are the 4 steps:</p><ol><li>Compute the distance between the encodings of “anchor” and “positive”: $\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2$</li><li>Compute the distance between the encodings of “anchor” and “negative”: $\mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2$</li><li>Compute the formula per training example: $ \mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2 - \mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2 + \alpha$</li><li>Compute the full formula by taking the max with zero and summing over the training examples:<script type="math/tex;mode=display">\mathcal{J} = \sum^{m}_{i=1} \large[ \small \mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2 - \mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2+ \alpha \large ] \small_+ \tag{3}</script></li></ol><h4 id="Hints"><a href="#Hints" class="headerlink" title="Hints"></a>Hints</h4><ul><li>Useful functions: <code>tf.reduce_sum()</code>, <code>tf.square()</code>, <code>tf.subtract()</code>, <code>tf.add()</code>, <code>tf.maximum()</code>.</li><li>For steps 1 and 2, you will sum over the entries of $\mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2$ and $\mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2$.</li><li>For step 4 you will sum over the training examples.</li></ul><h4 id="Additional-Hints"><a href="#Additional-Hints" class="headerlink" title="Additional Hints"></a>Additional Hints</h4><ul><li>Recall that the square of the L2 norm is the sum of the squared differences: $||x - y||_{2}^{2} = \sum_{i=1}^{N}(x_{i} - y_{i})^{2}$</li><li>Note that the <code>anchor</code>, <code>positive</code> and <code>negative</code> encodings are of shape <code>(m,128)</code>, where m is the number of training examples and 128 is the number of elements used to encode a single example.</li><li>For steps 1 and 2, you will maintain the number of <code>m</code> training examples and sum along the 128 values of each encoding.<br><a href="https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum" target="_blank" rel="noopener">tf.reduce_sum</a> has an <code>axis</code> parameter. This chooses along which axis the sums are applied.</li><li>Note that one way to choose the last axis in a tensor is to use negative indexing (<code>axis=-1</code>).</li><li>In step 4, when summing over training examples, the result will be a single scalar value.</li><li>For <code>tf.reduce_sum</code> to sum across all axes, keep the default value <code>axis=None</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: triplet_loss</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(y_true, y_pred, alpha = <span class="number">0.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the triplet loss as defined by formula (3)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.</span></span><br><span class="line"><span class="string">    y_pred -- python list containing three objects:</span></span><br><span class="line"><span class="string">            anchor -- the encodings for the anchor images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            positive -- the encodings for the positive images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            negative -- the encodings for the negative images, of shape (None, 128)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- real number, value of the loss</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    anchor, positive, negative = y_pred[<span class="number">0</span>], y_pred[<span class="number">1</span>], y_pred[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines)</span></span><br><span class="line">    <span class="comment"># Step 1: Compute the (encoding) distance between the anchor and the positive</span></span><br><span class="line">    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 2: Compute the (encoding) distance between the anchor and the negative</span></span><br><span class="line">    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 3: subtract the two previous distances and add alpha.</span></span><br><span class="line">    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)</span><br><span class="line">    <span class="comment"># Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.</span></span><br><span class="line">    loss = tf.reduce_sum(tf.maximum(basic_loss, <span class="number">0.</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    y_true = (<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>)</span><br><span class="line">    y_pred = (tf.random_normal([<span class="number">3</span>, <span class="number">128</span>], mean=<span class="number">6</span>, stddev=<span class="number">0.1</span>, seed = <span class="number">1</span>),</span><br><span class="line">              tf.random_normal([<span class="number">3</span>, <span class="number">128</span>], mean=<span class="number">1</span>, stddev=<span class="number">1</span>, seed = <span class="number">1</span>),</span><br><span class="line">              tf.random_normal([<span class="number">3</span>, <span class="number">128</span>], mean=<span class="number">3</span>, stddev=<span class="number">4</span>, seed = <span class="number">1</span>))</span><br><span class="line">    loss = triplet_loss(y_true, y_pred)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"loss = "</span> + str(loss.eval()))</span><br><span class="line"><span class="comment"># loss = 528.143</span></span><br></pre></td></tr></table></figure><h3 id="2-Loading-the-pre-trained-model"><a href="#2-Loading-the-pre-trained-model" class="headerlink" title="2 - Loading the pre-trained model"></a>2 - Loading the pre-trained model</h3><p>FaceNet is trained by minimizing the triplet loss. But since training requires a lot of data and a lot of computation, we won’t train it from scratch here. Instead, we load a previously trained model. Load a model using the following cell; this might take a couple of minutes to run.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FRmodel.compile(optimizer = <span class="string">'adam'</span>, loss = triplet_loss, metrics = [<span class="string">'accuracy'</span>])</span><br><span class="line">load_weights_from_FaceNet(FRmodel)</span><br></pre></td></tr></table></figure><p>Here are some examples of distances between the encodings between three individuals:</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584105966190.png" alt="1584105966190"></p><p>Let’s now use this model to perform face verification and face recognition!</p><h3 id="3-Applying-the-model"><a href="#3-Applying-the-model" class="headerlink" title="3 - Applying the model"></a>3 - Applying the model</h3><p>You are building a system for an office building where the building manager would like to offer facial recognition to allow the employees to enter the building.</p><p>You’d like to build a <strong>Face verification</strong> system that gives access to the list of people who live or work there. To get admitted, each person has to swipe an ID card (identification card) to identify themselves at the entrance. The face recognition system then checks that they are who they claim to be.</p><h4 id="3-1-Face-Verification"><a href="#3-1-Face-Verification" class="headerlink" title="3.1 - Face Verification"></a>3.1 - Face Verification</h4><p>Let’s build a database containing one encoding vector for each person who is allowed to enter the office. To generate the encoding we use <code>img_to_encoding(image_path, model)</code>, which runs the forward propagation of the model on the specified image.</p><p>Run the following code to build the database (represented as a python dictionary). This database maps each person’s name to a 128-dimensional encoding of their face.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">database = &#123;&#125;</span><br><span class="line">database[<span class="string">"danielle"</span>] = img_to_encoding(<span class="string">"images/danielle.png"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"younes"</span>] = img_to_encoding(<span class="string">"images/younes.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"tian"</span>] = img_to_encoding(<span class="string">"images/tian.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"andrew"</span>] = img_to_encoding(<span class="string">"images/andrew.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"kian"</span>] = img_to_encoding(<span class="string">"images/kian.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"dan"</span>] = img_to_encoding(<span class="string">"images/dan.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"sebastiano"</span>] = img_to_encoding(<span class="string">"images/sebastiano.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"bertrand"</span>] = img_to_encoding(<span class="string">"images/bertrand.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"kevin"</span>] = img_to_encoding(<span class="string">"images/kevin.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"felix"</span>] = img_to_encoding(<span class="string">"images/felix.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"benoit"</span>] = img_to_encoding(<span class="string">"images/benoit.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"arnaud"</span>] = img_to_encoding(<span class="string">"images/arnaud.jpg"</span>, FRmodel)</span><br></pre></td></tr></table></figure><p>Now, when someone shows up at your front door and swipes their ID card (thus giving you their name), you can look up their encoding in the database, and use it to check if the person standing at the front door matches the name on the ID.</p><p><strong>Exercise</strong>: Implement the verify() function which checks if the front-door camera picture (<code>image_path</code>) is actually the person called “identity”. You will have to go through the following steps:</p><ol><li>Compute the encoding of the image from <code>image_path</code>.</li><li>Compute the distance between this encoding and the encoding of the identity image stored in the database.</li><li>Open the door if the distance is less than 0.7, else do not open it.</li></ol><ul><li>As presented above, you should use the L2 distance <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html" target="_blank" rel="noopener">np.linalg.norm</a>.</li><li>(Note: In this implementation, compare the L2 distance, not the square of the L2 distance, to the threshold 0.7.)</li></ul><h4 id="Hints-1"><a href="#Hints-1" class="headerlink" title="Hints"></a>Hints</h4><ul><li><code>identity</code> is a string that is also a key in the <code>database</code> dictionary.</li><li><code>img_to_encoding</code> has two parameters: the <code>image_path</code> and <code>model</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: verify</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verify</span><span class="params">(image_path, identity, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function that verifies if the person on the "image_path" image is "identity".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.</span></span><br><span class="line"><span class="string">    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dist -- distance between the image_path and the image of "identity" in the database.</span></span><br><span class="line"><span class="string">    door_open -- True, if the door should open. False otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)</span></span><br><span class="line">    encoding = img_to_encoding(image_path, model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute distance with identity's image (≈ 1 line)</span></span><br><span class="line">    dist = np.linalg.norm(encoding - database[identity])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Open the door if dist &lt; 0.7, else don't open (≈ 3 lines)</span></span><br><span class="line">    <span class="keyword">if</span> dist &lt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"It's "</span> + str(identity) + <span class="string">", welcome in!"</span>)</span><br><span class="line">        door_open = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"It's not "</span> + str(identity) + <span class="string">", please go away"</span>)</span><br><span class="line">        door_open = <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dist, door_open</span><br></pre></td></tr></table></figure><p>Younes is trying to enter the office and the camera takes a picture of him (“images/camera_0.jpg”). Let’s run your verification algorithm on this picture:</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584106392783.png" alt="1584106392783"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">verify(<span class="string">"images/camera_0.jpg"</span>, <span class="string">"younes"</span>, database, FRmodel)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584106340980.png" alt="1584106340980"></p><p>Benoit, who does not work in the office, stole Kian’s ID card and tried to enter the office. The camera took a picture of Benoit (“images/camera_2.jpg). Let’s run the verification algorithm to check if benoit can enter.</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584106376027.png" alt="1584106376027"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">verify(<span class="string">"images/camera_2.jpg"</span>, <span class="string">"kian"</span>, database, FRmodel)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584106418445.png" alt="1584106418445"></p><h4 id="3-2-Face-Recognition"><a href="#3-2-Face-Recognition" class="headerlink" title="3.2 - Face Recognition"></a>3.2 - Face Recognition</h4><p>Your face verification system is mostly working well. But since Kian got his ID card stolen, when he came back to the office the next day and couldn’t get in!</p><p>To solve this, you’d like to change your face verification system to a face recognition system. This way, no one has to carry an ID card anymore. An authorized person can just walk up to the building, and the door will unlock for them!</p><p>You’ll implement a face recognition system that takes as input an image, and figures out if it is one of the authorized persons (and if so, who). Unlike the previous face verification system, we will no longer get a person’s name as one of the inputs.</p><p><strong>Exercise</strong>: Implement <code>who_is_it()</code>. You will have to go through the following steps:</p><ol><li><p>Compute the target encoding of the image from image_path</p></li><li><p>Find the encoding from the database that has smallest distance with the target encoding.</p><ul><li><p>Initialize the <code>min_dist</code> variable to a large enough number (100). It will help you keep track of what is the closest encoding to the input’s encoding.</p></li><li><p>Loop over the database dictionary’s names and encodings. To loop use</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for (name, db_enc) in database.items()</span><br></pre></td></tr></table></figure><ul><li>Compute the L2 distance between the target “encoding” and the current “encoding” from the database.</li><li>If this distance is less than the min_dist, then set <code>min_dist</code> to <code>dist</code>, and <code>identity</code> to <code>name</code>.</li></ul></li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: who_is_it</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">who_is_it</span><span class="params">(image_path, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements face recognition for the office by finding who is the person on the image_path image.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    database -- database containing image encodings along with the name of the person on the image</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    min_dist -- the minimum distance between image_path encoding and the encodings from the database</span></span><br><span class="line"><span class="string">    identity -- string, the name prediction for the person on image_path</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 1: Compute the target "encoding" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)</span></span><br><span class="line">    encoding = img_to_encoding(image_path, model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2: Find the closest encoding ##</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "min_dist" to a large value, say 100 (≈1 line)</span></span><br><span class="line">    min_dist = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over the database dictionary's names and encodings.</span></span><br><span class="line">    <span class="keyword">for</span> (name, db_enc) <span class="keyword">in</span> database.items():</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute L2 distance between the target "encoding" and the current db_enc from the database. (≈ 1 line)</span></span><br><span class="line">        dist = np.linalg.norm(encoding - db_enc)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">            min_dist = dist</span><br><span class="line">            identity = name</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> min_dist &gt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"Not in the database."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"it's "</span> + str(identity) + <span class="string">", the distance is "</span> + str(min_dist))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> min_dist, identity</span><br></pre></td></tr></table></figure><p>Younes is at the front-door and the camera takes a picture of him (“images/camera_0.jpg”). Let’s see if your who_it_is() algorithm identifies Younes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">who_is_it(<span class="string">"images/camera_0.jpg"</span>, database, FRmodel)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584106593459.png" alt="1584106593459"></p><p>You can change “<code>camera_0.jpg</code>“ (picture of younes) to “<code>camera_1.jpg</code>“ (picture of bertrand) and see the result.</p><h4 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations!"></a>Congratulations!</h4><ul><li>Your face recognition system is working well! It only lets in authorized persons, and people don’t need to carry an ID card around anymore!</li><li>You’ve now seen how a state-of-the-art face recognition system works.</li></ul><h4 id="Ways-to-improve-your-facial-recognition-model"><a href="#Ways-to-improve-your-facial-recognition-model" class="headerlink" title="Ways to improve your facial recognition model"></a>Ways to improve your facial recognition model</h4><p>Although we won’t implement it here, here are some ways to further improve the algorithm:</p><ul><li>Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then given a new image, compare the new face to multiple pictures of the person. This would increase accuracy.</li><li><p>Crop the images to just contain the face, and less of the “border” region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust.</p><p>Congrats on finishing this assignment!</p></li></ul><h3 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h3><ul><li>Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). <a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="noopener">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></li><li>Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014). <a href="https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf" target="_blank" rel="noopener">DeepFace: Closing the gap to human-level performance in face verification</a></li><li>The pretrained model we use is inspired by Victor Sy Wang’s implementation and was loaded using his code: <a href="https://github.com/iwantooxxoox/Keras-OpenFace" target="_blank" rel="noopener">https://github.com/iwantooxxoox/Keras-OpenFace</a>.</li><li>Our implementation also took a lot of inspiration from the official FaceNet github repository: <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">https://github.com/davidsandberg/facenet</a></li></ul><h2 id="Deep-Learning-amp-Art-Neural-Style-Transfer"><a href="#Deep-Learning-amp-Art-Neural-Style-Transfer" class="headerlink" title="Deep Learning &amp; Art: Neural Style Transfer"></a>Deep Learning &amp; Art: Neural Style Transfer</h2><p>In this assignment, you will learn about Neural Style Transfer. This algorithm was created by <a href="https://arxiv.org/abs/1508.06576" target="_blank" rel="noopener">Gatys et al. (2015).</a></p><p><strong>In this assignment, you will:</strong></p><ul><li>Implement the neural style transfer algorithm</li><li>Generate novel artistic images using your algorithm</li></ul><p>Most of the algorithms you’ve studied optimize a cost function to get a set of parameter values. In Neural Style Transfer, you’ll optimize a cost function to get pixel values!</p><h3 id="Updates-1"><a href="#Updates-1" class="headerlink" title="Updates"></a>Updates</h3><h4 id="If-you-were-working-on-the-notebook-before-this-update…-1"><a href="#If-you-were-working-on-the-notebook-before-this-update…-1" class="headerlink" title="If you were working on the notebook before this update…"></a>If you were working on the notebook before this update…</h4><ul><li>The current notebook is version “3a”.</li><li>You can find your original work saved in the notebook with the previous version name (“v2”)</li><li>To view the file directory, go to the menu “File-&gt;Open”, and this will open a new tab that shows the file directory.</li></ul><h4 id="List-of-updates-1"><a href="#List-of-updates-1" class="headerlink" title="List of updates"></a>List of updates</h4><ul><li>Use <code>pprint.PrettyPrinter</code> to format printing of the vgg model.</li><li>computing content cost: clarified and reformatted instructions, fixed broken links, added additional hints for unrolling.</li><li>style matrix: clarify two uses of variable “G” by using different notation for gram matrix.</li><li>style cost: use distinct notation for gram matrix, added additional hints.</li><li>Grammar and wording updates for clarity.</li><li><code>model_nn</code>: added hints.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> nst_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> pprint</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h3 id="1-Problem-Statement"><a href="#1-Problem-Statement" class="headerlink" title="1 - Problem Statement"></a>1 - Problem Statement</h3><p>Neural Style Transfer (NST) is one of the most fun techniques in deep learning. As seen below, it merges two images, namely: a <strong>“content” image (C) and a “style” image (S), to create a “generated” image (G</strong>).</p><p>The generated image G combines the “content” of the image C with the “style” of image S.</p><p>In this example, you are going to generate an image of the Louvre museum in Paris (content image C), mixed with a painting by Claude Monet, a leader of the impressionist movement (style image S).</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584081693904.png" alt="1584081693904"></p><p>Let’s see how you can do this.</p><h3 id="2-Transfer-Learning"><a href="#2-Transfer-Learning" class="headerlink" title="2 - Transfer Learning"></a>2 - Transfer Learning</h3><p>Neural Style Transfer (NST) uses a previously trained convolutional network, and builds on top of that. The idea of using a network trained on a different task and applying it to a new task is called transfer learning.</p><p>Following the <a href="https://arxiv.org/abs/1508.06576" target="_blank" rel="noopener">original NST paper</a>, we will use the VGG network. Specifically, we’ll use VGG-19, a 19-layer version of the VGG network. This model has already been trained on the very large ImageNet database, and thus has learned to recognize a variety of low level features (at the shallower layers) and high level features (at the deeper layers).</p><p>Run the following code to load parameters from the VGG model. This may take a few seconds.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pp = pprint.PrettyPrinter(indent=<span class="number">4</span>)</span><br><span class="line">model = load_vgg_model(<span class="string">"pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br><span class="line">pp.pprint(model)</span><br></pre></td></tr></table></figure><ul><li>The model is stored in a python dictionary.</li><li>The python dictionary contains key-value pairs for each layer.</li><li>The ‘key’ is the variable name and the ‘value’ is a tensor for that layer.</li></ul><h4 id="Assign-input-image-to-the-model’s-input-layer"><a href="#Assign-input-image-to-the-model’s-input-layer" class="headerlink" title="Assign input image to the model’s input layer"></a>Assign input image to the model’s input layer</h4><p>To run an image through this network, you just have to feed the image to the model. In TensorFlow, you can do so using the <a href="https://www.tensorflow.org/api_docs/python/tf/assign" target="_blank" rel="noopener">tf.assign</a> function. In particular, you will use the assign function like this:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model[<span class="string">"input"</span>].assign(image)</span><br></pre></td></tr></table></figure><p></p><p>This assigns the image as an input to the model.</p><h4 id="Activate-a-layer"><a href="#Activate-a-layer" class="headerlink" title="Activate a layer"></a>Activate a layer</h4><p>After this, if you want to access the activations of a particular layer, say layer <code>4_2</code> when the network is run on this image, you would run a TensorFlow session on the correct tensor <code>conv4_2</code>, as follows:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(model[<span class="string">"conv4_2"</span>])</span><br></pre></td></tr></table></figure><p></p><h3 id="3-Neural-Style-Transfer-NST"><a href="#3-Neural-Style-Transfer-NST" class="headerlink" title="3 - Neural Style Transfer (NST)"></a>3 - Neural Style Transfer (NST)</h3><p>We will build the Neural Style Transfer (NST) algorithm in three steps:</p><ul><li>Build the content cost function $J_{content}(C,G)$</li><li>Build the style cost function $J_{style}(S,G)$</li><li>Put it together to get $J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)$.</li></ul><h4 id="3-1-Computing-the-content-cost"><a href="#3-1-Computing-the-content-cost" class="headerlink" title="3.1 - Computing the content cost"></a>3.1 - Computing the content cost</h4><p>In our running example, the content image C will be the picture of the Louvre Museum in Paris. Run the code below to see a picture of the Louvre.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">content_image = scipy.misc.imread(<span class="string">"images/louvre.jpg"</span>)</span><br><span class="line">imshow(content_image);</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584083143450.png" alt="1584083143450"></p><p>The content image (C) shows the Louvre museum’s pyramid surrounded by old Paris buildings, against a sunny sky with a few clouds.</p><p><strong>3.1.1 - Make generated image G match the content of image C</strong></p><h5 id="Shallower-versus-deeper-layers"><a href="#Shallower-versus-deeper-layers" class="headerlink" title="Shallower versus deeper layers"></a>Shallower versus deeper layers</h5><ul><li>The shallower layers of a ConvNet tend to detect lower-level features such as edges and simple textures.</li><li>The deeper layers tend to detect higher-level features such as more complex textures as well as object classes.</li></ul><h5 id="Choose-a-“middle”-activation-layer-a-l"><a href="#Choose-a-“middle”-activation-layer-a-l" class="headerlink" title="Choose a “middle” activation layer $a^{[l]}$"></a>Choose a “middle” activation layer $a^{[l]}$</h5><p>We would like the “generated” image G to have similar content as the input image C. Suppose you have chosen some layer’s activations to represent the content of an image.</p><ul><li>In practice, you’ll get the most visually pleasing results if you choose a layer in the <strong>middle</strong> of the network—neither too shallow nor too deep.</li><li>(After you have finished this exercise, feel free to come back and experiment with using different layers, to see how the results vary.)</li></ul><h5 id="Forward-propagate-image-“C”"><a href="#Forward-propagate-image-“C”" class="headerlink" title="Forward propagate image “C”"></a>Forward propagate image “C”</h5><ul><li>Set the image C as the input to the pretrained VGG network, and run forward propagation.</li><li>Let $a^{(C)}$ be the hidden layer activations in the layer you had chosen. (In lecture, we had written this as $a^{<a href="C">l</a>}$, but here we’ll drop the superscript $[l]$ to simplify the notation.) This will be an $n_H \times n_W \times n_C$ tensor.</li></ul><h5 id="Forward-propagate-image-“G”"><a href="#Forward-propagate-image-“G”" class="headerlink" title="Forward propagate image “G”"></a>Forward propagate image “G”</h5><ul><li>Repeat this process with the image G: Set G as the input, and run forward progation.</li><li>Let $a^{(G)}$ be the corresponding hidden layer activation.</li></ul><h5 id="Content-Cost-Function-J-content-C-G"><a href="#Content-Cost-Function-J-content-C-G" class="headerlink" title="Content Cost Function $J_{content}(C,G)$"></a>Content Cost Function $J_{content}(C,G)$</h5><p>We will define the content cost function as:</p><script type="math/tex;mode=display">J_{content}(C,G) =  \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2\tag{1}</script><ul><li>Here, $n_H, n_W$ and $n_C$ are the height, width and number of channels of the hidden layer you have chosen, and appear in a normalization term in the cost.</li><li>For clarity, note that $a^{(C)}$ and $a^{(G)}$ are the 3D volumes corresponding to a hidden layer’s activations.</li><li>In order to compute the cost $J_{content}(C,G)$, it might also be convenient to unroll these 3D volumes into a 2D matrix, as shown below.</li><li>Technically this unrolling step isn’t needed to compute $J_{content}$, but it will be good practice for when you do need to carry out a similar operation later for computing the style cost $J_{style}$.</li></ul><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584083273358.png" alt="1584083273358"></p><p><strong>Exercise:</strong> Compute the “content cost” using TensorFlow.</p><p><strong>Instructions</strong>: The 3 steps to implement this function are:</p><ol><li>Retrieve dimensions from <code>a_G</code>:<ul><li>To retrieve dimensions from a tensor <code>X</code>, use: <code>X.get_shape().as_list()</code></li></ul></li><li>Unroll <code>a_C</code> and <code>a_G</code> as explained in the picture above<ul><li>You’ll likey want to use these functions: <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/transpose" target="_blank" rel="noopener">tf.transpose</a> and <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/reshape" target="_blank" rel="noopener">tf.reshape</a>.</li></ul></li><li>Compute the content cost:<ul><li>You’ll likely want to use these functions: <a href="https://www.tensorflow.org/api_docs/python/tf/reduce_sum" target="_blank" rel="noopener">tf.reduce_sum</a>, <a href="https://www.tensorflow.org/api_docs/python/tf/square" target="_blank" rel="noopener">tf.square</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/subtract" target="_blank" rel="noopener">tf.subtract</a>.</li></ul></li></ol><h5 id="Additional-Hints-for-“Unrolling”"><a href="#Additional-Hints-for-“Unrolling”" class="headerlink" title="Additional Hints for “Unrolling”"></a>Additional Hints for “Unrolling”</h5><ul><li>To unroll the tensor, we want the shape to change from $(m,n_H,n_W,n_C)$ to $(m, n_H \times n_W, n_C)$.</li><li><code>tf.reshape(tensor, shape)</code> takes a list of integers that represent the desired output shape.</li><li>For the <code>shape</code> parameter, a <code>-1</code> tells the function to choose the correct dimension size so that the output tensor still contains all the values of the original tensor.</li><li>So tf.reshape(a_C, shape=[m, n_H * n_W, n_C]) gives the same result as tf.reshape(a_C, shape=[m, -1, n_C]).</li><li>If you prefer to re-order the dimensions, you can use <code>tf.transpose(tensor, perm)</code>, where <code>perm</code> is a list of integers containing the original index of the dimensions.</li><li>For example, <code>tf.transpose(a_C, perm=[0,3,1,2])</code> changes the dimensions from $(m, n_H, n_W, n_C)$ to $(m, n_C, n_H, n_W)$.</li><li>There is more than one way to unroll the tensors.</li><li>Notice that it’s not necessary to use tf.transpose to ‘unroll’ the tensors in this case but this is a useful function to practice and understand for other situations that you’ll encounter.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_content_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the content cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_content -- scalar that you compute using equation 1 above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape a_C and a_G (≈2 lines)</span></span><br><span class="line">    a_C_unrolled = tf.reshape(a_C, [n_H*n_W, n_C])</span><br><span class="line">    a_G_unrolled = tf.reshape(a_G, [n_H*n_W, n_C])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute the cost with tensorflow (≈1 line)</span></span><br><span class="line">    J_content = <span class="number">1.</span>/(<span class="number">4</span> * n_H * n_W * n_C)*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled)))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_content</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    a_C = tf.random_normal([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>)</span><br><span class="line">    a_G = tf.random_normal([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>)</span><br><span class="line">    J_content = compute_content_cost(a_C, a_G)</span><br><span class="line">    print(<span class="string">"J_content = "</span> + str(J_content.eval()))</span><br><span class="line"><span class="comment"># J_content = 6.76559</span></span><br></pre></td></tr></table></figure><h5 id="What-you-should-remember"><a href="#What-you-should-remember" class="headerlink" title="What you should remember"></a>What you should remember</h5><ul><li>The content cost takes a hidden layer activation of the neural network, and measures how different $a^{(C)}$ and $a^{(G)}$ are.</li><li>When we minimize the content cost later, this will help make sure $G$ has similar content as $C$.</li></ul><h4 id="3-2-Computing-the-style-cost"><a href="#3-2-Computing-the-style-cost" class="headerlink" title="3.2 - Computing the style cost"></a>3.2 - Computing the style cost</h4><p>For our running example, we will use the following style image:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">style_image = scipy.misc.imread(<span class="string">"images/monet_800600.jpg"</span>)</span><br><span class="line">imshow(style_image);</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584084274530.png" alt="1584084274530"></p><p>This was painted in the style of <em><a href="https://en.wikipedia.org/wiki/Impressionism" target="_blank" rel="noopener">impressionism</a></em>.</p><p>Lets see how you can now define a “style” cost function $J_{style}(S,G)$.</p><h5 id="3-2-1-Style-matrix"><a href="#3-2-1-Style-matrix" class="headerlink" title="3.2.1 - Style matrix"></a>3.2.1 - Style matrix</h5><h6 id="Gram-matrix"><a href="#Gram-matrix" class="headerlink" title="Gram matrix"></a>Gram matrix</h6><ul><li>The style matrix is also called a “Gram matrix.”</li><li>In linear algebra, the Gram matrix G of a set of vectors $(v_{1},\dots ,v_{n})$ is the matrix of dot products, whose entries are ${\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j}) }$.</li><li>In other words, $G_{ij}$ compares how similar $v_i$ is to $v_j$: If they are highly similar, you would expect them to have a large dot product, and thus for $G_{ij}$ to be large.</li></ul><h6 id="Two-meanings-of-the-variable-G"><a href="#Two-meanings-of-the-variable-G" class="headerlink" title="Two meanings of the variable $G$"></a>Two meanings of the variable $G$</h6><ul><li>Note that there is an unfortunate collision in the variable names used here. We are following common terminology used in the literature.</li><li>$G$ is used to denote the Style matrix (or Gram matrix)</li><li>$G$ also denotes the generated image.</li><li>For this assignment, we will use $G_{gram}$ to refer to the Gram matrix, and $G$ to denote the generated image.</li></ul><h6 id="Compute-G-gram"><a href="#Compute-G-gram" class="headerlink" title="Compute $G_{gram}$"></a>Compute $G_{gram}$</h6><p>In Neural Style Transfer (NST), you can compute the Style matrix by multiplying the “unrolled” filter matrix with its transpose:</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584084356557.png" alt="1584084356557"></p><script type="math/tex;mode=display">\mathbf{G}_{gram} = \mathbf{A}_{unrolled} \mathbf{A}_{unrolled}^T</script><h6 id="G-gram-i-j-correlation"><a href="#G-gram-i-j-correlation" class="headerlink" title="$G_{(gram)i,j}$: correlation"></a>$G_{(gram)i,j}$: correlation</h6><p>The result is a matrix of dimension $(n_C,n_C)$ where $n_C$ is the number of filters (channels). The value $G_{(gram)i,j}$ measures how similar the activations of filter $i$ are to the activations of filter $j$.</p><h6 id="G-gram-i-i-prevalence-of-patterns-or-textures"><a href="#G-gram-i-i-prevalence-of-patterns-or-textures" class="headerlink" title="$G_{(gram),i,i}$: prevalence of patterns or textures"></a>$G_{(gram),i,i}$: prevalence of patterns or textures</h6><ul><li>The diagonal elements $G_{(gram)ii}$ measure how “active” a filter $i$ is.</li><li>For example, suppose filter $i$ is detecting vertical textures in the image. Then $G_{(gram)ii}$ measures how common vertical textures are in the image as a whole.</li><li>If $G_{(gram)ii}$ is large, this means that the image has a lot of vertical texture.</li></ul><p>By capturing the prevalence of different types of features ($G_{(gram)ii}$), as well as how much different features occur together ($G_{(gram)ij}$), the Style matrix $G_{gram}$ measures the style of an image.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: gram_matrix</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    A -- matrix of shape (n_C, n_H*n_W)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    GA -- Gram matrix of A, of shape (n_C, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    GA = tf.matmul(A, tf.transpose(A))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> GA</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    A = tf.random_normal([<span class="number">3</span>, <span class="number">2</span>*<span class="number">1</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>)</span><br><span class="line">    GA = gram_matrix(A)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"GA = \n"</span> + str(GA.eval()))</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584085275933.png" alt="1584085275933"></p><h5 id="3-2-2-Style-cost"><a href="#3-2-2-Style-cost" class="headerlink" title="3.2.2 - Style cost"></a>3.2.2 - Style cost</h5><p>Your goal will be to minimize the distance between the Gram matrix of the “style” image S and the gram matrix of the “generated” image G.</p><ul><li>For now, we are using only a single hidden layer $a^{[l]}$.</li><li>The corresponding style cost for this layer is defined as:</li></ul><script type="math/tex;mode=display">J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\tag{2}</script><ul><li>$G_{gram}^{(S)}$ Gram matrix of the “style” image.</li><li>$G_{gram}^{(G)}$ Gram matrix of the “generated” image.</li><li>Remember, this cost is computed using the hidden layer activations for a particular hidden layer in the network $a^{[l]}$</li></ul><p><strong>Exercise</strong>: Compute the style cost for a single layer.</p><p><strong>Instructions</strong>: The 3 steps to implement this function are:</p><ol><li>Retrieve dimensions from the hidden layer activations a_G:<ul><li>To retrieve dimensions from a tensor X, use: <code>X.get_shape().as_list()</code></li></ul></li><li>Unroll the hidden layer activations a_S and a_G into 2D matrices, as explained in the picture above (see the images in the sections “computing the content cost” and “style matrix”).<ul><li>You may use <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/transpose" target="_blank" rel="noopener">tf.transpose</a> and <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/reshape" target="_blank" rel="noopener">tf.reshape</a>.</li></ul></li><li>Compute the Style matrix of the images S and G. (Use the function you had previously written.)</li><li>Compute the Style cost:<ul><li>You may find <a href="https://www.tensorflow.org/api_docs/python/tf/reduce_sum" target="_blank" rel="noopener">tf.reduce_sum</a>, <a href="https://www.tensorflow.org/api_docs/python/tf/square" target="_blank" rel="noopener">tf.square</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/subtract" target="_blank" rel="noopener">tf.subtract</a> useful.</li></ul></li></ol><h5 id="Additional-Hints-1"><a href="#Additional-Hints-1" class="headerlink" title="Additional Hints"></a>Additional Hints</h5><ul><li>Since the activation dimensions are $(m, n_H, n_W, n_C)$ whereas the desired unrolled matrix shape is $(n_C, n_H*n_W)$, the order of the filter dimension $n_C$ is changed. So <code>tf.transpose</code> can be used to change the order of the filter dimension.</li><li>for the product $\mathbf{G}_{gram} = \mathbf{A}_{} \mathbf{A}_{}^T$, you will also need to specify the <code>perm</code> parameter for the <code>tf.transpose</code> function.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_layer_style_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)</span></span><br><span class="line">    a_S = tf.transpose(tf.reshape(a_S, [n_H*n_W, n_C]))</span><br><span class="line">    a_G = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing gram_matrices for both images S and G (≈2 lines)</span></span><br><span class="line">    GS = gram_matrix(a_S)</span><br><span class="line">    GG = gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing the loss (≈1 line)</span></span><br><span class="line">    J_style_layer = <span class="number">1.</span>/(<span class="number">4</span> * n_C * n_C * n_H * n_W * n_H * n_W) * tf.reduce_sum(tf.square(tf.subtract(GS, GG)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_style_layer</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    a_S = tf.random_normal([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>)</span><br><span class="line">    a_G = tf.random_normal([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>)</span><br><span class="line">    J_style_layer = compute_layer_style_cost(a_S, a_G)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"J_style_layer = "</span> + str(J_style_layer.eval()))</span><br><span class="line"><span class="comment"># J_style_layer = 9.19028</span></span><br></pre></td></tr></table></figure><h5 id="3-2-3-Style-Weights"><a href="#3-2-3-Style-Weights" class="headerlink" title="3.2.3 Style Weights"></a>3.2.3 Style Weights</h5><ul><li>So far you have captured the style from only one layer.</li><li>We’ll get better results if we “merge” style costs from several different layers.</li><li>Each layer will be given weights ($\lambda^{[l]}$) that reflect how much each layer will contribute to the style.</li><li>After completing this exercise, feel free to come back and experiment with different weights to see how it changes the generated image $G$.</li><li>By default, we’ll give each layer equal weight, and the weights add up to 1. ($\sum_{l}^L\lambda^{[l]} = 1$)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">STYLE_LAYERS = [</span><br><span class="line">    (<span class="string">'conv1_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv2_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv3_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv4_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv5_1'</span>, <span class="number">0.2</span>)]</span><br></pre></td></tr></table></figure><p>You can combine the style costs for different layers as follows:</p><script type="math/tex;mode=display">J_{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G)</script><p>where the values for $\lambda^{[l]}$ are given in <code>STYLE_LAYERS</code>.</p><h5 id="Exercise-compute-style-cost"><a href="#Exercise-compute-style-cost" class="headerlink" title="Exercise: compute style cost"></a>Exercise: compute style cost</h5><ul><li>We’ve implemented a compute_style_cost(…) function.</li><li>It calls your <code>compute_layer_style_cost(...)</code> several times, and weights their results using the values in <code>STYLE_LAYERS</code>.</li><li>Please read over it to make sure you understand what it’s doing.</li></ul><h6 id="Description-of-compute-style-cost"><a href="#Description-of-compute-style-cost" class="headerlink" title="Description of compute_style_cost"></a>Description of <code>compute_style_cost</code></h6><p>For each layer:</p><ul><li>Select the activation (the output tensor) of the current layer.</li><li>Get the style of the style image “S” from the current layer.</li><li>Get the style of the generated image “G” from the current layer.</li><li>Compute the “style cost” for the current layer</li><li>Add the weighted style cost to the overall style cost (J_style)</li></ul><p>Once you’re done with the loop:</p><ul><li>Return the overall style cost.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_style_cost</span><span class="params">(model, STYLE_LAYERS)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the overall style cost from several chosen layers</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    model -- our tensorflow model</span></span><br><span class="line"><span class="string">    STYLE_LAYERS -- A python list containing:</span></span><br><span class="line"><span class="string">                        - the names of the layers we would like to extract style from</span></span><br><span class="line"><span class="string">                        - a coefficient for each of them</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_style -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the overall style cost</span></span><br><span class="line">    J_style = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> layer_name, coeff <span class="keyword">in</span> STYLE_LAYERS:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select the output tensor of the currently selected layer</span></span><br><span class="line">        out = model[layer_name]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out</span></span><br><span class="line">        a_S = sess.run(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] </span></span><br><span class="line">        <span class="comment"># and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that</span></span><br><span class="line">        <span class="comment"># when we run the session, this will be the activations drawn from the appropriate layer, with G as input.</span></span><br><span class="line">        a_G = out</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute style_cost for the current layer</span></span><br><span class="line">        J_style_layer = compute_layer_style_cost(a_S, a_G)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add coeff * J_style_layer of this layer to overall style cost</span></span><br><span class="line">        J_style += coeff * J_style_layer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> J_style</span><br></pre></td></tr></table></figure><p><strong>Note</strong>: In the inner-loop of the for-loop above, <code>a_G</code> is a tensor and hasn’t been evaluated yet. It will be evaluated and updated at each iteration when we run the TensorFlow graph in model_nn() below.</p><h5 id="What-you-should-remember-1"><a href="#What-you-should-remember-1" class="headerlink" title="What you should remember"></a>What you should remember</h5><ul><li>The style of an image can be represented using the Gram matrix of a hidden layer’s activations.</li><li>We get even better results by combining this representation from multiple different layers.</li><li>This is in contrast to the content representation, where usually using just a single hidden layer is sufficient.</li><li>Minimizing the style cost will cause the image $G$ to follow the style of the image $S$.</li></ul><h4 id="3-3-Defining-the-total-cost-to-optimize"><a href="#3-3-Defining-the-total-cost-to-optimize" class="headerlink" title="3.3 - Defining the total cost to optimize"></a>3.3 - Defining the total cost to optimize</h4><p>Finally, let’s create a cost function that minimizes both the style and the content cost. The formula is:</p><script type="math/tex;mode=display">J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)</script><p><strong>Exercise</strong>: Implement the total cost function which includes both the content cost and the style cost.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: total_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the total cost function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    J_content -- content cost coded above</span></span><br><span class="line"><span class="string">    J_style -- style cost coded above</span></span><br><span class="line"><span class="string">    alpha -- hyperparameter weighting the importance of the content cost</span></span><br><span class="line"><span class="string">    beta -- hyperparameter weighting the importance of the style cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- total cost as defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    J = alpha * J_content + beta * J_style</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    J_content = np.random.randn()    </span><br><span class="line">    J_style = np.random.randn()</span><br><span class="line">    J = total_cost(J_content, J_style)</span><br><span class="line">    print(<span class="string">"J = "</span> + str(J))</span><br></pre></td></tr></table></figure><h5 id="What-you-should-remember-2"><a href="#What-you-should-remember-2" class="headerlink" title="What you should remember"></a>What you should remember</h5><ul><li>The total cost is a linear combination of the content cost $J_{content}(C,G)$ and the style cost $J_{style}(S,G)$.</li><li>$\alpha$ and $\beta$ are hyperparameters that control the relative weighting between content and style.</li></ul><h3 id="4-Solving-the-optimization-problem"><a href="#4-Solving-the-optimization-problem" class="headerlink" title="4 - Solving the optimization problem"></a>4 - Solving the optimization problem</h3><p>Finally, let’s put everything together to implement Neural Style Transfer!</p><p>Here’s what the program will have to do:</p><ol><li>Create an Interactive Session</li><li>Load the content image</li><li>Load the style image</li><li>Randomly initialize the image to be generated</li><li>Load the VGG19 model</li><li>Build the TensorFlow graph:<ul><li>Run the content image through the VGG19 model and compute the content cost</li><li>Run the style image through the VGG19 model and compute the style cost</li><li>Compute the total cost</li><li>Define the optimizer and the learning rate</li></ul></li><li>Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step.</li></ol><p>Lets go through the individual steps in detail.</p><h4 id="Interactive-Sessions"><a href="#Interactive-Sessions" class="headerlink" title="Interactive Sessions"></a>Interactive Sessions</h4><p>You’ve previously implemented the overall cost $J(G)$. We’ll now set up TensorFlow to optimize this with respect to $G$.</p><ul><li>To do so, your program has to reset the graph and use an “<a href="https://www.tensorflow.org/api_docs/python/tf/InteractiveSession" target="_blank" rel="noopener">Interactive Session</a>“.</li><li>Unlike a regular session, the “Interactive Session” installs itself as the default session to build a graph.</li><li>This allows you to run variables without constantly needing to refer to the session object (calling “sess.run()”), which simplifies the code.</li></ul><h4 id="Start-the-interactive-session"><a href="#Start-the-interactive-session" class="headerlink" title="Start the interactive session."></a>Start the interactive session.</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reset the graph</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start interactive session</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure><h4 id="Content-image"><a href="#Content-image" class="headerlink" title="Content image"></a>Content image</h4><p>Let’s load, reshape, and normalize our “content” image (the Louvre museum picture):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">content_image = scipy.misc.imread(<span class="string">"images/louvre_small.jpg"</span>)</span><br><span class="line">content_image = reshape_and_normalize_image(content_image)</span><br></pre></td></tr></table></figure><h4 id="Style-image"><a href="#Style-image" class="headerlink" title="Style image"></a>Style image</h4><p>Let’s load, reshape and normalize our “style” image (Claude Monet’s painting):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">style_image = scipy.misc.imread(<span class="string">"images/monet.jpg"</span>)</span><br><span class="line">style_image = reshape_and_normalize_image(style_image)</span><br></pre></td></tr></table></figure><h4 id="Generated-image-correlated-with-content-image"><a href="#Generated-image-correlated-with-content-image" class="headerlink" title="Generated image correlated with content image"></a>Generated image correlated with content image</h4><p>Now, we initialize the “generated” image as a noisy image created from the content_image.</p><ul><li>The generated image is slightly correlated with the content image.</li><li>By initializing the pixels of the generated image to be mostly noise but slightly correlated with the content image, this will help the content of the “generated” image more rapidly match the content of the “content” image.</li><li>Feel free to look in <code>nst_utils.py</code> to see the details of <code>generate_noise_image(...)</code>; to do so, click “File—&gt;Open…” at the upper-left corner of this Jupyter notebook.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">generated_image = generate_noise_image(content_image)</span><br><span class="line">imshow(generated_image[<span class="number">0</span>]);</span><br></pre></td></tr></table></figure><h4 id="Load-pre-trained-VGG19-model"><a href="#Load-pre-trained-VGG19-model" class="headerlink" title="Load pre-trained VGG19 model"></a>Load pre-trained VGG19 model</h4><p>Next, as explained in part (2), let’s load the VGG19 model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = load_vgg_model(<span class="string">"pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584087836172.png" alt="1584087836172"></p><h4 id="Content-Cost"><a href="#Content-Cost" class="headerlink" title="Content Cost"></a>Content Cost</h4><p>To get the program to compute the content cost, we will now assign <code>a_C</code> and <code>a_G</code> to be the appropriate hidden layer activations. We will use layer <code>conv4_2</code> to compute the content cost. The code below does the following:</p><ol><li>Assign the content image to be the input to the VGG model.</li><li>Set a_C to be the tensor giving the hidden layer activation for layer “conv4_2”.</li><li>Set a_G to be the tensor giving the hidden layer activation for the same layer.</li><li>Compute the content cost using a_C and a_G.</li></ol><p><strong>Note</strong>: At this point, a_G is a tensor and hasn’t been evaluated. It will be evaluated and updated at each iteration when we run the Tensorflow graph in model_nn() below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign the content image to be the input of the VGG model.  </span></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(content_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select the output tensor of layer conv4_2</span></span><br><span class="line">out = model[<span class="string">'conv4_2'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a_C to be the hidden layer activation from the layer we have selected</span></span><br><span class="line">a_C = sess.run(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] </span></span><br><span class="line"><span class="comment"># and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that</span></span><br><span class="line"><span class="comment"># when we run the session, this will be the activations drawn from the appropriate layer, with G as input.</span></span><br><span class="line">a_G = out</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the content cost</span></span><br><span class="line">J_content = compute_content_cost(a_C, a_G)</span><br></pre></td></tr></table></figure><h4 id="Style-cost"><a href="#Style-cost" class="headerlink" title="Style cost"></a>Style cost</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign the input of the model to be the "style" image </span></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(style_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the style cost</span></span><br><span class="line">J_style = compute_style_cost(model, STYLE_LAYERS)</span><br></pre></td></tr></table></figure><h4 id="Exercise-total-cost"><a href="#Exercise-total-cost" class="headerlink" title="Exercise: total cost"></a>Exercise: total cost</h4><ul><li>Now that you have J_content and J_style, compute the total cost J by calling <code>total_cost()</code>.</li><li>Use <code>alpha = 10</code> and <code>beta = 40</code>.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">J = total_cost(J_content, J_style, <span class="number">10</span>, <span class="number">40</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><ul><li>Use the Adam optimizer to minimize the total cost <code>J</code>.</li><li>Use a learning rate of 2.0.</li><li><a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" target="_blank" rel="noopener">Adam Optimizer documentation</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define optimizer (1 line)</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define train_step (1 line)</span></span><br><span class="line">train_step = optimizer.minimize(J)</span><br></pre></td></tr></table></figure><h4 id="Exercise-implement-the-model"><a href="#Exercise-implement-the-model" class="headerlink" title="Exercise: implement the model"></a>Exercise: implement the model</h4><ul><li>Implement the model_nn() function.</li><li>The function <strong>initializes</strong> the variables of the tensorflow graph,</li><li><strong>assigns</strong> the input image (initial generated image) as the input of the VGG19 model</li><li>and <strong>runs</strong> the <code>train_step</code> tensor (it was created in the code above this function) for a large number of steps.</li></ul><h5 id="Hints-2"><a href="#Hints-2" class="headerlink" title="Hints"></a>Hints</h5><ul><li><p>To initialize global variables, use this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure></li><li><p>Run <code>sess.run()</code> to evaluate a variable.</p></li><li><a href="https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/assign" target="_blank" rel="noopener">assign</a> can be used like this:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model[<span class="string">"input"</span>].assign(image)</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess, input_image, num_iterations = <span class="number">200</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize global variables (you need to run the session on the initializer)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the noisy input image (initial generated image) through the model. Use assign().</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    sess.run(model[<span class="string">"input"</span>].assign(input_image))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Run the session on the train_step to minimize the total cost</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the generated image by running the session on the current model['input']</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">        generated_image = sess.run(model[<span class="string">"input"</span>])</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print every 20 iteration.</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            Jt, Jc, Js = sess.run([J, J_content, J_style])</span><br><span class="line">            print(<span class="string">"Iteration "</span> + str(i) + <span class="string">" :"</span>)</span><br><span class="line">            print(<span class="string">"total cost = "</span> + str(Jt))</span><br><span class="line">            print(<span class="string">"content cost = "</span> + str(Jc))</span><br><span class="line">            print(<span class="string">"style cost = "</span> + str(Js))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save current generated image in the "/output" directory</span></span><br><span class="line">            save_image(<span class="string">"output/"</span> + str(i) + <span class="string">".png"</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save last generated image</span></span><br><span class="line">    save_image(<span class="string">'output/generated_image.jpg'</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_image</span><br></pre></td></tr></table></figure><p>Run the following cell to generate an artistic image. It should take about 3min on CPU for every 20 iterations but you start observing attractive results after ≈140 iterations. Neural Style Transfer is generally trained using GPUs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_nn(sess, generated_image)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584088516041.png" alt="1584088516041"></p><p>You’re done! After running this, in the upper bar of the notebook click on “File” and then “Open”. Go to the “/output” directory to see all the saved images. Open “generated_image” to see the generated image! :)</p><p>You should see something the image presented below on the right:</p><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584088539696.png" alt="1584088539696"></p><p>We didn’t want you to wait too long to see an initial result, and so had set the hyperparameters accordingly. To get the best looking results, running the optimization algorithm longer (and perhaps with a smaller learning rate) might work better. After completing and submitting this assignment, we encourage you to come back and play more with this notebook, and see if you can generate even better looking images.</p><p>Here are few other examples:</p><ul><li>The beautiful ruins of the ancient city of Persepolis (Iran) with the style of Van Gogh (The Starry Night)</li></ul><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584088682024.png" alt="1584088682024"></p><ul><li>The tomb of Cyrus the great in Pasargadae with the style of a Ceramic Kashi from Ispahan.</li></ul><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584088725213.png" alt="1584088725213"></p><ul><li>A scientific study of a turbulent fluid with the style of a abstract blue fluid painting.</li></ul><p><img src="/2020/03/10/deeplearning-ai笔记（4-4）/1584088734228.png" alt="1584088734228"></p><h3 id="5-Test-with-your-own-image-Optional-Ungraded"><a href="#5-Test-with-your-own-image-Optional-Ungraded" class="headerlink" title="5 - Test with your own image (Optional/Ungraded)"></a>5 - Test with your own image (Optional/Ungraded)</h3><p>Finally, you can also rerun the algorithm on your own images!</p><p>To do so, go back to part 4 and change the content image and style image with your own pictures. In detail, here’s what you should do:</p><ol><li>Click on “File -&gt; Open” in the upper tab of the notebook</li><li>Go to “/images” and upload your images (requirement: (WIDTH = 300, HEIGHT = 225)), rename them “my_content.png” and “my_style.png” for example.</li><li>Change the code in part (3.4) from :<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">content_image = scipy.misc.imread(<span class="string">"images/louvre.jpg"</span>)</span><br><span class="line">style_image = scipy.misc.imread(<span class="string">"images/claude-monet.jpg"</span>)</span><br></pre></td></tr></table></figure></li></ol><p>to:<br></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">content_image = scipy.misc.imread(<span class="string">"images/my_content.jpg"</span>)</span><br><span class="line">style_image = scipy.misc.imread(<span class="string">"images/my_style.jpg"</span>)</span><br></pre></td></tr></table></figure><p></p><ol><li>Rerun the cells (you may need to restart the Kernel in the upper tab of the notebook).</li></ol><p>You can share your generated images with us on social media with the hashtag #deeplearniNgAI or by direct tagging!</p><p>You can also tune your hyperparameters:</p><ul><li>Which layers are responsible for representing the style? STYLE_LAYERS</li><li>How many iterations do you want to run the algorithm? num_iterations</li><li>What is the relative weighting between content and style? alpha/beta</li></ul><h3 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6 - Conclusion"></a>6 - Conclusion</h3><p>Great job on completing this assignment! You are now able to use Neural Style Transfer to generate artistic images. This is also your first time building a model in which the optimization algorithm updates the pixel values rather than the neural network’s parameters. Deep learning has many different types of models and this is only one of them!</p><h4 id="What-you-should-remember-3"><a href="#What-you-should-remember-3" class="headerlink" title="What you should remember"></a>What you should remember</h4><ul><li>Neural Style Transfer is an algorithm that given a content image C and a style image S can generate an artistic image</li><li>It uses representations (hidden layer activations) based on a pretrained ConvNet.</li><li>The content cost function is computed using one hidden layer’s activations.</li><li>The style cost function for one layer is computed using the Gram matrix of that layer’s activations. The overall style cost function is obtained using several hidden layers.</li><li>Optimizing the total cost function results in synthesizing new images.</li></ul><h2 id="Congratulations-on-finishing-the-course"><a href="#Congratulations-on-finishing-the-course" class="headerlink" title="Congratulations on finishing the course!"></a>Congratulations on finishing the course!</h2><p>This was the final programming exercise of this course. Congratulations—you’ve finished all the programming exercises of this course on Convolutional Networks! We hope to also see you in Course 5, on Sequence models!</p><h3 id="References-1"><a href="#References-1" class="headerlink" title="References:"></a>References:</h3><p>The Neural Style Transfer algorithm was due to Gatys et al. (2015). Harish Narayanan and Github user “log0” also have highly readable write-ups from which we drew inspiration. The pre-trained network used in this implementation is a VGG network, which is due to Simonyan and Zisserman (2015). Pre-trained weights were from the work of the MathConvNet team.</p><ul><li>Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). <a href="https://arxiv.org/abs/1508.06576" target="_blank" rel="noopener">A Neural Algorithm of Artistic Style</a></li><li>Harish Narayanan, <a href="https://harishnarayanan.org/writing/artistic-style-transfer/" target="_blank" rel="noopener">Convolutional neural networks for artistic style transfer.</a></li><li>Log0, <a href="http://www.chioka.in/tensorflow-implementation-neural-algorithm-of-artistic-style" target="_blank" rel="noopener">TensorFlow Implementation of “A Neural Algorithm of Artistic Style”.</a></li><li>Karen Simonyan and Andrew Zisserman (2015). <a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">Very deep convolutional networks for large-scale image recognition</a></li><li><a href="http://www.vlfeat.org/matconvnet/pretrained/" target="_blank" rel="noopener">MatConvNet.</a></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://mooc.study.163.com/university/deeplearning_ai#/c" target="_blank" rel="noopener">https://mooc.study.163.com/university/deeplearning_ai#/c</a></p><p><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">https://www.coursera.org/specializations/deep-learning</a></p><p><a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes" target="_blank" rel="noopener">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p><p><a href="https://blog.csdn.net/Koala_Tree/article/details/79913655" target="_blank" rel="noopener">https://blog.csdn.net/Koala_Tree/article/details/79913655</a></p></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">-------------本文结束-------------</div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/深度学习/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/01/24/实战Google深度学习框架-第10-12章/" rel="next" title="实战Google深度学习框架-第10~12章"><i class="fa fa-chevron-left"></i> 实战Google深度学习框架-第10~12章</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/03/16/deeplearning-ai笔记（5-1）/" rel="prev" title="deeplearning-ai笔记（5-1）">deeplearning-ai笔记（5-1） <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/kikyo.jpg" alt="Kikyō"><p class="site-author-name" itemprop="name">Kikyō</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">33</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">4</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">7</span> <span class="site-state-item-name">标签</span></a></div></nav></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#特殊应用：人脸识别和神经风格转换（Special-applications-Face-recognition-amp-Neural-style-transfer）"><span class="nav-text">特殊应用：人脸识别和神经风格转换（Special applications: Face recognition &amp;Neural style transfer）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-什么是人脸识别？（What-is-face-recognition-）"><span class="nav-text">4.1 什么是人脸识别？（What is face recognition?）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-One-Shot学习（One-shot-learning）"><span class="nav-text">4.2 One-Shot学习（One-shot learning）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Similarity函数"><span class="nav-text">Similarity函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Siamese-网络（Siamese-network）"><span class="nav-text">4.3 Siamese 网络（Siamese network）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Triplet-损失（Triplet-损失）"><span class="nav-text">4.4 Triplet 损失（Triplet 损失）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-人脸验证与二分类（Face-verification-and-binary-classification）"><span class="nav-text">4.5 人脸验证与二分类（Face verification and binary classification）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-什么是神经风格迁移？（What-is-neural-style-transfer-）"><span class="nav-text">4.6 什么是神经风格迁移？（What is neural style transfer?）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7-CNN特征可视化（What-are-deep-ConvNets-learning-）"><span class="nav-text">4.7 CNN特征可视化（What are deep ConvNets learning?）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-8-代价函数（Cost-function）"><span class="nav-text">4.8 代价函数（Cost function）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-9-内容代价函数（Content-cost-function）"><span class="nav-text">4.9 内容代价函数（Content cost function）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-10-风格代价函数（Style-cost-function）"><span class="nav-text">4.10 风格代价函数（Style cost function）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-11-一维到三维推广（1D-and-3D-generalizations-of-models）"><span class="nav-text">4.11 一维到三维推广（1D and 3D generalizations of models）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Face-Recognition"><span class="nav-text">Face Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Channels-first-notation"><span class="nav-text">Channels-first notation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Updates"><span class="nav-text">Updates</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#If-you-were-working-on-the-notebook-before-this-update…"><span class="nav-text">If you were working on the notebook before this update…</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#List-of-updates"><span class="nav-text">List of updates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Load-packages"><span class="nav-text">Load packages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-Naive-Face-Verification"><span class="nav-text">0 - Naive Face Verification</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Encoding-face-images-into-a-128-dimensional-vector"><span class="nav-text">1 - Encoding face images into a 128-dimensional vector</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-Using-a-ConvNet-to-compute-encodings"><span class="nav-text">1.1 - Using a ConvNet to compute encodings</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-The-Triplet-Loss"><span class="nav-text">1.2 - The Triplet Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hints"><span class="nav-text">Hints</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Additional-Hints"><span class="nav-text">Additional Hints</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Loading-the-pre-trained-model"><span class="nav-text">2 - Loading the pre-trained model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Applying-the-model"><span class="nav-text">3 - Applying the model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Face-Verification"><span class="nav-text">3.1 - Face Verification</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hints-1"><span class="nav-text">Hints</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Face-Recognition"><span class="nav-text">3.2 - Face Recognition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Congratulations"><span class="nav-text">Congratulations!</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ways-to-improve-your-facial-recognition-model"><span class="nav-text">Ways to improve your facial recognition model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-text">References:</span></a></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning-amp-Art-Neural-Style-Transfer"><span class="nav-text">Deep Learning &amp; Art: Neural Style Transfer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Updates-1"><span class="nav-text">Updates</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#If-you-were-working-on-the-notebook-before-this-update…-1"><span class="nav-text">If you were working on the notebook before this update…</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#List-of-updates-1"><span class="nav-text">List of updates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Problem-Statement"><span class="nav-text">1 - Problem Statement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Transfer-Learning"><span class="nav-text">2 - Transfer Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Assign-input-image-to-the-model’s-input-layer"><span class="nav-text">Assign input image to the model’s input layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Activate-a-layer"><span class="nav-text">Activate a layer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Neural-Style-Transfer-NST"><span class="nav-text">3 - Neural Style Transfer (NST)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Computing-the-content-cost"><span class="nav-text">3.1 - Computing the content cost</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Shallower-versus-deeper-layers"><span class="nav-text">Shallower versus deeper layers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Choose-a-“middle”-activation-layer-a-l"><span class="nav-text">Choose a “middle” activation layer $a^{[l]}$</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Forward-propagate-image-“C”"><span class="nav-text">Forward propagate image “C”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Forward-propagate-image-“G”"><span class="nav-text">Forward propagate image “G”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Content-Cost-Function-J-content-C-G"><span class="nav-text">Content Cost Function $J_{content}(C,G)$</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Additional-Hints-for-“Unrolling”"><span class="nav-text">Additional Hints for “Unrolling”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#What-you-should-remember"><span class="nav-text">What you should remember</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Computing-the-style-cost"><span class="nav-text">3.2 - Computing the style cost</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-1-Style-matrix"><span class="nav-text">3.2.1 - Style matrix</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Gram-matrix"><span class="nav-text">Gram matrix</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Two-meanings-of-the-variable-G"><span class="nav-text">Two meanings of the variable $G$</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Compute-G-gram"><span class="nav-text">Compute $G_{gram}$</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#G-gram-i-j-correlation"><span class="nav-text">$G_{(gram)i,j}$: correlation</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#G-gram-i-i-prevalence-of-patterns-or-textures"><span class="nav-text">$G_{(gram),i,i}$: prevalence of patterns or textures</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-2-Style-cost"><span class="nav-text">3.2.2 - Style cost</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Additional-Hints-1"><span class="nav-text">Additional Hints</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-3-Style-Weights"><span class="nav-text">3.2.3 Style Weights</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Exercise-compute-style-cost"><span class="nav-text">Exercise: compute style cost</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Description-of-compute-style-cost"><span class="nav-text">Description of compute_style_cost</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#What-you-should-remember-1"><span class="nav-text">What you should remember</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Defining-the-total-cost-to-optimize"><span class="nav-text">3.3 - Defining the total cost to optimize</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#What-you-should-remember-2"><span class="nav-text">What you should remember</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Solving-the-optimization-problem"><span class="nav-text">4 - Solving the optimization problem</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Interactive-Sessions"><span class="nav-text">Interactive Sessions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Start-the-interactive-session"><span class="nav-text">Start the interactive session.</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Content-image"><span class="nav-text">Content image</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Style-image"><span class="nav-text">Style image</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Generated-image-correlated-with-content-image"><span class="nav-text">Generated image correlated with content image</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Load-pre-trained-VGG19-model"><span class="nav-text">Load pre-trained VGG19 model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Content-Cost"><span class="nav-text">Content Cost</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Style-cost"><span class="nav-text">Style cost</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise-total-cost"><span class="nav-text">Exercise: total cost</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimizer"><span class="nav-text">Optimizer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exercise-implement-the-model"><span class="nav-text">Exercise: implement the model</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Hints-2"><span class="nav-text">Hints</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Test-with-your-own-image-Optional-Ungraded"><span class="nav-text">5 - Test with your own image (Optional/Ungraded)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Conclusion"><span class="nav-text">6 - Conclusion</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#What-you-should-remember-3"><span class="nav-text">What you should remember</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Congratulations-on-finishing-the-course"><span class="nav-text">Congratulations on finishing the course!</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#References-1"><span class="nav-text">References:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-text">参考资料</span></a></li></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Kikyō</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-area-chart"></i> </span><span class="post-meta-item-text">Site words total count&#58;</span> <span title="Site words total count">160k</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".search-popup-overlay").click(onPopupClose),$(".popup").toggle();var t=$("#local-search-input");t.attr("autocapitalize","none"),t.attr("autocorrect","off"),t.focus()}var isfetched=!1,isXml=!0,search_path="search.xml";0===search_path.length?search_path="search.xml":/json$/i.test(search_path)&&(isXml=!1);var path="/"+search_path,onPopupClose=function(t){$(".popup").hide(),$("#local-search-input").val(""),$(".search-result-list").remove(),$("#no-result").remove(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")},searchFunc=function(t,e,o){"use strict";$("body").append('<div class="search-popup-overlay local-search-pop-overlay"><div id="search-loading-icon"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div>').css("overflow","hidden"),$("#search-loading-icon").css("margin","20% auto 0 auto").css("text-align","center"),$.ajax({url:t,dataType:isXml?"xml":"json",async:!0,success:function(t){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var n=isXml?$("entry",t).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get():t,r=document.getElementById(e),s=document.getElementById(o),a=function(){var t=r.value.trim().toLowerCase(),e=t.split(/[\s\-]+/);e.length>1&&e.push(t);var o=[];if(t.length>0&&n.forEach(function(n){function r(e,o,n,r){for(var s=r[r.length-1],a=s.position,i=s.word,l=[],h=0;a+i.length<=n&&0!=r.length;){i===t&&h++,l.push({position:a,length:i.length});var p=a+i.length;for(r.pop();0!=r.length&&(s=r[r.length-1],a=s.position,i=s.word,p>a);)r.pop()}return c+=h,{hits:l,start:o,end:n,searchTextCount:h}}function s(t,e){var o="",n=e.start;return e.hits.forEach(function(e){o+=t.substring(n,e.position);var r=e.position+e.length;o+='<b class="search-keyword">'+t.substring(e.position,r)+"</b>",n=r}),o+=t.substring(n,e.end)}var a=!1,i=0,c=0,l=n.title.trim(),h=l.toLowerCase(),p=n.content.trim().replace(/<[^>]+>/g,""),u=p.toLowerCase(),f=decodeURIComponent(n.url),d=[],g=[];if(""!=l&&(e.forEach(function(t){function e(t,e,o){var n=t.length;if(0===n)return[];var r=0,s=[],a=[];for(o||(e=e.toLowerCase(),t=t.toLowerCase());(s=e.indexOf(t,r))>-1;)a.push({position:s,word:t}),r=s+n;return a}d=d.concat(e(t,h,!1)),g=g.concat(e(t,u,!1))}),(d.length>0||g.length>0)&&(a=!0,i=d.length+g.length)),a){[d,g].forEach(function(t){t.sort(function(t,e){return e.position!==t.position?e.position-t.position:t.word.length-e.word.length})});var v=[];0!=d.length&&v.push(r(l,0,l.length,d));for(var $=[];0!=g.length;){var C=g[g.length-1],m=C.position,x=C.word,w=m-20,y=m+80;0>w&&(w=0),y<m+x.length&&(y=m+x.length),y>p.length&&(y=p.length),$.push(r(p,w,y,g))}$.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hits.length!==e.hits.length?e.hits.length-t.hits.length:t.start-e.start});var T=parseInt("1");T>=0&&($=$.slice(0,T));var b="";b+=0!=v.length?"<li><a href='"+f+"' class='search-result-title'>"+s(l,v[0])+"</a>":"<li><a href='"+f+"' class='search-result-title'>"+l+"</a>",$.forEach(function(t){b+="<a href='"+f+'\'><p class="search-result">'+s(p,t)+"...</p></a>"}),b+="</li>",o.push({item:b,searchTextCount:c,hitCount:i,id:o.length})}}),1===e.length&&""===e[0])s.innerHTML='<div id="no-result"><i class="fa fa-search fa-5x" /></div>';else if(0===o.length)s.innerHTML='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>';else{o.sort(function(t,e){return t.searchTextCount!==e.searchTextCount?e.searchTextCount-t.searchTextCount:t.hitCount!==e.hitCount?e.hitCount-t.hitCount:e.id-t.id});var a='<ul class="search-result-list">';o.forEach(function(t){a+=t.item}),a+="</ul>",s.innerHTML=a}};r.addEventListener("input",a),$(".local-search-pop-overlay").remove(),$("body").css("overflow",""),proceedsearch()}})};$(".popup-trigger").click(function(t){t.stopPropagation(),isfetched===!1?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(onPopupClose),$(".popup").click(function(t){t.stopPropagation()}),$(document).on("keyup",function(t){var e=27===t.which&&$(".search-popup").is(":visible");e&&onPopupClose()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script></body></html><!-- rebuild by neat -->